#!/usr/bin/env python3
"""
5K A/B Test Infrastructure Setup and Deployment
Production deployment preparation for RA-Guard based on T3-Verified results

Usage:
    python deploy_5k_ab_test.py --config ab_test_config.yaml --environment production
"""

import json
import yaml
import numpy as np
from typing import Dict, List, Optional
import logging
from pathlib import Path
from dataclasses import dataclass
import argparse
from datetime import datetime, timedelta
import time
import subprocess

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ABTestConfig:
    """Configuration for 5K A/B test deployment"""
    sample_size: int = 5000
    duration_days: int = 14
    ramp_schedule: List[Dict] = None
    success_threshold: float = 2.97  # nDCG points
    expected_effect: float = 4.24   # nDCG points
    significance_level: float = 0.05
    power: float = 0.9
    
    def __post_init__(self):
        if self.ramp_schedule is None:
            self.ramp_schedule = [
                {'day': 1, 'traffic_percent': 1, 'duration_hours': 24},
                {'day': 2, 'traffic_percent': 5, 'duration_hours': 24},
                {'day': 3, 'traffic_percent': 10, 'duration_hours': 24},
                {'day': 4, 'traffic_percent': 25, 'duration_hours': 24},
                {'day': 5, 'traffic_percent': 50, 'duration_hours': 216}  # Remaining 9 days
            ]

class ProductionDeploymentManager:
    """Manages production deployment of RA-Guard for 5K A/B testing"""
    
    def __init__(self, config: ABTestConfig):
        self.config = config
        self.deployment_id = f"ra_guard_5k_ab_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = None
        
    def prepare_infrastructure(self) -> Dict:
        """Prepare production infrastructure for A/B testing"""
        
        logger.info("Preparing production infrastructure for 5K A/B test...")
        
        infrastructure_setup = {
            'deployment_metadata': {
                'deployment_id': self.deployment_id,
                'deployment_type': '5K_AB_TEST',
                'start_time': datetime.now().isoformat(),
                'expected_end_time': (datetime.now() + timedelta(days=self.config.duration_days)).isoformat(),
                'ra_guard_version': 'v1.0_T3_verified'
            },
            'infrastructure_components': self._setup_infrastructure_components(),
            'monitoring_setup': self._setup_monitoring(),
            'traffic_routing': self._setup_traffic_routing(),
            'rollback_procedures': self._setup_rollback_procedures(),
            'success_criteria': self._define_success_criteria()
        }
        
        return infrastructure_setup
    
    def _setup_infrastructure_components(self) -> Dict:
        """Setup core infrastructure components"""
        
        logger.info("Configuring infrastructure components...")
        
        return {
            'ra_guard_service': {
                'deployment_environment': 'production',
                'service_name': 'ra-guard-enhancement',
                'replicas': 3,
                'resource_allocation': {
                    'cpu': '2 cores per replica',
                    'memory': '4GB per replica',
                    'gpu': 'not_required'
                },
                'scaling_policy': {
                    'min_replicas': 3,
                    'max_replicas': 10,
                    'target_cpu_utilization': 70
                }
            },
            'image_gallery': {
                'storage': 's3://production-gallery/{cocktails|flowers|professional}/',
                'candidate_pool_size': '50-200 images per query',
                'total_gallery_size': '60K+ images across 3 domains',
                'metadata_database': {
                    'type': 'postgresql',
                    'schema': 'image_gallery_with_vectors',
                    'indexes': ['domain', 'provider', 'phash']
                },
                'feature_cache': {
                    'clip_embeddings': 'redis_cluster_precomputed',
                    'detector_outputs': 'redis_cluster_cached',
                    'candidate_sets': 'application_cache_1h_ttl'
                }
            },
            'load_balancer': {
                'type': 'application_load_balancer',
                'traffic_splitting': {
                    'control_group': 50,  # Baseline retrieval + ranking
                    'treatment_group': 50  # Baseline retrieval + RA-Guard reranking
                },
                'health_checks': {
                    'interval_seconds': 30,
                    'timeout_seconds': 5,
                    'healthy_threshold': 2,
                    'unhealthy_threshold': 3
                }
            },
            'data_pipeline': {
                'real_time_logging': True,
                'candidate_logging': True,  # Critical for A/B consistency
                'metrics_collection': {
                    'ndcg_calculation': 'real_time',
                    'latency_tracking': 'per_request',
                    'error_monitoring': 'continuous',
                    'gallery_performance': 'continuous'
                },
                'data_retention': '90_days'
            }
        }
    
    def _setup_monitoring(self) -> Dict:
        """Setup comprehensive monitoring and alerting"""
        
        logger.info("Configuring monitoring and alerting systems...")
        
        return {
            'real_time_dashboards': {
                'primary_metrics': [
                    'nDCG@10 improvement',
                    'request_latency_p95',
                    'error_rate',
                    'traffic_allocation',
                    'user_satisfaction_score'
                ],
                'update_frequency': '1_minute',
                'stakeholder_access': ['technical_team', 'product_management', 'executives']
            },
            'alerting_rules': [
                {
                    'metric': 'error_rate',
                    'threshold': '> 2%',
                    'severity': 'critical',
                    'action': 'immediate_rollback'
                },
                {
                    'metric': 'latency_p95',
                    'threshold': '> +200ms baseline',
                    'severity': 'high',
                    'action': 'investigate_and_potentially_rollback'
                },
                {
                    'metric': 'ndcg_improvement',
                    'threshold': '< 0 for 4 hours',
                    'severity': 'medium',
                    'action': 'detailed_analysis_required'
                },
                {
                    'metric': 'user_complaints',
                    'threshold': '> 20% increase',
                    'severity': 'high',
                    'action': 'immediate_investigation'
                }
            ],
            'automated_reporting': {
                'daily_summary': True,
                'weekly_deep_dive': True,
                'real_time_anomaly_detection': True
            }
        }
    
    def _setup_traffic_routing(self) -> Dict:
        """Setup traffic routing for gradual ramp"""
        
        logger.info("Configuring traffic routing and ramp strategy...")
        
        return {
            'routing_strategy': 'gradual_ramp',
            'ramp_schedule': self.config.ramp_schedule,
            'user_assignment': {
                'method': 'deterministic_hash',
                'hash_key': 'user_id',
                'consistency': 'maintained_throughout_test'
            },
            'traffic_controls': {
                'manual_override': True,
                'emergency_stop': True,
                'rollback_capability': 'immediate'
            },
            'experiment_isolation': {
                'feature_flags': 'ra_guard_enhancement',
                'killswitch': 'enabled',
                'partial_rollback': 'supported'
            }
        }
    
    def _setup_rollback_procedures(self) -> Dict:
        """Setup comprehensive rollback procedures"""
        
        logger.info("Defining rollback procedures and safety measures...")
        
        return {
            'automated_rollback_triggers': [
                'error_rate > 2%',
                'latency_p95 > +200ms for 15 minutes',
                'nDCG_improvement < -1.0 for 2 hours'
            ],
            'manual_rollback_process': {
                'decision_makers': ['technical_lead', 'product_manager'],
                'approval_required': 'single_approver',
                'execution_time': '< 5 minutes'
            },
            'rollback_validation': {
                'post_rollback_monitoring': '24_hours',
                'metrics_recovery_check': 'automated',
                'user_impact_assessment': 'required'
            },
            'communication_plan': {
                'internal_notification': 'slack_alert',
                'stakeholder_update': 'email_summary',
                'post_mortem': 'required_if_rollback'
            }
        }
    
    def _define_success_criteria(self) -> Dict:
        """Define comprehensive success criteria"""
        
        logger.info("Defining success criteria and evaluation metrics...")
        
        return {
            'primary_success_metric': {
                'metric': 'nDCG@10_improvement',
                'success_threshold': f'+{self.config.success_threshold} points',
                'expected_value': f'+{self.config.expected_effect} points',
                'measurement_method': 'paired_comparison',
                'confidence_level': 95
            },
            'guardrail_metrics': [
                {
                    'metric': 'latency_p95',
                    'threshold': '< +100ms',
                    'tolerance': 'strict'
                },
                {
                    'metric': 'error_rate',
                    'threshold': '< +1%',
                    'tolerance': 'strict'
                },
                {
                    'metric': 'user_satisfaction',
                    'threshold': 'no_degradation',
                    'tolerance': 'moderate'
                }
            ],
            'business_metrics': [
                {
                    'metric': 'user_engagement',
                    'measurement': 'click_through_rate',
                    'expectation': 'positive_or_neutral'
                },
                {
                    'metric': 'search_success_rate',
                    'measurement': 'successful_query_completion',
                    'expectation': 'improvement'
                }
            ],
            'statistical_requirements': {
                'minimum_sample_size': 2500,  # Per group
                'statistical_power': 90,
                'significance_level': 5,
                'effect_size_detectability': 'medium'
            }
        }
    
    def generate_deployment_checklist(self) -> List[Dict]:
        """Generate comprehensive deployment checklist"""
        
        logger.info("Generating deployment readiness checklist...")
        
        checklist = [
            {
                'category': 'Infrastructure Readiness',
                'items': [
                    {'task': 'RA-Guard service deployed to production', 'status': 'pending', 'owner': 'devops_team'},
                    {'task': 'Image gallery infrastructure deployed (S3 + metadata DB)', 'status': 'pending', 'owner': 'data_team'},
                    {'task': 'CLIP embeddings and detector outputs precomputed', 'status': 'pending', 'owner': 'ml_team'},
                    {'task': 'Feature caching (Redis clusters) operational', 'status': 'pending', 'owner': 'devops_team'},
                    {'task': 'Load balancer configured with traffic splitting', 'status': 'pending', 'owner': 'devops_team'},
                    {'task': 'Monitoring dashboards operational', 'status': 'pending', 'owner': 'monitoring_team'},
                    {'task': 'Alerting rules configured and tested', 'status': 'pending', 'owner': 'monitoring_team'},
                    {'task': 'Rollback procedures tested in staging', 'status': 'pending', 'owner': 'devops_team'}
                ]
            },
            {
                'category': 'Technical Validation',
                'items': [
                    {'task': 'End-to-end system testing completed', 'status': 'pending', 'owner': 'qa_team'},
                    {'task': 'Real image gallery validation with candidate reranking', 'status': 'pending', 'owner': 'qa_team'},
                    {'task': 'Feature caching latency validation (< 150ms P95)', 'status': 'pending', 'owner': 'performance_team'},
                    {'task': 'Candidate logging and offline/online consistency verified', 'status': 'pending', 'owner': 'data_team'},
                    {'task': 'Gallery performance testing (1000 QPS)', 'status': 'pending', 'owner': 'performance_team'},
                    {'task': 'Performance baseline established', 'status': 'pending', 'owner': 'performance_team'},
                    {'task': 'RA-Guard accuracy validation in staging', 'status': 'pending', 'owner': 'ml_team'},
                    {'task': 'Data pipeline validation completed', 'status': 'pending', 'owner': 'data_team'},
                    {'task': 'Security review and approval', 'status': 'pending', 'owner': 'security_team'}
                ]
            },
            {
                'category': 'Business Readiness',
                'items': [
                    {'task': 'Stakeholder approval obtained', 'status': 'pending', 'owner': 'product_management'},
                    {'task': 'Customer support team briefed', 'status': 'pending', 'owner': 'customer_success'},
                    {'task': 'Communication plan activated', 'status': 'pending', 'owner': 'marketing'},
                    {'task': 'Risk assessment completed', 'status': 'pending', 'owner': 'risk_management'},
                    {'task': 'Legal and compliance review', 'status': 'pending', 'owner': 'legal_team'}
                ]
            },
            {
                'category': 'Operational Readiness',
                'items': [
                    {'task': 'On-call rotation scheduled', 'status': 'pending', 'owner': 'operations_team'},
                    {'task': 'Incident response procedures updated', 'status': 'pending', 'owner': 'operations_team'},
                    {'task': 'Data analysis scripts prepared', 'status': 'pending', 'owner': 'analytics_team'},
                    {'task': 'Decision framework for go/no-go defined', 'status': 'pending', 'owner': 'product_management'},
                    {'task': 'Post-test analysis plan finalized', 'status': 'pending', 'owner': 'analytics_team'}
                ]
            }
        ]
        
        return checklist
    
    def create_deployment_timeline(self) -> Dict:
        """Create detailed deployment timeline"""
        
        logger.info("Creating deployment timeline...")
        
        start_date = datetime.now().date()
        
        timeline = {
            'timeline_metadata': {
                'created_date': datetime.now().isoformat(),
                'deployment_start_target': (start_date + timedelta(days=7)).isoformat(),
                'total_duration_days': self.config.duration_days + 7,  # Include prep time
                'critical_path_items': 'infrastructure_setup, stakeholder_approval'
            },
            'phases': [
                {
                    'phase': 'Preparation',
                    'duration_days': 7,
                    'start_date': start_date.isoformat(),
                    'end_date': (start_date + timedelta(days=6)).isoformat(),
                    'milestones': [
                        {'day': 1, 'milestone': 'Infrastructure deployment begins'},
                        {'day': 3, 'milestone': 'Monitoring and alerting setup complete'},
                        {'day': 5, 'milestone': 'End-to-end testing completed'},
                        {'day': 7, 'milestone': 'Go/no-go decision for A/B test launch'}
                    ]
                },
                {
                    'phase': 'Gradual Ramp',
                    'duration_days': 5,
                    'start_date': (start_date + timedelta(days=7)).isoformat(),
                    'end_date': (start_date + timedelta(days=11)).isoformat(),
                    'milestones': [
                        {'day': 8, 'milestone': '1% traffic enabled - monitor for 24h'},
                        {'day': 9, 'milestone': '5% traffic - validate performance'},
                        {'day': 10, 'milestone': '10% traffic - check guardrail metrics'},
                        {'day': 11, 'milestone': '25% traffic - interim analysis'},
                        {'day': 12, 'milestone': '50% traffic - full A/B test begins'}
                    ]
                },
                {
                    'phase': 'Full A/B Test',
                    'duration_days': 9,
                    'start_date': (start_date + timedelta(days=12)).isoformat(),
                    'end_date': (start_date + timedelta(days=20)).isoformat(),
                    'milestones': [
                        {'day': 15, 'milestone': 'Interim analysis - statistical power check'},
                        {'day': 18, 'milestone': 'Final data collection'},
                        {'day': 21, 'milestone': 'Test completion and analysis'}
                    ]
                },
                {
                    'phase': 'Analysis & Decision',
                    'duration_days': 3,
                    'start_date': (start_date + timedelta(days=21)).isoformat(),
                    'end_date': (start_date + timedelta(days=23)).isoformat(),
                    'milestones': [
                        {'day': 22, 'milestone': 'Statistical analysis completed'},
                        {'day': 23, 'milestone': 'Business impact assessment'},
                        {'day': 24, 'milestone': 'Final deployment decision'}
                    ]
                }
            ]
        }
        
        return timeline

class ABTestAnalyzer:
    """Real-time analysis and monitoring for A/B test"""
    
    def __init__(self, config: ABTestConfig):
        self.config = config
        
    def calculate_required_sample_size(self) -> Dict:
        """Calculate required sample size for statistical power"""
        
        # Using power analysis for two-sample t-test
        effect_size = self.config.expected_effect / 100  # Convert to proportion
        alpha = self.config.significance_level
        power = self.config.power
        
        # Simplified sample size calculation
        from scipy import stats
        
        z_alpha = stats.norm.ppf(1 - alpha/2)
        z_beta = stats.norm.ppf(power)
        
        # Assuming equal variance and allocation
        n_per_group = 2 * ((z_alpha + z_beta) / effect_size) ** 2
        
        return {
            'required_sample_size': {
                'per_group': int(np.ceil(n_per_group)),
                'total': int(np.ceil(n_per_group * 2)),
                'planned_per_group': self.config.sample_size // 2,
                'planned_total': self.config.sample_size
            },
            'power_analysis': {
                'effect_size': effect_size,
                'alpha': alpha,
                'power': power,
                'adequate_sample_size': self.config.sample_size >= n_per_group * 2
            }
        }
    
    def generate_analysis_framework(self) -> Dict:
        """Generate framework for ongoing analysis"""
        
        return {
            'statistical_monitoring': {
                'interim_analyses': [
                    {'day': 3, 'purpose': 'early_safety_check', 'sample_size_threshold': 0.2},
                    {'day': 7, 'purpose': 'power_validation', 'sample_size_threshold': 0.5},
                    {'day': 10, 'purpose': 'interim_efficacy', 'sample_size_threshold': 0.75},
                    {'day': 14, 'purpose': 'final_analysis', 'sample_size_threshold': 1.0}
                ],
                'alpha_spending': {
                    'method': 'obrien_fleming',
                    'total_alpha': 0.05,
                    'interim_boundaries': [0.005, 0.015, 0.035, 0.05]
                }
            },
            'business_metrics_tracking': {
                'daily_kpis': [
                    'nDCG@10 improvement',
                    'user_satisfaction_delta',
                    'search_success_rate',
                    'user_engagement_metrics'
                ],
                'weekly_deep_dive': [
                    'domain_specific_performance',
                    'user_segment_analysis',
                    'temporal_pattern_analysis'
                ]
            },
            'decision_framework': {
                'go_criteria': [
                    'primary_metric_success',
                    'guardrail_metrics_safe',
                    'business_metrics_positive',
                    'statistical_significance_achieved'
                ],
                'no_go_criteria': [
                    'primary_metric_failure',
                    'guardrail_breach',
                    'user_experience_degradation',
                    'technical_issues'
                ]
            }
        }

def main():
    """Main execution function for 5K A/B test deployment"""
    
    parser = argparse.ArgumentParser(description='Deploy RA-Guard 5K A/B Test')
    parser.add_argument('--config', help='A/B test configuration file')
    parser.add_argument('--environment', default='production', help='Deployment environment')
    parser.add_argument('--dry-run', action='store_true', help='Generate plans without deployment')
    
    args = parser.parse_args()
    
    # Initialize configuration
    config = ABTestConfig()
    
    # Initialize deployment manager
    deployment_manager = ProductionDeploymentManager(config)
    
    # Initialize analyzer
    analyzer = ABTestAnalyzer(config)
    
    # Generate deployment plans
    logger.info("Generating 5K A/B test deployment plans...")
    
    infrastructure_setup = deployment_manager.prepare_infrastructure()
    deployment_checklist = deployment_manager.generate_deployment_checklist()
    deployment_timeline = deployment_manager.create_deployment_timeline()
    sample_size_analysis = analyzer.calculate_required_sample_size()
    analysis_framework = analyzer.generate_analysis_framework()
    
    # Compile complete deployment package
    deployment_package = {
        'deployment_metadata': {
            'package_created': datetime.now().isoformat(),
            'deployment_id': deployment_manager.deployment_id,
            'ra_guard_version': 'T3_verified_production_ready',
            'expected_business_impact': '+4.24 nDCG points',
            'deployment_confidence': 'HIGH'
        },
        'infrastructure_setup': infrastructure_setup,
        'deployment_checklist': deployment_checklist,
        'deployment_timeline': deployment_timeline,
        'sample_size_analysis': sample_size_analysis,
        'analysis_framework': analysis_framework,
        'risk_mitigation': {
            'identified_risks': [
                'Performance degradation',
                'User experience impact',
                'Technical failures',
                'Statistical power shortfall'
            ],
            'mitigation_strategies': [
                'Gradual traffic ramp',
                'Real-time monitoring',
                'Immediate rollback capability',
                'Conservative success thresholds'
            ]
        }
    }
    
    # Save deployment package
    output_dir = Path('deployment/5k_ab_test')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    deployment_file = output_dir / f'{deployment_manager.deployment_id}_deployment_package.json'
    
    with open(deployment_file, 'w') as f:
        json.dump(deployment_package, f, indent=2, default=str)
    
    # Print deployment summary
    print(f"\nğŸš€ 5K A/B Test Deployment Package Generated")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"Deployment ID: {deployment_manager.deployment_id}")
    print(f"Sample Size: {config.sample_size:,} users")
    print(f"Duration: {config.duration_days} days")
    print(f"Expected Effect: +{config.expected_effect:.2f} nDCG points")
    print(f"Success Threshold: +{config.success_threshold:.2f} nDCG points")
    
    print(f"\nğŸ“… Deployment Timeline:")
    for phase in deployment_timeline['phases']:
        print(f"  {phase['phase']}: {phase['duration_days']} days ({phase['start_date']} â†’ {phase['end_date']})")
    
    print(f"\nğŸ“Š Sample Size Analysis:")
    sample_analysis = sample_size_analysis['required_sample_size']
    print(f"  Required per group: {sample_analysis['per_group']:,}")
    print(f"  Planned per group: {sample_analysis['planned_per_group']:,}")
    print(f"  Sample size adequate: {'âœ… YES' if sample_size_analysis['power_analysis']['adequate_sample_size'] else 'âŒ NO'}")
    
    print(f"\nâœ… Deployment Checklist Summary:")
    for category in deployment_checklist:
        pending_items = len([item for item in category['items'] if item['status'] == 'pending'])
        print(f"  {category['category']}: {pending_items} items pending")
    
    print(f"\nğŸ“‚ Deployment Package: {deployment_file}")
    
    if args.dry_run:
        print(f"\nğŸ” DRY RUN MODE - No actual deployment performed")
        print(f"ğŸ“‹ Review deployment package and execute checklist items")
        print(f"ğŸš€ Ready to proceed with production deployment")
    else:
        print(f"\nâš ï¸  PRODUCTION DEPLOYMENT MODE")
        print(f"ğŸ“‹ Execute checklist items before proceeding")
        print(f"ğŸš€ Launch A/B test when ready")
    
    logger.info("5K A/B test deployment package generation completed")

if __name__ == "__main__":
    main()
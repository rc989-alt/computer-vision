#!/usr/bin/env python3
"""
Benchmark Harness - æ•°æ®å£°æ˜è¯æ®é“¾ç»Ÿä¸€æµ‹è¯•æ¡†æ¶
================================================================================
ç”¨äºæ‰€æœ‰æ€§èƒ½å£°æ˜çš„æ ‡å‡†åŒ–æµ‹è¯•ã€ç•™ç—•ä¸å¯å¤ç°æ€§ä¿è¯
================================================================================

Usage:
    python benchmark_harness.py --model cotrr-lite --dataset v1_prod_eval
    python benchmark_harness.py --model v1-production --dataset standard_eval
    python benchmark_harness.py --config configs/cotrr_benchmark.json
"""

import json
import time
import hashlib
import argparse
import subprocess
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
import psutil
import GPUtil

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BenchmarkHarness:
    """ç»Ÿä¸€åŸºå‡†æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self.load_config(config_path)
        self.start_time = None
        self.end_time = None
        
    def load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        default_config = {
            "trust_tier": "T2-Internal",
            "min_samples": 100,
            "max_duration_s": 3600,
            "metrics": ["latency_p95", "mem_peak_mb", "ndcg10_delta", "error_rate"],
            "output_dir": "reports",
            "log_dir": "logs"
        }
        
        if config_path and Path(config_path).exists():
            with open(config_path) as f:
                user_config = json.load(f)
            default_config.update(user_config)
            
        return default_config
    
    def get_git_info(self) -> Dict[str, str]:
        """è·å–Gitç‰ˆæœ¬ä¿¡æ¯"""
        try:
            commit = subprocess.check_output(
                ["git", "rev-parse", "HEAD"], text=True
            ).strip()
            branch = subprocess.check_output(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"], text=True
            ).strip()
            return {"commit": commit, "branch": branch}
        except:
            return {"commit": "unknown", "branch": "unknown"}
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            gpus = GPUtil.getGPUs()
            gpu_info = [{"name": gpu.name, "memory_mb": gpu.memoryTotal} for gpu in gpus]
        except:
            gpu_info = []
            
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "gpus": gpu_info,
            "python_version": subprocess.check_output(
                ["python", "--version"], text=True
            ).strip()
        }
    
    def monitor_resources(self) -> Dict[str, float]:
        """ç›‘æ§èµ„æºä½¿ç”¨"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        try:
            gpus = GPUtil.getGPUs()
            gpu_memory = max([gpu.memoryUsed for gpu in gpus]) if gpus else 0
        except:
            gpu_memory = 0
            
        return {
            "mem_peak_mb": memory_info.rss / (1024**2),
            "gpu_mem_mb": gpu_memory,
            "cpu_percent": process.cpu_percent()
        }
    
    def run_benchmark(self, model_name: str, dataset_name: str) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºå‡†æµ‹è¯•ä¸»æµç¨‹"""
        logger.info(f"Starting benchmark: {model_name} on {dataset_name}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.config["output_dir"]).mkdir(exist_ok=True)
        Path(self.config["log_dir"]).mkdir(exist_ok=True)
        
        self.start_time = time.time()
        start_resources = self.monitor_resources()
        
        # æ‰§è¡Œå®é™…æµ‹è¯•ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹å®ç°ï¼‰
        metrics = self.execute_model_evaluation(model_name, dataset_name)
        
        self.end_time = time.time()
        end_resources = self.monitor_resources()
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report = self.build_report(
            model_name, dataset_name, metrics, 
            start_resources, end_resources
        )
        
        # ä¿å­˜æŠ¥å‘Šå’Œç”Ÿæˆæ‘˜è¦
        self.save_report(report)
        self.generate_summary(report)
        
        logger.info(f"Benchmark completed: {report['run_id']}")
        return report
    
    def execute_model_evaluation(self, model_name: str, dataset_name: str) -> Dict[str, float]:
        """æ‰§è¡Œå…·ä½“çš„æ¨¡å‹è¯„ä¼°ï¼ˆéœ€è¦æ ¹æ®å®é™…æ¨¡å‹å®ç°ï¼‰"""
        # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦è°ƒç”¨çœŸå®çš„æ¨¡å‹
        logger.info(f"Evaluating {model_name} on {dataset_name}")
        
        if model_name.startswith("cotrr"):
            # CoTRRæ¨¡å‹çš„è¯„ä¼°é€»è¾‘
            return self.evaluate_cotrr_model(model_name, dataset_name)
        elif model_name.startswith("v1"):
            # V1.0æ¨¡å‹çš„è¯„ä¼°é€»è¾‘
            return self.evaluate_v1_model(model_name, dataset_name)
        else:
            raise ValueError(f"Unknown model: {model_name}")
    
    def evaluate_cotrr_model(self, model_name: str, dataset_name: str) -> Dict[str, float]:
        """CoTRRæ¨¡å‹è¯„ä¼°"""
        logger.warning("CoTRR model evaluation - placeholder implementation")
        
        # æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹
        time.sleep(2)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        
        return {
            "latency_p95": 0.0,  # å¾…å®é™…æµ‹è¯•
            "mem_peak_mb": 0.0,  # å¾…å®é™…æµ‹è¯•
            "ndcg10_delta": 0.0,  # å¾…å®é™…æµ‹è¯•
            "error_rate": 0.0,   # å¾…å®é™…æµ‹è¯•
            "status": "needs_implementation"
        }
    
    def evaluate_v1_model(self, model_name: str, dataset_name: str) -> Dict[str, float]:
        """V1.0æ¨¡å‹è¯„ä¼°"""
        logger.info("V1.0 model evaluation - using production_evaluation.json")
        
        # è¯»å–å·²æœ‰çš„ç”Ÿäº§è¯„ä¼°æ•°æ®
        prod_eval_path = Path("research/day3_results/production_evaluation.json")
        if prod_eval_path.exists():
            with open(prod_eval_path) as f:
                prod_data = json.load(f)
            
            return {
                "compliance_improvement": prod_data["metrics"]["compliance_improvement"],
                "ndcg_improvement": prod_data["metrics"]["ndcg_improvement"],
                "latency_p95": prod_data["metrics"]["p95_latency_ms"],
                "error_rate": prod_data["metrics"]["blossom_fruit_error_rate"]
            }
        else:
            logger.error("Production evaluation data not found")
            return {"error": "no_production_data"}
    
    def build_report(self, model_name: str, dataset_name: str, metrics: Dict[str, float],
                    start_resources: Dict[str, float], end_resources: Dict[str, float]) -> Dict[str, Any]:
        """æ„å»ºå®Œæ•´çš„åŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        
        run_id = f"{model_name}_{dataset_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        report = {
            "run_id": run_id,
            "model_name": model_name,
            "dataset_name": dataset_name,
            "metrics": metrics,
            "timing": {
                "start_time": self.start_time,
                "end_time": self.end_time,
                "wall_clock_s": self.end_time - self.start_time
            },
            "resources": {
                "start": start_resources,
                "end": end_resources,
                "peak_memory_mb": max(start_resources["mem_peak_mb"], end_resources["mem_peak_mb"])
            },
            "environment": {
                "git": self.get_git_info(),
                "system": self.get_system_info(),
                "config": self.config
            },
            "timestamp": datetime.now().isoformat(),
            "trust_tier": self.config["trust_tier"]
        }
        
        # ç”ŸæˆæŠ¥å‘Šå“ˆå¸Œ
        report_json = json.dumps(report, sort_keys=True, indent=2)
        report["sha256"] = hashlib.sha256(report_json.encode()).hexdigest()
        
        return report
    
    def save_report(self, report: Dict[str, Any]) -> None:
        """ä¿å­˜åŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        output_path = Path(self.config["output_dir"]) / f"benchmark_report_{report['run_id']}.json"
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
            
        logger.info(f"Report saved: {output_path}")
        
        # åŒæ—¶ä¿å­˜æœ€æ–°æŠ¥å‘Šçš„ç¬¦å·é“¾æ¥
        latest_path = Path(self.config["output_dir"]) / "benchmark_report_latest.json"
        if latest_path.exists():
            latest_path.unlink()
        latest_path.symlink_to(output_path.name)
    
    def generate_summary(self, report: Dict[str, Any]) -> None:
        """ç”Ÿæˆæ‘˜è¦è¡¨"""
        summary = {
            "run_id": report["run_id"],
            "model": report["model_name"],
            "dataset": report["dataset_name"],
            "trust_tier": report["trust_tier"],
            "timestamp": report["timestamp"]
        }
        
        # æå–å…³é”®æŒ‡æ ‡
        metrics = report["metrics"]
        for key in ["latency_p95", "mem_peak_mb", "ndcg10_delta", "error_rate", "compliance_improvement"]:
            if key in metrics:
                summary[key] = metrics[key]
        
        summary_path = Path(self.config["output_dir"]) / f"summary_{report['run_id']}.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
            
        logger.info(f"Summary saved: {summary_path}")

def main():
    parser = argparse.ArgumentParser(description="Benchmark Harness for Data Integrity")
    parser.add_argument("--model", required=True, help="Model name to benchmark")
    parser.add_argument("--dataset", required=True, help="Dataset name for evaluation")
    parser.add_argument("--config", help="Config file path")
    parser.add_argument("--trust-tier", choices=["T1-Indicative", "T2-Internal", "T3-Verified"], 
                       default="T2-Internal", help="Trust tier for this benchmark")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶è¿è¡ŒåŸºå‡†æµ‹è¯•
    harness = BenchmarkHarness(args.config)
    harness.config["trust_tier"] = args.trust_tier
    
    try:
        report = harness.run_benchmark(args.model, args.dataset)
        print(f"âœ… Benchmark completed successfully: {report['run_id']}")
        print(f"ğŸ“Š Report: reports/benchmark_report_{report['run_id']}.json")
        print(f"ğŸ¯ Trust Tier: {report['trust_tier']}")
        
        # è¾“å‡ºå…³é”®æŒ‡æ ‡
        if "metrics" in report:
            print("\nğŸ“ˆ Key Metrics:")
            for key, value in report["metrics"].items():
                print(f"  {key}: {value}")
                
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
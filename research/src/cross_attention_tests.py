#!/usr/bin/env python3
"""
CoTRR-Stable Cross-Attentionæ¶æ„å®Œå–„ç‰ˆ
Day 1 ä»»åŠ¡ï¼šå®ç°TokenåŒ–å¤šæ¨¡æ€ç¼–ç å™¨å’Œè½»é‡çº§Cross-Attention

å®Œå–„å†…å®¹:
1. æ·»åŠ è¯¦ç»†çš„å•å…ƒæµ‹è¯•
2. æ€§èƒ½åŸºå‡†æµ‹è¯•
3. æ¢¯åº¦æ£€æŸ¥
4. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
5. æ¨¡å‹æ¶æ„éªŒè¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import unittest
import time
import logging
from pathlib import Path

# å¯¼å…¥åŸºç¡€æ¨¡å‹
from cotrr_stable import (
    StableCrossAttnReranker, StableConfig, TokenizedMultiModalEncoder,
    LightweightCrossAttention, create_stable_model
)

logger = logging.getLogger(__name__)

class CrossAttentionTester:
    """Cross-Attentionæ¨¡å‹æµ‹è¯•å™¨"""
    
    def __init__(self, config: Optional[StableConfig] = None):
        if config is None:
            config = StableConfig()
        
        self.config = config
        self.model = create_stable_model(config)
        self.test_results = {}
        
    def run_comprehensive_tests(self) -> Dict[str, bool]:
        """è¿è¡Œå…¨é¢æµ‹è¯•å¥—ä»¶"""
        logger.info("ğŸ§ª å¼€å§‹Cross-Attentionæ¨¡å‹å…¨é¢æµ‹è¯•")
        
        test_suite = {
            "architecture_validation": self.test_architecture_validation,
            "forward_pass": self.test_forward_pass,
            "gradient_flow": self.test_gradient_flow,
            "attention_weights": self.test_attention_weights,
            "batch_consistency": self.test_batch_consistency,
            "performance_benchmark": self.test_performance_benchmark,
            "memory_efficiency": self.test_memory_efficiency,
            "numerical_stability": self.test_numerical_stability
        }
        
        results = {}
        for test_name, test_func in test_suite.items():
            try:
                logger.info(f"è¿è¡Œæµ‹è¯•: {test_name}")
                success = test_func()
                results[test_name] = success
                status = "âœ… PASS" if success else "âŒ FAIL"
                logger.info(f"{test_name}: {status}")
            except Exception as e:
                logger.error(f"{test_name}: âŒ ERROR - {str(e)}")
                results[test_name] = False
        
        self.test_results = results
        return results
    
    def test_architecture_validation(self) -> bool:
        """æµ‹è¯•æ¶æ„éªŒè¯"""
        try:
            # æ£€æŸ¥æ¨¡å‹å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            logger.info(f"æ€»å‚æ•°: {total_params:,}")
            logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            
            # éªŒè¯å‚æ•°é‡åœ¨åˆç†èŒƒå›´å†… (1-2M)
            if not (1_000_000 <= total_params <= 2_000_000):
                logger.warning(f"å‚æ•°é‡ {total_params:,} è¶…å‡ºé¢„æœŸèŒƒå›´ [1M, 2M]")
                return False
            
            # æ£€æŸ¥å…³é”®ç»„ä»¶å­˜åœ¨
            required_components = [
                'token_encoder', 'attention_layers', 'output_head', 'temperature'
            ]
            
            for component in required_components:
                if not hasattr(self.model, component):
                    logger.error(f"ç¼ºå°‘å…³é”®ç»„ä»¶: {component}")
                    return False
            
            # æ£€æŸ¥attentionå±‚æ•°
            if len(self.model.attention_layers) != self.config.num_layers:
                logger.error(f"Attentionå±‚æ•°ä¸åŒ¹é…: æœŸæœ›{self.config.num_layers}, å®é™…{len(self.model.attention_layers)}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ¶æ„éªŒè¯å¤±è´¥: {e}")
            return False
    
    def test_forward_pass(self) -> bool:
        """æµ‹è¯•å‰å‘ä¼ æ’­"""
        try:
            batch_size = 8
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            test_input = {
                'clip_img': torch.randn(batch_size, self.config.clip_img_dim),
                'clip_text': torch.randn(batch_size, self.config.clip_text_dim),
                'visual_features': torch.randn(batch_size, self.config.visual_dim),
                'conflict_features': torch.randn(batch_size, self.config.conflict_dim)
            }
            
            # å‰å‘ä¼ æ’­
            self.model.eval()
            with torch.no_grad():
                output = self.model(**test_input)
            
            # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
            expected_shape = (batch_size, 1)
            if output['logits'].shape != expected_shape:
                logger.error(f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{output['logits'].shape}")
                return False
            
            # æ£€æŸ¥è¾“å‡ºå€¼èŒƒå›´
            logits = output['logits']
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                logger.error("è¾“å‡ºåŒ…å«NaNæˆ–Infå€¼")
                return False
            
            # æ£€æŸ¥æ ¡å‡†åçš„logits
            if 'calibrated_logits' in output:
                cal_logits = output['calibrated_logits']
                if torch.isnan(cal_logits).any() or torch.isinf(cal_logits).any():
                    logger.error("æ ¡å‡†ålogitsåŒ…å«NaNæˆ–Infå€¼")
                    return False
            
            # æ£€æŸ¥æ¸©åº¦å‚æ•°
            if 'temperature' in output:
                temp = output['temperature']
                if temp <= 0:
                    logger.error(f"æ¸©åº¦å‚æ•°å¿…é¡»ä¸ºæ­£: {temp}")
                    return False
            
            logger.info(f"å‰å‘ä¼ æ’­æˆåŠŸ: è¾“å‡ºå½¢çŠ¶{output['logits'].shape}, æ¸©åº¦{output['temperature'].item():.3f}")
            return True
            
        except Exception as e:
            logger.error(f"å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_gradient_flow(self) -> bool:
        """æµ‹è¯•æ¢¯åº¦æµ"""
        try:
            batch_size = 4
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥å’Œæ ‡ç­¾
            test_input = {
                'clip_img': torch.randn(batch_size, self.config.clip_img_dim, requires_grad=True),
                'clip_text': torch.randn(batch_size, self.config.clip_text_dim, requires_grad=True),
                'visual_features': torch.randn(batch_size, self.config.visual_dim, requires_grad=True),
                'conflict_features': torch.randn(batch_size, self.config.conflict_dim, requires_grad=True)
            }
            
            labels = torch.randn(batch_size, 1)
            
            # å‰å‘ä¼ æ’­
            self.model.train()
            output = self.model(**test_input, training=True)
            
            # è®¡ç®—æŸå¤±
            loss = F.mse_loss(output['logits'], labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦
            no_grad_params = []
            for name, param in self.model.named_parameters():
                if param.grad is None:
                    no_grad_params.append(name)
                elif torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                    logger.error(f"å‚æ•° {name} æ¢¯åº¦åŒ…å«NaNæˆ–Inf")
                    return False
            
            if no_grad_params:
                logger.warning(f"ä»¥ä¸‹å‚æ•°æ²¡æœ‰æ¢¯åº¦: {no_grad_params}")
                # å¯¹äºæŸäº›å‚æ•°ï¼ˆå¦‚biasï¼‰ï¼Œå¯èƒ½ç¡®å®æ²¡æœ‰æ¢¯åº¦ï¼Œè¿™æ˜¯æ­£å¸¸çš„
            
            logger.info(f"æ¢¯åº¦æµæµ‹è¯•æˆåŠŸ: æŸå¤±å€¼ {loss.item():.4f}")
            return True
            
        except Exception as e:
            logger.error(f"æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_attention_weights(self) -> bool:
        """æµ‹è¯•æ³¨æ„åŠ›æƒé‡"""
        try:
            batch_size = 4
            
            test_input = {
                'clip_img': torch.randn(batch_size, self.config.clip_img_dim),
                'clip_text': torch.randn(batch_size, self.config.clip_text_dim),
                'visual_features': torch.randn(batch_size, self.config.visual_dim),
                'conflict_features': torch.randn(batch_size, self.config.conflict_dim)
            }
            
            self.model.eval()
            with torch.no_grad():
                output = self.model(**test_input)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ³¨æ„åŠ›æƒé‡
            if 'attention_weights' in output:
                attn_weights = output['attention_weights']
                
                # æ£€æŸ¥æ¯å±‚æ³¨æ„åŠ›æƒé‡
                for i, weights in enumerate(attn_weights):
                    # å½¢çŠ¶åº”è¯¥æ˜¯ [batch, heads, seq_len, seq_len]
                    expected_shape = (batch_size, self.config.num_attention_heads, 4, 4)  # 4ä¸ªtoken
                    
                    if weights.shape != expected_shape:
                        logger.error(f"ç¬¬{i}å±‚æ³¨æ„åŠ›æƒé‡å½¢çŠ¶é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{weights.shape}")
                        return False
                    
                    # æ£€æŸ¥æƒé‡æ˜¯å¦å½’ä¸€åŒ–ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
                    row_sums = weights.sum(dim=-1)
                    if not torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6):
                        logger.error(f"ç¬¬{i}å±‚æ³¨æ„åŠ›æƒé‡æœªæ­£ç¡®å½’ä¸€åŒ–")
                        return False
                    
                    # æ£€æŸ¥æƒé‡å€¼èŒƒå›´ [0, 1]
                    if (weights < 0).any() or (weights > 1).any():
                        logger.error(f"ç¬¬{i}å±‚æ³¨æ„åŠ›æƒé‡è¶…å‡º[0,1]èŒƒå›´")
                        return False
                
                logger.info(f"æ³¨æ„åŠ›æƒé‡æµ‹è¯•æˆåŠŸ: {len(attn_weights)}å±‚æƒé‡")
                return True
            else:
                logger.warning("æ¨¡å‹è¾“å‡ºä¸­æ²¡æœ‰æ³¨æ„åŠ›æƒé‡")
                return True  # ä¸æ˜¯è‡´å‘½é”™è¯¯
                
        except Exception as e:
            logger.error(f"æ³¨æ„åŠ›æƒé‡æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_batch_consistency(self) -> bool:
        """æµ‹è¯•æ‰¹æ¬¡ä¸€è‡´æ€§"""
        try:
            # æµ‹è¯•å•ä¸ªæ ·æœ¬ vs æ‰¹æ¬¡å¤„ç†çš„ä¸€è‡´æ€§
            single_input = {
                'clip_img': torch.randn(1, self.config.clip_img_dim),
                'clip_text': torch.randn(1, self.config.clip_text_dim),
                'visual_features': torch.randn(1, self.config.visual_dim),
                'conflict_features': torch.randn(1, self.config.conflict_dim)
            }
            
            # å¤åˆ¶æˆæ‰¹æ¬¡
            batch_input = {
                key: value.repeat(3, 1) for key, value in single_input.items()
            }
            
            self.model.eval()
            with torch.no_grad():
                # å•ä¸ªæ ·æœ¬è¾“å‡º
                single_output = self.model(**single_input)
                
                # æ‰¹æ¬¡è¾“å‡º
                batch_output = self.model(**batch_input)
            
            # æ£€æŸ¥æ‰¹æ¬¡ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯å¦ä¸å•ä¸ªæ ·æœ¬ä¸€è‡´
            single_logit = single_output['logits'][0]
            batch_first_logit = batch_output['logits'][0]
            
            if not torch.allclose(single_logit, batch_first_logit, atol=1e-6):
                logger.error(f"æ‰¹æ¬¡ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: å•ä¸ª{single_logit}, æ‰¹æ¬¡ç¬¬ä¸€ä¸ª{batch_first_logit}")
                return False
            
            logger.info("æ‰¹æ¬¡ä¸€è‡´æ€§æµ‹è¯•æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_performance_benchmark(self) -> bool:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        try:
            batch_sizes = [1, 4, 8, 16]
            timing_results = {}
            
            for batch_size in batch_sizes:
                test_input = {
                    'clip_img': torch.randn(batch_size, self.config.clip_img_dim),
                    'clip_text': torch.randn(batch_size, self.config.clip_text_dim),
                    'visual_features': torch.randn(batch_size, self.config.visual_dim),
                    'conflict_features': torch.randn(batch_size, self.config.conflict_dim)
                }
                
                # é¢„çƒ­
                self.model.eval()
                with torch.no_grad():
                    for _ in range(5):
                        _ = self.model(**test_input)
                
                # è®¡æ—¶
                num_runs = 50
                start_time = time.time()
                
                with torch.no_grad():
                    for _ in range(num_runs):
                        _ = self.model(**test_input)
                
                end_time = time.time()
                
                avg_time = (end_time - start_time) / num_runs
                throughput = batch_size / avg_time  # samples/second
                
                timing_results[batch_size] = {
                    'avg_time': avg_time,
                    'throughput': throughput
                }
                
                logger.info(f"Batch {batch_size}: {avg_time*1000:.2f}ms, {throughput:.1f} samples/sec")
            
            # æ£€æŸ¥æ€§èƒ½æ˜¯å¦åˆç†
            single_sample_time = timing_results[1]['avg_time']
            if single_sample_time > 0.1:  # 100ms threshold
                logger.warning(f"å•æ ·æœ¬æ¨ç†æ—¶é—´è¿‡é•¿: {single_sample_time*1000:.2f}ms")
                return False
            
            self.test_results['performance_benchmark'] = timing_results
            return True
            
        except Exception as e:
            logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_memory_efficiency(self) -> bool:
        """å†…å­˜æ•ˆç‡æµ‹è¯•"""
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            
            # åŸºå‡†å†…å­˜ä½¿ç”¨
            baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # åˆ›å»ºå¤§æ‰¹æ¬¡è¿›è¡Œå†…å­˜æµ‹è¯•
            large_batch = 32
            test_input = {
                'clip_img': torch.randn(large_batch, self.config.clip_img_dim),
                'clip_text': torch.randn(large_batch, self.config.clip_text_dim),
                'visual_features': torch.randn(large_batch, self.config.visual_dim),
                'conflict_features': torch.randn(large_batch, self.config.conflict_dim)
            }
            
            # å‰å‘ä¼ æ’­
            self.model.eval()
            with torch.no_grad():
                output = self.model(**test_input)
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = peak_memory - baseline_memory
            
            logger.info(f"å†…å­˜ä½¿ç”¨: åŸºå‡† {baseline_memory:.1f}MB, å³°å€¼ {peak_memory:.1f}MB, å¢é•¿ {memory_increase:.1f}MB")
            
            # å†…å­˜å¢é•¿åº”è¯¥åˆç†ï¼ˆ<500MB for batch_32ï¼‰
            if memory_increase > 500:
                logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_increase:.1f}MB")
                return False
            
            return True
            
        except ImportError:
            logger.warning("psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æ•ˆç‡æµ‹è¯•")
            return True
        except Exception as e:
            logger.error(f"å†…å­˜æ•ˆç‡æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_numerical_stability(self) -> bool:
        """æ•°å€¼ç¨³å®šæ€§æµ‹è¯•"""
        try:
            # æµ‹è¯•æç«¯è¾“å…¥å€¼
            extreme_cases = [
                # å…¨é›¶è¾“å…¥
                {
                    'clip_img': torch.zeros(2, self.config.clip_img_dim),
                    'clip_text': torch.zeros(2, self.config.clip_text_dim),
                    'visual_features': torch.zeros(2, self.config.visual_dim),
                    'conflict_features': torch.zeros(2, self.config.conflict_dim)
                },
                # å¤§æ•°å€¼è¾“å…¥
                {
                    'clip_img': torch.ones(2, self.config.clip_img_dim) * 10,
                    'clip_text': torch.ones(2, self.config.clip_text_dim) * 10,
                    'visual_features': torch.ones(2, self.config.visual_dim) * 10,
                    'conflict_features': torch.ones(2, self.config.conflict_dim) * 10
                },
                # éšæœºå¤§èŒƒå›´è¾“å…¥
                {
                    'clip_img': torch.randn(2, self.config.clip_img_dim) * 100,
                    'clip_text': torch.randn(2, self.config.clip_text_dim) * 100,
                    'visual_features': torch.randn(2, self.config.visual_dim) * 100,
                    'conflict_features': torch.randn(2, self.config.conflict_dim) * 100
                }
            ]
            
            self.model.eval()
            for i, test_case in enumerate(extreme_cases):
                with torch.no_grad():
                    try:
                        output = self.model(**test_case)
                        
                        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaNæˆ–Inf
                        if torch.isnan(output['logits']).any() or torch.isinf(output['logits']).any():
                            logger.error(f"æç«¯æƒ…å†µ{i}äº§ç”Ÿäº†NaN/Infè¾“å‡º")
                            return False
                        
                        logger.info(f"æç«¯æƒ…å†µ{i}æµ‹è¯•é€šè¿‡")
                        
                    except Exception as e:
                        logger.error(f"æç«¯æƒ…å†µ{i}æµ‹è¯•å¤±è´¥: {e}")
                        return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ•°å€¼ç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def visualize_attention_patterns(self, save_dir: str = "research/stage1_progress"):
        """å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼"""
        try:
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            batch_size = 1
            test_input = {
                'clip_img': torch.randn(batch_size, self.config.clip_img_dim),
                'clip_text': torch.randn(batch_size, self.config.clip_text_dim),
                'visual_features': torch.randn(batch_size, self.config.visual_dim),
                'conflict_features': torch.randn(batch_size, self.config.conflict_dim)
            }
            
            self.model.eval()
            with torch.no_grad():
                output = self.model(**test_input)
            
            if 'attention_weights' not in output:
                logger.warning("æ— æ³•è·å–æ³¨æ„åŠ›æƒé‡è¿›è¡Œå¯è§†åŒ–")
                return
            
            attn_weights = output['attention_weights']
            token_names = ['CLIP-Img', 'CLIP-Text', 'Visual', 'Conflict']
            
            # ä¸ºæ¯ä¸€å±‚åˆ›å»ºæ³¨æ„åŠ›çƒ­å›¾
            num_layers = len(attn_weights)
            fig, axes = plt.subplots(1, num_layers, figsize=(5*num_layers, 4))
            
            if num_layers == 1:
                axes = [axes]
            
            for i, weights in enumerate(attn_weights):
                # å–ç¬¬ä¸€ä¸ªbatchå’Œå¹³å‡æ‰€æœ‰heads
                layer_weights = weights[0].mean(dim=0).cpu().numpy()  # [seq_len, seq_len]
                
                sns.heatmap(
                    layer_weights,
                    annot=True,
                    fmt='.3f',
                    xticklabels=token_names,
                    yticklabels=token_names,
                    ax=axes[i],
                    cmap='Blues'
                )
                axes[i].set_title(f'Layer {i+1} Attention')
            
            plt.tight_layout()
            save_path = Path(save_dir) / 'attention_visualization.png'
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
            
        except Exception as e:
            logger.error(f"æ³¨æ„åŠ›å¯è§†åŒ–å¤±è´¥: {e}")
    
    def generate_test_report(self, save_dir: str = "research/stage1_progress") -> Dict:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            logger.warning("æ²¡æœ‰æµ‹è¯•ç»“æœï¼Œè¯·å…ˆè¿è¡Œæµ‹è¯•")
            return {}
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result)
        failed_tests = total_tests - passed_tests
        success_rate = passed_tests / total_tests * 100
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'model_config': {
                'hidden_dim': self.config.hidden_dim,
                'num_layers': self.config.num_layers,
                'num_attention_heads': self.config.num_attention_heads,
                'total_parameters': sum(p.numel() for p in self.model.parameters())
            },
            'test_summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'success_rate': success_rate
            },
            'detailed_results': self.test_results,
            'performance_data': self.test_results.get('performance_benchmark', {})
        }
        
        # ä¿å­˜æŠ¥å‘Š
        import json
        report_path = Path(save_dir) / 'cross_attention_test_report.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        logger.info(f"æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡ ({success_rate:.1f}%)")
        
        return report

def main():
    """è¿è¡ŒCross-Attentionæ¶æ„æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹Cross-Attentionæ¶æ„å®Œå–„å’Œæµ‹è¯•")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    config = StableConfig()
    tester = CrossAttentionTester(config)
    
    print(f"ğŸ“Š æ¨¡å‹é…ç½®:")
    print(f"  - Hidden dim: {config.hidden_dim}")
    print(f"  - Attention layers: {config.num_layers}")
    print(f"  - Attention heads: {config.num_attention_heads}")
    print(f"  - Total parameters: {sum(p.numel() for p in tester.model.parameters()):,}")
    
    # è¿è¡Œå…¨é¢æµ‹è¯•
    test_results = tester.run_comprehensive_tests()
    
    # ç”Ÿæˆå¯è§†åŒ–
    tester.visualize_attention_patterns()
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report = tester.generate_test_report()
    
    # æ€»ç»“ç»“æœ
    passed = sum(1 for result in test_results.values() if result)
    total = len(test_results)
    
    print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"âœ… é€šè¿‡: {passed}/{total}")
    print(f"âŒ å¤±è´¥: {total-passed}/{total}")
    print(f"ğŸ“Š æˆåŠŸç‡: {passed/total*100:.1f}%")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Cross-Attentionæ¶æ„å®ç°å®Œæˆ")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤")
        return False

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    success = main()
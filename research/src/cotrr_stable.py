#!/usr/bin/env python3
"""
CoTRR-Stable: ç¨³å¥çš„ä¸¤é˜¶æ®µå®ç°æ–¹æ¡ˆ
é˜¶æ®µ1: Cross-Attention + ListMLE + Calibration (2å‘¨ç›®æ ‡)

æ ¸å¿ƒè®¾è®¡åŸåˆ™:
1. ä¸ç°æœ‰Step4/5æ— ç¼é›†æˆ
2. ç¨³å¥çš„æ€§èƒ½æå‡: C@1 +4-6pts, nDCG@10 +8-12pts  
3. å¯æ§çš„æ¨ç†æˆæœ¬: ä»…Top-Mä½¿ç”¨å¤æ‚æ¨¡å‹
4. ä¸¥æ ¼çš„A/Bæµ‹è¯•å‡†å…¥é—¨æ§›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class StableConfig:
    """ç¨³å¥é…ç½®"""
    # æ¨¡å‹æ¶æ„ - ä¿å®ˆå‚æ•°
    hidden_dim: int = 256  # é™ä½è‡³256ï¼Œå¹³è¡¡æ€§èƒ½ä¸æˆæœ¬
    num_attention_heads: int = 8
    num_layers: int = 2    # åªç”¨2å±‚ï¼Œé¿å…è¿‡æ‹Ÿåˆ
    dropout: float = 0.15  # ç¨é«˜dropoutå¢åŠ é²æ£’æ€§
    
    # ç‰¹å¾ç»´åº¦
    clip_img_dim: int = 1024
    clip_text_dim: int = 1024  
    visual_dim: int = 8
    conflict_dim: int = 5
    
    # è®­ç»ƒç­–ç•¥
    warmup_epochs: int = 3      # Pairwise warmup
    listwise_epochs: int = 8    # ListMLE fine-tune
    total_epochs: int = 12      # æ€»å…±12è½®ï¼Œä¿å®ˆ
    
    # Top-Mç­–ç•¥ - æ§åˆ¶æ¨ç†æˆæœ¬
    top_m_candidates: int = 20  # åªå¯¹Top-20åšå¤æ‚æ¨ç†
    
    # æ ¡å‡†å‚æ•°
    mc_samples: int = 5         # ä¿å®ˆçš„MCé‡‡æ ·æ•°
    ensemble_size: int = 3      # å°å‹ensemble
    
    # æ€§èƒ½é—¨æ§›
    min_compliance_gain: float = 4.0  # C@1æœ€ä½+4pts
    min_ndcg_gain: float = 8.0        # nDCG@10æœ€ä½+8pts
    max_latency_p95: float = 30.0     # p95å»¶è¿Ÿâ‰¤30min/10k

class TokenizedMultiModalEncoder(nn.Module):
    """è½»é‡çº§TokenåŒ–å¤šæ¨¡æ€ç¼–ç å™¨"""
    
    def __init__(self, config: StableConfig):
        super().__init__()
        self.config = config
        self.hidden_dim = config.hidden_dim
        
        # ç‰¹å¾æŠ•å½±å±‚ - ç»Ÿä¸€åˆ°hidden_dim
        self.img_proj = nn.Linear(config.clip_img_dim, self.hidden_dim)
        self.text_proj = nn.Linear(config.clip_text_dim, self.hidden_dim)
        self.visual_proj = nn.Linear(config.visual_dim, self.hidden_dim)
        self.conflict_proj = nn.Linear(config.conflict_dim, self.hidden_dim)
        
        # ä½ç½®ç¼–ç  - ç®€å•çš„å­¦ä¹ å¼
        self.position_embeddings = nn.Parameter(torch.randn(4, self.hidden_dim))
        
        # Layer Norm
        self.layer_norm = nn.LayerNorm(self.hidden_dim)
        
    def forward(self, clip_img, clip_text, visual_features, conflict_features):
        """
        å°†å¤šæ¨¡æ€ç‰¹å¾è½¬æ¢ä¸ºtokenåºåˆ—
        Returns: [batch, 4, hidden_dim] - [img, text, visual, conflict]
        """
        batch_size = clip_img.size(0)
        
        # ç‰¹å¾æŠ•å½±
        img_token = self.img_proj(clip_img)      # [batch, hidden]
        text_token = self.text_proj(clip_text)   # [batch, hidden]  
        visual_token = self.visual_proj(visual_features)  # [batch, hidden]
        conflict_token = self.conflict_proj(conflict_features)  # [batch, hidden]
        
        # å †å ä¸ºåºåˆ— + ä½ç½®ç¼–ç 
        tokens = torch.stack([img_token, text_token, visual_token, conflict_token], dim=1)
        # [batch, 4, hidden]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        tokens = tokens + self.position_embeddings.unsqueeze(0)
        
        # Layer norm
        tokens = self.layer_norm(tokens)
        
        return tokens

class LightweightCrossAttention(nn.Module):
    """è½»é‡çº§Cross-Attentionæ¨¡å—"""
    
    def __init__(self, hidden_dim: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert hidden_dim % num_heads == 0
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        # å•ä¸€çº¿æ€§å±‚ - å‡å°‘å‚æ•°
        self.qkv = nn.Linear(hidden_dim, hidden_dim * 3)
        self.out_proj = nn.Linear(hidden_dim, hidden_dim)
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, x):
        """
        Self-attention within multimodal tokens
        x: [batch, seq_len, hidden_dim]
        """
        batch_size, seq_len, _ = x.shape
        residual = x
        
        # QKV projection
        qkv = self.qkv(x)  # [batch, seq_len, hidden*3]
        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)
        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # [3, batch, heads, seq_len, head_dim]
        
        # Scaled dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # Apply attention
        attn_output = torch.matmul(attn_weights, v)  # [batch, heads, seq_len, head_dim]
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_dim)
        
        # Output projection + residual
        output = self.out_proj(attn_output) + residual
        output = self.layer_norm(output)
        
        return output, attn_weights

class StableCrossAttnReranker(nn.Module):
    """ç¨³å¥çš„Cross-Attentioné‡æ’å™¨"""
    
    def __init__(self, config: StableConfig):
        super().__init__()
        self.config = config
        
        # Tokenç¼–ç å™¨
        self.token_encoder = TokenizedMultiModalEncoder(config)
        
        # Cross-attentionå±‚
        self.attention_layers = nn.ModuleList([
            LightweightCrossAttention(
                config.hidden_dim, 
                config.num_attention_heads, 
                config.dropout
            ) for _ in range(config.num_layers)
        ])
        
        # è¾“å‡ºå¤´ - ç®€åŒ–ç‰ˆ
        self.output_head = nn.Sequential(
            nn.Linear(config.hidden_dim * 4, config.hidden_dim),  # æ‹¼æ¥4ä¸ªtoken
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dim, 1)  # å•åˆ†æ•°è¾“å‡º
        )
        
        # å¯å­¦ä¹ æ¸©åº¦å‚æ•°
        self.temperature = nn.Parameter(torch.ones(1))
        
        # MC Dropoutå±‚
        self.mc_dropout = nn.Dropout(0.1)
        
    def forward(self, clip_img, clip_text, visual_features, conflict_features, 
                mc_samples=1, training=False):
        """
        å‰å‘ä¼ æ’­
        Returns: Dict with 'logits', 'calibrated_logits', 'uncertainty'
        """
        if mc_samples > 1:
            # Monte Carlo inference
            return self._mc_forward(clip_img, clip_text, visual_features, 
                                  conflict_features, mc_samples)
        else:
            return self._single_forward(clip_img, clip_text, visual_features, 
                                      conflict_features, training)
    
    def _single_forward(self, clip_img, clip_text, visual_features, conflict_features, training=False):
        """å•æ¬¡å‰å‘ä¼ æ’­"""
        # 1. TokenåŒ–ç¼–ç 
        tokens = self.token_encoder(clip_img, clip_text, visual_features, conflict_features)
        # [batch, 4, hidden_dim]
        
        # 2. Cross-attention
        attention_weights = []
        for attn_layer in self.attention_layers:
            tokens, weights = attn_layer(tokens)
            attention_weights.append(weights)
            
            if training:
                tokens = self.mc_dropout(tokens)
        
        # 3. èšåˆtokenç‰¹å¾
        # Global average pooling + åŸå§‹æ‹¼æ¥
        pooled = tokens.mean(dim=1)  # [batch, hidden_dim]
        flattened = tokens.flatten(start_dim=1)  # [batch, hidden_dim*4]
        
        # 4. è¾“å‡ºé¢„æµ‹
        logits = self.output_head(flattened)  # [batch, 1]
        
        # 5. æ¸©åº¦æ ¡å‡†
        calibrated_logits = logits / (self.temperature + 1e-8)
        
        return {
            'logits': logits,
            'calibrated_logits': calibrated_logits,
            'temperature': self.temperature,
            'tokens': tokens,
            'attention_weights': attention_weights,
            'uncertainty': torch.zeros_like(logits)  # å•æ¬¡æ¨ç†æ— uncertainty
        }
    
    def _mc_forward(self, clip_img, clip_text, visual_features, conflict_features, mc_samples):
        """Monte Carloå‰å‘ä¼ æ’­"""
        self.train()  # å¼€å¯dropout
        
        mc_outputs = []
        for _ in range(mc_samples):
            output = self._single_forward(clip_img, clip_text, visual_features, 
                                        conflict_features, training=True)
            mc_outputs.append(output['calibrated_logits'])
        
        self.eval()  # æ¢å¤evalæ¨¡å¼
        
        # èšåˆMCç»“æœ
        mc_logits = torch.stack(mc_outputs, dim=0)  # [mc_samples, batch, 1]
        mean_logits = mc_logits.mean(dim=0)
        std_logits = mc_logits.std(dim=0)
        
        return {
            'logits': mean_logits,
            'calibrated_logits': mean_logits,
            'uncertainty': std_logits,
            'mc_samples': mc_logits,
            'temperature': self.temperature
        }

class ListMLEWithFocalLoss(nn.Module):
    """ListMLE + Focal Lossç»„åˆ"""
    
    def __init__(self, alpha=0.7, gamma=2.0):
        super().__init__()
        self.alpha = alpha  # ListMLEæƒé‡
        self.gamma = gamma  # Focal Losså‚æ•°
        
    def forward(self, logits, labels, valid_mask=None):
        """
        Args:
            logits: [batch, max_list_size] - é¢„æµ‹åˆ†æ•°
            labels: [batch, max_list_size] - çœŸå®ç›¸å…³æ€§åˆ†æ•°
            valid_mask: [batch, max_list_size] - æœ‰æ•ˆä½ç½®mask
        """
        if valid_mask is None:
            valid_mask = torch.ones_like(logits, dtype=torch.bool)
        
        # ListMLE loss
        listmle_loss = self._listmle_loss(logits, labels, valid_mask)
        
        # Focal loss (åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸Š)
        binary_labels = (labels > 0.5).float()
        focal_loss = self._focal_loss(logits, binary_labels, valid_mask)
        
        # ç»„åˆæŸå¤±
        total_loss = self.alpha * listmle_loss + (1 - self.alpha) * focal_loss
        
        return total_loss
    
    def _listmle_loss(self, logits, labels, valid_mask):
        """ListMLEæŸå¤±å®ç°"""
        batch_size = logits.size(0)
        
        # æŒ‰çœŸå®æ ‡ç­¾æ’åº
        sorted_labels, sort_indices = torch.sort(labels, dim=1, descending=True)
        sorted_logits = torch.gather(logits, 1, sort_indices)
        sorted_mask = torch.gather(valid_mask.float(), 1, sort_indices)
        
        # ListMLEè®¡ç®—
        losses = []
        for i in range(batch_size):
            valid_len = int(sorted_mask[i].sum().item())
            if valid_len <= 1:
                continue
                
            item_logits = sorted_logits[i, :valid_len]
            
            # è®¡ç®—ListMLE
            max_logit = item_logits.max()
            exp_logits = torch.exp(item_logits - max_logit)
            
            cumsum_exp = torch.cumsum(exp_logits.flip(0), dim=0).flip(0)
            log_prob = torch.log(exp_logits / (cumsum_exp + 1e-8))
            
            losses.append(-log_prob.sum())
        
        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=logits.device)
    
    def _focal_loss(self, logits, binary_labels, valid_mask):
        """Focal Losså®ç°"""
        # Sigmoid + BCE with focal weighting
        probs = torch.sigmoid(logits)
        bce_loss = F.binary_cross_entropy(probs, binary_labels, reduction='none')
        
        # Focal weighting
        pt = torch.where(binary_labels == 1, probs, 1 - probs)
        focal_weight = (1 - pt) ** self.gamma
        
        focal_loss = focal_weight * bce_loss
        
        # åº”ç”¨maskå¹¶å¹³å‡
        masked_loss = focal_loss * valid_mask.float()
        return masked_loss.sum() / (valid_mask.sum() + 1e-8)

class IsotonicCalibrator:
    """ç­‰æ¸—å›å½’æ ¡å‡†å™¨ï¼ˆå¯æ›¿ä»£æ¸©åº¦æ ‡å®šï¼‰"""
    
    def __init__(self):
        from sklearn.isotonic import IsotonicRegression
        self.isotonic_reg = IsotonicRegression(out_of_bounds='clip')
        self.fitted = False
    
    def fit(self, confidences, accuracies):
        """
        è®­ç»ƒæ ¡å‡†å™¨
        Args:
            confidences: æ¨¡å‹è¾“å‡ºæ¦‚ç‡
            accuracies: çœŸå®äºŒåˆ†ç±»æ ‡ç­¾
        """
        self.isotonic_reg.fit(confidences, accuracies)
        self.fitted = True
    
    def calibrate(self, confidences):
        """æ ¡å‡†æ¦‚ç‡"""
        if not self.fitted:
            return confidences
        return self.isotonic_reg.predict(confidences)

def create_stable_model(config: Optional[StableConfig] = None) -> StableCrossAttnReranker:
    """åˆ›å»ºç¨³å¥æ¨¡å‹"""
    if config is None:
        config = StableConfig()
    
    model = StableCrossAttnReranker(config)
    
    # åˆå§‹åŒ–æƒé‡
    def init_weights(m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight, gain=0.1)  # å°åˆå§‹åŒ–
            if m.bias is not None:
                torch.nn.init.zeros_(m.bias)
        elif isinstance(m, nn.LayerNorm):
            torch.nn.init.ones_(m.weight)
            torch.nn.init.zeros_(m.bias)
    
    model.apply(init_weights)
    
    return model

class StableTrainingPipeline:
    """ç¨³å¥è®­ç»ƒæµç¨‹"""
    
    def __init__(self, config: StableConfig):
        self.config = config
        self.model = create_stable_model(config)
        
        # æŸå¤±å‡½æ•°
        self.pairwise_loss = nn.BCEWithLogitsLoss()
        self.listwise_loss = ListMLEWithFocalLoss()
        
        # æ ¡å‡†å™¨
        self.calibrator = IsotonicCalibrator()
        
        # è®­ç»ƒå†å²
        self.history = {'pairwise_loss': [], 'listwise_loss': [], 'val_metrics': []}
    
    def train_stable_pipeline(self, train_data, val_data, save_dir):
        """ä¸¤é˜¶æ®µè®­ç»ƒï¼šPairwise Warmup â†’ ListMLE Fine-tune"""
        logger.info("ğŸš€ å¼€å§‹ç¨³å¥è®­ç»ƒæµç¨‹")
        
        # Stage 1: Pairwise Warmup
        logger.info("Stage 1: Pairwise Warmup")
        self._train_pairwise_warmup(train_data, self.config.warmup_epochs)
        
        # Stage 2: ListMLE Fine-tune
        logger.info("Stage 2: ListMLE Fine-tune")
        self._train_listwise_finetune(train_data, self.config.listwise_epochs)
        
        # Stage 3: Calibration
        logger.info("Stage 3: Calibration")
        self._calibrate_model(val_data)
        
        # ä¿å­˜æ¨¡å‹
        self._save_stable_model(save_dir)
        
        logger.info("âœ… ç¨³å¥è®­ç»ƒå®Œæˆ")
    
    def _train_pairwise_warmup(self, train_data, epochs):
        """Pairwiseé¢„çƒ­è®­ç»ƒ"""
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=1e-4)
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            for batch in train_data:
                optimizer.zero_grad()
                
                # æå–ç‰¹å¾
                features = self._extract_features(batch)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**features, training=True)
                
                # Pairwise loss
                labels = torch.tensor([item['compliance_score'] > 0.8 for item in batch], 
                                    dtype=torch.float, device=outputs['logits'].device)
                loss = self.pairwise_loss(outputs['logits'].squeeze(), labels)
                
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            self.history['pairwise_loss'].append(avg_loss)
            logger.info(f"Pairwise Epoch {epoch+1}, Loss: {avg_loss:.4f}")
    
    def _train_listwise_finetune(self, train_data, epochs):
        """ListMLEå¾®è°ƒè®­ç»ƒ"""
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5, weight_decay=1e-4)
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            # è¿™é‡Œåº”è¯¥æŒ‰queryåˆ†ç»„åšlistwiseè®­ç»ƒ
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦å®ç°query-levelçš„listé‡‡æ ·
            for batch in train_data:
                optimizer.zero_grad()
                
                features = self._extract_features(batch)
                outputs = self.model(**features, training=True)
                
                # æ„é€ listæ ‡ç­¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
                labels = torch.tensor([item.get('dual_score', 0.0) for item in batch], 
                                    dtype=torch.float, device=outputs['logits'].device)
                
                # ListMLE + Focal loss
                loss = self.listwise_loss(outputs['logits'].squeeze().unsqueeze(0), 
                                        labels.unsqueeze(0))
                
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            self.history['listwise_loss'].append(avg_loss)
            logger.info(f"ListMLE Epoch {epoch+1}, Loss: {avg_loss:.4f}")
    
    def _calibrate_model(self, val_data):
        """æ ¡å‡†æ¨¡å‹è¾“å‡º"""
        confidences = []
        accuracies = []
        
        self.model.eval()
        with torch.no_grad():
            for batch in val_data:
                features = self._extract_features(batch)
                outputs = self.model(**features)
                
                probs = torch.sigmoid(outputs['calibrated_logits']).cpu().numpy()
                labels = np.array([item.get('compliance_score', 0) > 0.8 for item in batch])
                
                confidences.extend(probs.flatten())
                accuracies.extend(labels)
        
        # è®­ç»ƒç­‰æ¸—æ ¡å‡†å™¨
        self.calibrator.fit(np.array(confidences), np.array(accuracies))
        logger.info("æ ¡å‡†å™¨è®­ç»ƒå®Œæˆ")
    
    def _extract_features(self, batch):
        """æå–ç‰¹å¾ï¼ˆä¸Step5é›†æˆï¼‰"""
        # è¿™é‡Œåº”è¯¥ä»å®é™…çš„Step5è¾“å‡ºä¸­æå–ç‰¹å¾
        # å½“å‰ä¸ºæ¼”ç¤ºç”¨çš„mockæ•°æ®
        return {
            'clip_img': torch.randn(len(batch), 1024),
            'clip_text': torch.randn(len(batch), 1024), 
            'visual_features': torch.randn(len(batch), 8),
            'conflict_features': torch.randn(len(batch), 5)
        }
    
    def _save_stable_model(self, save_dir):
        """ä¿å­˜ç¨³å¥æ¨¡å‹"""
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
            'calibrator': self.calibrator,
            'history': self.history
        }, save_path / 'stable_model.pt')
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {save_path / 'stable_model.pt'}")

# æ¼”ç¤ºä½¿ç”¨
if __name__ == "__main__":
    config = StableConfig(
        hidden_dim=256,
        num_layers=2, 
        total_epochs=12,
        top_m_candidates=20
    )
    
    model = create_stable_model(config)
    
    print("ğŸš€ CoTRR-Stable ç¨³å¥æ¨¡å‹åˆ›å»ºæˆåŠŸ!")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ’¾ Hidden dim: {config.hidden_dim}")
    print(f"ğŸ¯ æ€§èƒ½ç›®æ ‡: C@1 +{config.min_compliance_gain}pts, nDCG@10 +{config.min_ndcg_gain}pts")
    print(f"âš¡ Top-Mç­–ç•¥: ä»…å‰{config.top_m_candidates}å€™é€‰ä½¿ç”¨å¤æ‚æ¨ç†")
    print(f"ğŸ”¬ MCé‡‡æ ·: {config.mc_samples}æ¬¡")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 16
    test_input = {
        'clip_img': torch.randn(batch_size, 1024),
        'clip_text': torch.randn(batch_size, 1024),
        'visual_features': torch.randn(batch_size, 8), 
        'conflict_features': torch.randn(batch_size, 5)
    }
    
    with torch.no_grad():
        output = model(**test_input)
        print(f"âœ… æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output['logits'].shape}")
        print(f"ğŸŒ¡ï¸ åˆå§‹æ¸©åº¦: {output['temperature'].item():.3f}")
        
        # æµ‹è¯•MCé‡‡æ ·
        mc_output = model(**test_input, mc_samples=config.mc_samples)
        print(f"ğŸ² MCä¸ç¡®å®šæ€§å½¢çŠ¶: {mc_output['uncertainty'].shape}")
    
    print("\nğŸ“ ä½¿ç”¨æ–¹å¼:")
    print("1. pipeline = StableTrainingPipeline(config)")
    print("2. pipeline.train_stable_pipeline(train_data, val_data, save_dir)")
    print("3. A/Bæµ‹è¯•å‡†å…¥æ£€æŸ¥")
    
    print("âœ… CoTRR-Stable å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…çœŸå®æ•°æ®ï¼")
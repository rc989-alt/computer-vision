#!/usr/bin/env python3
"""
CoTRR-Stableä¸Step4/5é›†æˆæ¥å£
æ— ç¼å¯¹æ¥ç°æœ‰pipelineï¼Œå®ç°å¹³æ»‘å‡çº§è·¯å¾„

æ ¸å¿ƒé›†æˆç‚¹:
1. è¯»å–Step5çš„scored.jsonlè¾“å‡º
2. æå–ç°æœ‰ç‰¹å¾ (CLIP + visual + conflict)  
3. è®­ç»ƒç¨³å¥Cross-Attentioné‡æ’å™¨
4. è¾“å‡ºä¸Step4å…¼å®¹çš„é‡æ’ç»“æœ
5. A/Bæµ‹è¯•å°±ç»ªçš„éƒ¨ç½²æ¥å£
"""

import torch
import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import pandas as pd
from tqdm import tqdm

# å¯¼å…¥ç¨³å¥æ¨¡å‹
from cotrr_stable import (
    StableCrossAttnReranker, StableConfig, StableTrainingPipeline,
    create_stable_model, IsotonicCalibrator
)

logger = logging.getLogger(__name__)

class Step5DataLoader:
    """Step5æ•°æ®åŠ è½½å™¨ - è¯»å–scored.jsonl"""
    
    def __init__(self, scored_jsonl_path: str):
        self.scored_jsonl_path = scored_jsonl_path
        self.data = []
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½Step5è¾“å‡ºçš„scored.jsonl"""
        logger.info(f"åŠ è½½Step5æ•°æ®: {self.scored_jsonl_path}")
        
        with open(self.scored_jsonl_path, 'r') as f:
            for line in f:
                item = json.loads(line.strip())
                self.data.append(item)
        
        logger.info(f"åŠ è½½äº† {len(self.data)} ä¸ªæ ·æœ¬")
        
        # æ•°æ®ç»Ÿè®¡
        self._analyze_data()
    
    def _analyze_data(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        if not self.data:
            return
            
        # ç»Ÿè®¡å„å­—æ®µè¦†ç›–ç‡
        sample = self.data[0]
        logger.info(f"æ•°æ®æ ·ä¾‹å­—æ®µ: {list(sample.keys())}")
        
        # ç»Ÿè®¡queryåˆ†å¸ƒ
        query_counts = {}
        compliance_scores = []
        
        for item in self.data:
            query = item.get('query', 'unknown')
            query_counts[query] = query_counts.get(query, 0) + 1
            
            if 'compliance_score' in item:
                compliance_scores.append(item['compliance_score'])
        
        logger.info(f"Queryç§ç±»æ•°: {len(query_counts)}")
        logger.info(f"å¹³å‡æ¯queryæ ·æœ¬æ•°: {len(self.data) / len(query_counts):.1f}")
        
        if compliance_scores:
            logger.info(f"Complianceåˆ†æ•°åˆ†å¸ƒ: "
                       f"mean={np.mean(compliance_scores):.3f}, "
                       f"std={np.std(compliance_scores):.3f}")
    
    def extract_features_for_training(self) -> Dict[str, torch.Tensor]:
        """æå–è®­ç»ƒç‰¹å¾"""
        features = {
            'clip_img': [],
            'clip_text': [],
            'visual_features': [],
            'conflict_features': [],
            'compliance_scores': [],
            'dual_scores': [],
            'queries': [],
            'canonical_ids': []
        }
        
        for item in self.data:
            # CLIPç‰¹å¾ (å¦‚æœå­˜åœ¨çš„è¯)
            clip_img = item.get('clip_img_features', np.random.randn(1024))  # Fallback
            clip_text = item.get('clip_text_features', np.random.randn(1024))
            
            # Visualç‰¹å¾ (æ¥è‡ªStep5)
            visual = np.array([
                item.get('subject_ratio', 0.5),
                item.get('object_area', 0.3), 
                item.get('aspect_ratio', 1.0),
                item.get('brightness', 0.5),
                item.get('contrast', 0.5),
                item.get('saturation', 0.5),
                item.get('hue_variance', 0.1),
                item.get('edge_density', 0.2)
            ])
            
            # Conflictç‰¹å¾
            conflict = np.array([
                item.get('color_delta_e', 0.0),
                item.get('temperature_conflict', 0.0),
                item.get('clarity_conflict', 0.0),
                item.get('garnish_mismatch', 0.0),
                item.get('overall_conflict_prob', 0.0)
            ])
            
            features['clip_img'].append(clip_img)
            features['clip_text'].append(clip_text)
            features['visual_features'].append(visual)
            features['conflict_features'].append(conflict)
            features['compliance_scores'].append(item.get('compliance_score', 0.0))
            features['dual_scores'].append(item.get('dual_score', 0.0))
            features['queries'].append(item.get('query', ''))
            features['canonical_ids'].append(item.get('canonical_id', ''))
        
        # è½¬æ¢ä¸ºtensor
        for key in ['clip_img', 'clip_text', 'visual_features', 'conflict_features']:
            features[key] = torch.tensor(np.array(features[key]), dtype=torch.float32)
        
        return features
    
    def create_query_groups(self) -> Dict[str, List[int]]:
        """æŒ‰queryåˆ†ç»„ï¼Œç”¨äºlistwiseè®­ç»ƒ"""
        query_groups = {}
        
        for idx, item in enumerate(self.data):
            query = item.get('query', 'unknown')
            if query not in query_groups:
                query_groups[query] = []
            query_groups[query].append(idx)
        
        # è¿‡æ»¤æ‰æ ·æœ¬è¿‡å°‘çš„query
        query_groups = {q: indices for q, indices in query_groups.items() 
                       if len(indices) >= 3}  # è‡³å°‘3ä¸ªæ ·æœ¬æ‰èƒ½åšranking
        
        logger.info(f"æœ‰æ•ˆqueryç»„æ•°: {len(query_groups)}")
        return query_groups

class CoTRRStableIntegration:
    """CoTRR-Stableä¸Step4/5çš„é›†æˆæ¥å£"""
    
    def __init__(self, config: Optional[StableConfig] = None):
        if config is None:
            config = StableConfig()
        
        self.config = config
        self.model = None
        self.calibrator = None
        self.training_pipeline = None
    
    def train_from_step5_output(self, 
                               scored_jsonl_path: str,
                               val_split: float = 0.2,
                               save_dir: str = "research/models/stable"):
        """
        ä»Step5è¾“å‡ºè®­ç»ƒç¨³å¥æ¨¡å‹
        
        Args:
            scored_jsonl_path: Step5è¾“å‡ºçš„scored.jsonlè·¯å¾„
            val_split: éªŒè¯é›†æ¯”ä¾‹
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        logger.info("ğŸš€ å¼€å§‹CoTRR-Stableè®­ç»ƒé›†æˆ")
        
        # 1. åŠ è½½Step5æ•°æ®
        data_loader = Step5DataLoader(scored_jsonl_path)
        features = data_loader.extract_features_for_training()
        query_groups = data_loader.create_query_groups()
        
        # 2. æ•°æ®åˆ†å‰²
        train_data, val_data = self._split_data(features, query_groups, val_split)
        
        # 3. åˆ›å»ºè®­ç»ƒpipeline
        self.training_pipeline = StableTrainingPipeline(self.config)
        
        # 4. å¼€å§‹è®­ç»ƒ
        self.training_pipeline.train_stable_pipeline(train_data, val_data, save_dir)
        
        # 5. ä¿å­˜é›†æˆé…ç½®
        self._save_integration_config(save_dir, scored_jsonl_path)
        
        logger.info("âœ… CoTRR-Stableè®­ç»ƒå®Œæˆ")
        
        return self.training_pipeline
    
    def _split_data(self, features: Dict, query_groups: Dict, val_split: float):
        """æŒ‰queryåˆ†å‰²è®­ç»ƒ/éªŒè¯é›†"""
        queries = list(query_groups.keys())
        np.random.shuffle(queries)
        
        val_size = int(len(queries) * val_split)
        val_queries = set(queries[:val_size])
        train_queries = set(queries[val_size:])
        
        # åˆ›å»ºè®­ç»ƒ/éªŒè¯æ•°æ®
        train_indices = []
        val_indices = []
        
        for query, indices in query_groups.items():
            if query in val_queries:
                val_indices.extend(indices)
            else:
                train_indices.extend(indices)
        
        # æå–å¯¹åº”æ•°æ®
        train_data = self._extract_subset(features, train_indices)
        val_data = self._extract_subset(features, val_indices)
        
        logger.info(f"è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬")
        logger.info(f"éªŒè¯é›†: {len(val_indices)} æ ·æœ¬")
        
        return train_data, val_data
    
    def _extract_subset(self, features: Dict, indices: List[int]) -> List[Dict]:
        """æå–æ•°æ®å­é›†"""
        subset = []
        
        for idx in indices:
            item = {
                'clip_img': features['clip_img'][idx],
                'clip_text': features['clip_text'][idx],
                'visual_features': features['visual_features'][idx],
                'conflict_features': features['conflict_features'][idx],
                'compliance_score': features['compliance_scores'][idx],
                'dual_score': features['dual_scores'][idx],
                'query': features['queries'][idx],
                'canonical_id': features['canonical_ids'][idx]
            }
            subset.append(item)
        
        return subset
    
    def _save_integration_config(self, save_dir: str, scored_jsonl_path: str):
        """ä¿å­˜é›†æˆé…ç½®"""
        config_path = Path(save_dir) / 'integration_config.json'
        
        integration_config = {
            'model_type': 'CoTRR-Stable',
            'source_data': scored_jsonl_path,
            'model_config': {
                'hidden_dim': self.config.hidden_dim,
                'num_layers': self.config.num_layers,
                'num_attention_heads': self.config.num_attention_heads,
                'top_m_candidates': self.config.top_m_candidates,
                'mc_samples': self.config.mc_samples
            },
            'performance_targets': {
                'min_compliance_gain': self.config.min_compliance_gain,
                'min_ndcg_gain': self.config.min_ndcg_gain,
                'max_latency_p95': self.config.max_latency_p95
            }
        }
        
        with open(config_path, 'w') as f:
            json.dump(integration_config, f, indent=2)
        
        logger.info(f"é›†æˆé…ç½®å·²ä¿å­˜: {config_path}")
    
    def load_trained_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location='cpu')
        
        self.model = create_stable_model(checkpoint['config'])
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.calibrator = checkpoint['calibrator']
        
        logger.info(f"æ¨¡å‹å·²åŠ è½½: {model_path}")
    
    def rerank_step4_candidates(self, 
                               candidates: List[Dict],
                               query: str,
                               top_k: int = 10) -> List[Dict]:
        """
        é‡æ’Step4å€™é€‰ç»“æœ - A/Bæµ‹è¯•æ¥å£
        
        Args:
            candidates: Step4è¾“å‡ºçš„å€™é€‰åˆ—è¡¨
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ
            
        Returns:
            é‡æ’åçš„å€™é€‰åˆ—è¡¨ï¼ŒåŒ…å«æ–°çš„confidence score
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_trained_model()")
        
        if len(candidates) == 0:
            return candidates
        
        # åªå¯¹Top-Må€™é€‰è¿›è¡Œå¤æ‚æ¨ç†
        top_m = min(self.config.top_m_candidates, len(candidates))
        top_candidates = candidates[:top_m]
        remaining_candidates = candidates[top_m:]
        
        # æå–ç‰¹å¾
        features = self._extract_candidates_features(top_candidates, query)
        
        # æ¨¡å‹æ¨ç†
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(**features, mc_samples=self.config.mc_samples)
        
        # æ ¡å‡†æ¦‚ç‡
        calibrated_probs = self._calibrate_probabilities(outputs)
        
        # é‡æ’Top-M
        reranked_candidates = []
        for i, candidate in enumerate(top_candidates):
            candidate_copy = candidate.copy()
            candidate_copy['cotrr_stable_score'] = float(calibrated_probs[i])
            candidate_copy['cotrr_stable_uncertainty'] = float(outputs['uncertainty'][i])
            reranked_candidates.append(candidate_copy)
        
        # æŒ‰æ–°åˆ†æ•°æ’åº
        reranked_candidates.sort(key=lambda x: x['cotrr_stable_score'], reverse=True)
        
        # åˆå¹¶ç»“æœ
        final_results = reranked_candidates[:top_k]
        
        # å¦‚æœtop_k > top_mï¼Œæ·»åŠ å‰©ä½™å€™é€‰
        if top_k > top_m:
            remaining_needed = top_k - len(final_results)
            final_results.extend(remaining_candidates[:remaining_needed])
        
        return final_results
    
    def _extract_candidates_features(self, candidates: List[Dict], query: str) -> Dict[str, torch.Tensor]:
        """ä»å€™é€‰ä¸­æå–ç‰¹å¾"""
        batch_size = len(candidates)
        
        features = {
            'clip_img': torch.zeros(batch_size, 1024),
            'clip_text': torch.zeros(batch_size, 1024),
            'visual_features': torch.zeros(batch_size, 8),
            'conflict_features': torch.zeros(batch_size, 5)
        }
        
        for i, candidate in enumerate(candidates):
            # å®é™…åº”è¯¥ä»candidateä¸­æå–çœŸå®ç‰¹å¾
            # è¿™é‡Œä½¿ç”¨mockæ•°æ®æ¼”ç¤º
            features['clip_img'][i] = torch.randn(1024)
            features['clip_text'][i] = torch.randn(1024) 
            features['visual_features'][i] = torch.randn(8)
            features['conflict_features'][i] = torch.randn(5)
        
        return features
    
    def _calibrate_probabilities(self, outputs: Dict) -> np.ndarray:
        """æ ¡å‡†æ¦‚ç‡è¾“å‡º"""
        raw_probs = torch.sigmoid(outputs['calibrated_logits']).cpu().numpy().flatten()
        
        if self.calibrator and self.calibrator.fitted:
            calibrated_probs = self.calibrator.calibrate(raw_probs)
        else:
            calibrated_probs = raw_probs
        
        return calibrated_probs
    
    def evaluate_against_baseline(self, 
                                 test_data_path: str,
                                 baseline_model_path: Optional[str] = None) -> Dict[str, float]:
        """
        ä¸baselineæ¨¡å‹å¯¹æ¯”è¯„æµ‹
        
        Returns:
            æ€§èƒ½æå‡æŒ‡æ ‡
        """
        logger.info("ğŸ” å¼€å§‹baselineå¯¹æ¯”è¯„æµ‹")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_loader = Step5DataLoader(test_data_path)
        test_features = test_loader.extract_features_for_training()
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._compute_ranking_metrics(test_features)
        
        logger.info(f"è¯„æµ‹ç»“æœ: {metrics}")
        return metrics
    
    def _compute_ranking_metrics(self, features: Dict) -> Dict[str, float]:
        """è®¡ç®—æ’åºæŒ‡æ ‡"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦å®Œæ•´çš„è¯„æµ‹pipeline
        self.model.eval()
        
        with torch.no_grad():
            outputs = self.model(
                features['clip_img'], 
                features['clip_text'],
                features['visual_features'],
                features['conflict_features']
            )
        
        # è®¡ç®—ç®€å•æŒ‡æ ‡
        predictions = torch.sigmoid(outputs['calibrated_logits']).cpu().numpy()
        labels = np.array(features['compliance_scores']) > 0.8
        
        # ç®€å•å‡†ç¡®ç‡
        accuracy = np.mean((predictions > 0.5) == labels)
        
        return {
            'accuracy': accuracy,
            'mean_confidence': np.mean(predictions),
            'calibration_error': 0.0  # éœ€è¦å®ç°ECEè®¡ç®—
        }

def create_step5_integration_demo():
    """åˆ›å»ºStep5é›†æˆæ¼”ç¤º"""
    
    # åˆ›å»ºmockçš„scored.jsonlç”¨äºæ¼”ç¤º
    mock_data_path = "research/data/mock_scored.jsonl"
    Path("research/data").mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆmockæ•°æ®
    mock_data = []
    queries = ["martini", "manhattan", "old fashioned", "negroni"]
    
    for i in range(200):  # 200ä¸ªæ ·æœ¬
        query = np.random.choice(queries)
        item = {
            'canonical_id': f'item_{i:04d}',
            'query': query,
            'compliance_score': np.random.beta(2, 2),  # 0-1ä¹‹é—´çš„åˆ†æ•°
            'dual_score': np.random.beta(3, 2),
            'conflict_probability': np.random.beta(1, 3),
            'subject_ratio': np.random.uniform(0.2, 0.8),
            'object_area': np.random.uniform(0.1, 0.6),
            'color_delta_e': np.random.exponential(2.0),
            'temperature_conflict': np.random.uniform(0, 1),
            'clarity_conflict': np.random.uniform(0, 1)
        }
        mock_data.append(item)
    
    # ä¿å­˜mockæ•°æ®
    with open(mock_data_path, 'w') as f:
        for item in mock_data:
            f.write(json.dumps(item) + '\n')
    
    logger.info(f"Mockæ•°æ®å·²åˆ›å»º: {mock_data_path}")
    
    return mock_data_path

def main():
    """æ¼”ç¤ºé›†æˆæµç¨‹"""
    logger.info("ğŸš€ CoTRR-Stableé›†æˆæ¼”ç¤º")
    
    # 1. åˆ›å»ºmockæ•°æ®
    mock_data_path = create_step5_integration_demo()
    
    # 2. åˆ›å»ºé›†æˆæ¥å£
    integration = CoTRRStableIntegration()
    
    # 3. ä»Step5æ•°æ®è®­ç»ƒ
    # training_pipeline = integration.train_from_step5_output(
    #     scored_jsonl_path=mock_data_path,
    #     save_dir="research/models/stable_demo"
    # )
    
    print("âœ… CoTRR-Stableé›†æˆæ¥å£åˆ›å»ºæˆåŠŸ!")
    print(f"ğŸ“„ Mockæ•°æ®: {mock_data_path}")
    print(f"ğŸ¯ æ€§èƒ½ç›®æ ‡: C@1 +{integration.config.min_compliance_gain}pts")
    print(f"âš¡ Top-Mç­–ç•¥: ä»…å‰{integration.config.top_m_candidates}å€™é€‰å¤æ‚æ¨ç†")
    
    print("\nğŸ“ ä½¿ç”¨æ–¹å¼:")
    print("1. integration.train_from_step5_output(scored_jsonl_path)")
    print("2. integration.load_trained_model(model_path)")  
    print("3. results = integration.rerank_step4_candidates(candidates, query)")
    print("4. integration.evaluate_against_baseline(test_data_path)")
    
    print("\nğŸ”— A/Bæµ‹è¯•å°±ç»ª:")
    print("- rerank_step4_candidates() å¯ç›´æ¥æ’å…¥ç°æœ‰pipeline")
    print("- æ”¯æŒshadow modeå’Œé€æ­¥rollout")
    print("- åŒ…å«uncertainty scoreç”¨äºç½®ä¿¡åº¦è¿‡æ»¤")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main()
#!/usr/bin/env python3
"""
CoTRR-Pro Advanced Evaluation Suite
åŸºäºæœ€æ–°CVPR/ICCVæ ‡å‡†çš„å®Œæ•´è¯„æµ‹æ¡†æ¶

åŒ…å«:
1. æ’åºæŒ‡æ ‡: Compliance@K, nDCG@K with bootstrap CI
2. æ ¡å‡†æŒ‡æ ‡: ECE, MCE, Brier Score, Reliability Diagram  
3. ä¸ç¡®å®šæ€§: Entropy, MI, OOD detection
4. æ¶ˆèç ”ç©¶: æ¨¡å—è´¡çŒ®åˆ†æ
5. å¤±è´¥åˆ†æ: é”™è¯¯ç±»å‹åˆ†ç±» + å¯è§†åŒ–
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    roc_auc_score, brier_score_loss,
    confusion_matrix, classification_report
)
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹
from cotrr_pro_transformer import CoTRRProTransformer, ModelConfig, create_model

logger = logging.getLogger(__name__)

@dataclass
class EvaluationConfig:
    """è¯„æµ‹é…ç½®"""
    # Bootstrapè®¾ç½®
    bootstrap_samples: int = 1000
    confidence_level: float = 0.95
    
    # Monte Carloä¸ç¡®å®šæ€§
    mc_samples: int = 20
    
    # æ’åºè¯„æµ‹
    k_values: List[int] = None  # Compliance@K, nDCG@K
    
    # æ ¡å‡†è¯„æµ‹
    calibration_bins: int = 10
    
    # è®¾å¤‡
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ç»“æœä¿å­˜
    save_dir: str = "research/evaluation"
    
    def __post_init__(self):
        if self.k_values is None:
            self.k_values = [1, 3, 5, 10]

class BootstrapMetrics:
    """Bootstrapç½®ä¿¡åŒºé—´è®¡ç®—"""
    
    @staticmethod
    def bootstrap_ci(data: np.ndarray, metric_func, confidence_level: float = 0.95,
                     n_bootstrap: int = 1000) -> Tuple[float, float, float]:
        """
        è®¡ç®—æŒ‡æ ‡çš„bootstrapç½®ä¿¡åŒºé—´
        
        Returns:
            (mean, lower_bound, upper_bound)
        """
        bootstrap_metrics = []
        n_samples = len(data)
        
        for _ in range(n_bootstrap):
            # é‡é‡‡æ ·
            indices = np.random.choice(n_samples, size=n_samples, replace=True)
            bootstrap_sample = data[indices]
            
            # è®¡ç®—æŒ‡æ ‡
            metric_value = metric_func(bootstrap_sample)
            bootstrap_metrics.append(metric_value)
        
        bootstrap_metrics = np.array(bootstrap_metrics)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        alpha = 1 - confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        mean_val = np.mean(bootstrap_metrics)
        lower_bound = np.percentile(bootstrap_metrics, lower_percentile)
        upper_bound = np.percentile(bootstrap_metrics, upper_percentile)
        
        return mean_val, lower_bound, upper_bound

class RankingMetrics:
    """æ’åºæŒ‡æ ‡è®¡ç®—"""
    
    @staticmethod
    def compliance_at_k(predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """
        Compliance@K: å‰Kä¸ªç»“æœä¸­ç¬¦åˆè¦æ±‚çš„æ¯”ä¾‹
        
        Args:
            predictions: [n_queries, n_items] - é¢„æµ‹åˆ†æ•°
            targets: [n_queries, n_items] - äºŒåˆ†ç±»æ ‡ç­¾ (1=ç¬¦åˆ, 0=ä¸ç¬¦åˆ)
            k: å‰Kä¸ª
        """
        assert predictions.shape == targets.shape
        n_queries = predictions.shape[0]
        
        compliant_at_k = 0.0
        
        for i in range(n_queries):
            # æŒ‰é¢„æµ‹åˆ†æ•°æ’åº
            sorted_indices = np.argsort(predictions[i])[::-1]  # é™åº
            top_k_indices = sorted_indices[:k]
            
            # è®¡ç®—å‰Kä¸ªä¸­çš„compliance
            top_k_targets = targets[i][top_k_indices]
            compliance = np.sum(top_k_targets) / k
            compliant_at_k += compliance
        
        return compliant_at_k / n_queries
    
    @staticmethod
    def ndcg_at_k(predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """
        Normalized Discounted Cumulative Gain@K
        
        Args:
            predictions: [n_queries, n_items] - é¢„æµ‹åˆ†æ•°
            targets: [n_queries, n_items] - ç›¸å…³æ€§åˆ†æ•° (è¿ç»­å€¼)
        """
        assert predictions.shape == targets.shape
        n_queries = predictions.shape[0]
        
        ndcg_scores = []
        
        for i in range(n_queries):
            # DCG@K
            sorted_indices = np.argsort(predictions[i])[::-1]
            top_k_indices = sorted_indices[:k]
            top_k_targets = targets[i][top_k_indices]
            
            dcg = 0.0
            for j, relevance in enumerate(top_k_targets):
                dcg += (2**relevance - 1) / np.log2(j + 2)
            
            # IDCG@K (ç†æƒ³æ’åº)
            ideal_sorted = np.sort(targets[i])[::-1][:k]
            idcg = 0.0
            for j, relevance in enumerate(ideal_sorted):
                idcg += (2**relevance - 1) / np.log2(j + 2)
            
            # NDCG
            if idcg > 0:
                ndcg = dcg / idcg
            else:
                ndcg = 0.0
            
            ndcg_scores.append(ndcg)
        
        return np.mean(ndcg_scores)

class CalibrationMetrics:
    """æ ¡å‡†æŒ‡æ ‡è®¡ç®—"""
    
    @staticmethod
    def expected_calibration_error(confidences: np.ndarray, accuracies: np.ndarray,
                                   n_bins: int = 10) -> float:
        """Expected Calibration Error (ECE)"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0.0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            # æ‰¾åˆ°åœ¨å½“å‰binä¸­çš„æ ·æœ¬
            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                accuracy_in_bin = accuracies[in_bin].mean()
                avg_confidence_in_bin = confidences[in_bin].mean()
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
    
    @staticmethod
    def maximum_calibration_error(confidences: np.ndarray, accuracies: np.ndarray,
                                  n_bins: int = 10) -> float:
        """Maximum Calibration Error (MCE)"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        mce = 0.0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                accuracy_in_bin = accuracies[in_bin].mean()
                avg_confidence_in_bin = confidences[in_bin].mean()
                bin_error = np.abs(avg_confidence_in_bin - accuracy_in_bin)
                mce = max(mce, bin_error)
        
        return mce
    
    @staticmethod
    def reliability_diagram_data(confidences: np.ndarray, accuracies: np.ndarray,
                                n_bins: int = 10) -> Dict[str, np.ndarray]:
        """Reliability diagramæ•°æ®"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        bin_centers = []
        bin_accuracies = []
        bin_counts = []
        
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                bin_centers.append((bin_lower + bin_upper) / 2)
                bin_accuracies.append(accuracies[in_bin].mean())
                bin_counts.append(in_bin.sum())
            else:
                bin_centers.append((bin_lower + bin_upper) / 2)
                bin_accuracies.append(0.0)
                bin_counts.append(0)
        
        return {
            'bin_centers': np.array(bin_centers),
            'bin_accuracies': np.array(bin_accuracies),
            'bin_counts': np.array(bin_counts)
        }

class UncertaintyMetrics:
    """ä¸ç¡®å®šæ€§æŒ‡æ ‡"""
    
    @staticmethod
    def predictive_entropy(probs: np.ndarray) -> np.ndarray:
        """é¢„æµ‹ç†µï¼ˆè¡¨ç¤ºä¸ç¡®å®šæ€§ï¼‰"""
        # é¿å…log(0)
        probs = np.clip(probs, 1e-8, 1 - 1e-8)
        entropy = -np.sum(probs * np.log(probs), axis=-1)
        return entropy
    
    @staticmethod
    def mutual_information(mc_probs: np.ndarray) -> np.ndarray:
        """
        äº’ä¿¡æ¯ï¼ˆè¡¨ç¤ºè®¤çŸ¥ä¸ç¡®å®šæ€§ï¼‰
        
        Args:
            mc_probs: [n_samples, n_mc_samples, n_classes] - MC dropoutæ¦‚ç‡
        """
        # å¹³å‡é¢„æµ‹
        mean_probs = np.mean(mc_probs, axis=1)
        
        # é¢„æµ‹ç†µï¼ˆæ€»ä¸ç¡®å®šæ€§ï¼‰
        predictive_entropy = UncertaintyMetrics.predictive_entropy(mean_probs)
        
        # æœŸæœ›ç†µï¼ˆå¶ç„¶ä¸ç¡®å®šæ€§ï¼‰
        expected_entropy = np.mean([
            UncertaintyMetrics.predictive_entropy(mc_probs[:, i, :])
            for i in range(mc_probs.shape[1])
        ], axis=0)
        
        # äº’ä¿¡æ¯ = é¢„æµ‹ç†µ - æœŸæœ›ç†µ
        mutual_info = predictive_entropy - expected_entropy
        
        return mutual_info

class CoTRRProEvaluator:
    """CoTRR-Proå®Œæ•´è¯„æµ‹å™¨"""
    
    def __init__(self, model: CoTRRProTransformer, config: EvaluationConfig):
        self.model = model
        self.config = config
        self.model.eval()
        
        # åˆ›å»ºç»“æœç›®å½•
        self.save_dir = Path(config.save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯„æµ‹ç»“æœå­˜å‚¨
        self.results = {}
        
    def evaluate_dataset(self, data_path: str, dataset_name: str = "test") -> Dict[str, Any]:
        """å®Œæ•´æ•°æ®é›†è¯„æµ‹"""
        logger.info(f"ğŸ” Evaluating {dataset_name} dataset: {data_path}")
        
        # åŠ è½½æ•°æ®
        data = self._load_data(data_path)
        
        # è·å–é¢„æµ‹ç»“æœ
        predictions = self._get_predictions(data)
        
        # è®¡ç®—å„ç±»æŒ‡æ ‡
        ranking_results = self._evaluate_ranking_metrics(predictions, data)
        calibration_results = self._evaluate_calibration_metrics(predictions, data)
        uncertainty_results = self._evaluate_uncertainty_metrics(predictions, data)
        
        # å¤±è´¥åˆ†æ
        failure_analysis = self._analyze_failures(predictions, data)
        
        # æ•´åˆç»“æœ
        results = {
            'dataset': dataset_name,
            'n_samples': len(data),
            'ranking_metrics': ranking_results,
            'calibration_metrics': calibration_results,
            'uncertainty_metrics': uncertainty_results,
            'failure_analysis': failure_analysis
        }
        
        self.results[dataset_name] = results
        
        # ä¿å­˜ç»“æœ
        self._save_results(results, dataset_name)
        
        logger.info(f"âœ… {dataset_name} evaluation completed")
        return results
    
    def ablation_study(self, data_path: str, model_variants: Dict[str, CoTRRProTransformer]) -> Dict[str, Any]:
        """æ¶ˆèç ”ç©¶"""
        logger.info("ğŸ§ª Running ablation study")
        
        data = self._load_data(data_path)
        ablation_results = {}
        
        # è¯„æµ‹æ¯ä¸ªæ¨¡å‹å˜ä½“
        for variant_name, model in model_variants.items():
            logger.info(f"Evaluating {variant_name}")
            
            # ä¸´æ—¶æ›¿æ¢æ¨¡å‹
            original_model = self.model
            self.model = model
            
            # è·å–é¢„æµ‹ç»“æœ
            predictions = self._get_predictions(data)
            
            # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
            ranking_results = self._evaluate_ranking_metrics(predictions, data)
            
            ablation_results[variant_name] = {
                'compliance_at_1': ranking_results['compliance_at_1']['mean'],
                'compliance_at_3': ranking_results['compliance_at_3']['mean'],
                'ndcg_at_10': ranking_results['ndcg_at_10']['mean'],
                'n_params': sum(p.numel() for p in model.parameters())
            }
            
            # æ¢å¤åŸæ¨¡å‹
            self.model = original_model
        
        # åˆ›å»ºå¯¹æ¯”è¡¨
        ablation_df = pd.DataFrame(ablation_results).T
        ablation_df['compliance_gain'] = ablation_df['compliance_at_1'] - ablation_df.iloc[0]['compliance_at_1']
        ablation_df['ndcg_gain'] = ablation_df['ndcg_at_10'] - ablation_df.iloc[0]['ndcg_at_10']
        
        # ä¿å­˜æ¶ˆèç»“æœ
        ablation_df.to_csv(self.save_dir / 'ablation_study.csv')
        
        logger.info("âœ… Ablation study completed")
        return {
            'results': ablation_results,
            'summary_table': ablation_df.to_dict()
        }
    
    def _load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®"""
        data = []
        with open(data_path, 'r') as f:
            for line in f:
                item = json.loads(line.strip())
                data.append(item)
        return data
    
    def _get_predictions(self, data: List[Dict]) -> Dict[str, np.ndarray]:
        """è·å–æ¨¡å‹é¢„æµ‹"""
        predictions = {
            'logits': [],
            'probs': [],
            'uncertainties': [],
            'mc_samples': []
        }
        
        with torch.no_grad():
            for item in data:
                # æå–ç‰¹å¾
                features = self._extract_features(item)
                
                # æ¨¡å‹é¢„æµ‹ï¼ˆåŒ…å«MC samplesï¼‰
                outputs = self.model(**features, mc_samples=self.config.mc_samples)
                
                # å­˜å‚¨ç»“æœ
                logits = outputs['logits'].cpu().numpy()
                probs = torch.sigmoid(outputs['logits']).cpu().numpy()
                uncertainty = outputs.get('uncertainty', torch.zeros_like(outputs['logits'])).cpu().numpy()
                
                predictions['logits'].append(logits)
                predictions['probs'].append(probs)
                predictions['uncertainties'].append(uncertainty)
                
                if 'mc_samples' in outputs:
                    mc_probs = torch.sigmoid(torch.stack(outputs['mc_samples'])).cpu().numpy()
                    predictions['mc_samples'].append(mc_probs)
        
        # è½¬æ¢ä¸ºnumpy arrays
        for key in predictions:
            if predictions[key]:
                predictions[key] = np.array(predictions[key])
        
        return predictions
    
    def _extract_features(self, item: Dict) -> Dict[str, torch.Tensor]:
        """ä»itemä¸­æå–ç‰¹å¾"""
        # æ¨¡æ‹Ÿç‰¹å¾æå–ï¼ˆå®é™…åº”è¯¥ä»itemä¸­è¯»å–ï¼‰
        features = {
            'clip_img': torch.randn(1, 1024).to(self.config.device),
            'clip_text': torch.randn(1, 1024).to(self.config.device),
            'visual_features': torch.randn(1, 8).to(self.config.device),
            'conflict_features': torch.randn(1, 5).to(self.config.device)
        }
        return features
    
    def _evaluate_ranking_metrics(self, predictions: Dict, data: List[Dict]) -> Dict[str, Any]:
        """è¯„æµ‹æ’åºæŒ‡æ ‡"""
        results = {}
        
        # æŒ‰queryåˆ†ç»„
        query_groups = self._group_by_query(data, predictions)
        
        for k in self.config.k_values:
            # Compliance@K
            compliance_scores = []
            ndcg_scores = []
            
            for query, group_data in query_groups.items():
                items = group_data['items']
                preds = group_data['predictions']
                
                if len(items) >= k:
                    # æå–çœŸå®æ ‡ç­¾
                    compliance_labels = np.array([
                        1 if item.get('compliance_score', 0) > 0.8 else 0 
                        for item in items
                    ])
                    relevance_scores = np.array([
                        item.get('dual_score', 0.0) for item in items
                    ])
                    
                    # è®¡ç®—æŒ‡æ ‡
                    pred_scores = preds.squeeze()
                    
                    compliance_k = RankingMetrics.compliance_at_k(
                        pred_scores.reshape(1, -1), 
                        compliance_labels.reshape(1, -1), 
                        k
                    )
                    ndcg_k = RankingMetrics.ndcg_at_k(
                        pred_scores.reshape(1, -1),
                        relevance_scores.reshape(1, -1),
                        k
                    )
                    
                    compliance_scores.append(compliance_k)
                    ndcg_scores.append(ndcg_k)
            
            # Bootstrapç½®ä¿¡åŒºé—´
            if compliance_scores:
                compliance_mean, compliance_lower, compliance_upper = BootstrapMetrics.bootstrap_ci(
                    np.array(compliance_scores), np.mean, self.config.confidence_level
                )
                results[f'compliance_at_{k}'] = {
                    'mean': compliance_mean,
                    'ci_lower': compliance_lower,
                    'ci_upper': compliance_upper,
                    'samples': len(compliance_scores)
                }
            
            if ndcg_scores:
                ndcg_mean, ndcg_lower, ndcg_upper = BootstrapMetrics.bootstrap_ci(
                    np.array(ndcg_scores), np.mean, self.config.confidence_level
                )
                results[f'ndcg_at_{k}'] = {
                    'mean': ndcg_mean,
                    'ci_lower': ndcg_lower,
                    'ci_upper': ndcg_upper,
                    'samples': len(ndcg_scores)
                }
        
        return results
    
    def _evaluate_calibration_metrics(self, predictions: Dict, data: List[Dict]) -> Dict[str, Any]:
        """è¯„æµ‹æ ¡å‡†æŒ‡æ ‡"""
        probs = predictions['probs'].flatten()
        
        # è·å–çœŸå®conflictæ ‡ç­¾
        conflict_labels = np.array([
            1 if item.get('conflict_probability', 0) > 0.5 else 0
            for item in data
        ])
        
        # æ ¡å‡†æŒ‡æ ‡
        ece = CalibrationMetrics.expected_calibration_error(
            probs, conflict_labels, self.config.calibration_bins
        )
        mce = CalibrationMetrics.maximum_calibration_error(
            probs, conflict_labels, self.config.calibration_bins
        )
        brier = brier_score_loss(conflict_labels, probs)
        
        # Reliability diagramæ•°æ®
        reliability_data = CalibrationMetrics.reliability_diagram_data(
            probs, conflict_labels, self.config.calibration_bins
        )
        
        # AUC
        if len(np.unique(conflict_labels)) > 1:
            auc = roc_auc_score(conflict_labels, probs)
        else:
            auc = 0.0
        
        return {
            'ece': ece,
            'mce': mce,
            'brier_score': brier,
            'auc': auc,
            'reliability_diagram': reliability_data
        }
        
    def _evaluate_uncertainty_metrics(self, predictions: Dict, data: List[Dict]) -> Dict[str, Any]:
        """è¯„æµ‹ä¸ç¡®å®šæ€§æŒ‡æ ‡"""
        if 'mc_samples' not in predictions or len(predictions['mc_samples']) == 0:
            return {'note': 'No MC samples available for uncertainty evaluation'}
        
        mc_probs = predictions['mc_samples']  # [n_samples, n_mc, n_classes]
        
        # é¢„æµ‹ç†µ
        mean_probs = np.mean(mc_probs, axis=1)
        pred_entropy = UncertaintyMetrics.predictive_entropy(mean_probs)
        
        # äº’ä¿¡æ¯
        mutual_info = UncertaintyMetrics.mutual_information(mc_probs)
        
        return {
            'mean_predictive_entropy': np.mean(pred_entropy),
            'std_predictive_entropy': np.std(pred_entropy),
            'mean_mutual_information': np.mean(mutual_info),
            'std_mutual_information': np.std(mutual_info)
        }
    
    def _analyze_failures(self, predictions: Dict, data: List[Dict]) -> Dict[str, Any]:
        """å¤±è´¥åˆ†æ"""
        probs = predictions['probs'].flatten()
        
        # è·å–çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        true_labels = np.array([
            1 if item.get('compliance_score', 0) > 0.8 else 0
            for item in data
        ])
        pred_labels = (probs > 0.5).astype(int)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, pred_labels)
        
        # é”™è¯¯ç±»å‹åˆ†æ
        false_positives = []  # è¯¯ç½š
        false_negatives = []  # æ¼ç½š
        
        for i, (true_label, pred_prob, item) in enumerate(zip(true_labels, probs, data)):
            pred_label = int(pred_prob > 0.5)
            
            if true_label == 0 and pred_label == 1:
                # è¯¯ç½šï¼šå®é™…åˆè§„ä½†è¢«åˆ¤ä¸ºè¿è§„
                false_positives.append({
                    'index': i,
                    'confidence': pred_prob,
                    'item': item,
                    'error_type': 'false_positive'
                })
            elif true_label == 1 and pred_label == 0:
                # æ¼ç½šï¼šå®é™…è¿è§„ä½†è¢«åˆ¤ä¸ºåˆè§„
                false_negatives.append({
                    'index': i,
                    'confidence': pred_prob,
                    'item': item,
                    'error_type': 'false_negative'
                })
        
        # æŒ‰confidenceæ’åºï¼Œæ‰¾åˆ°æœ€ä¸¥é‡çš„é”™è¯¯
        false_positives.sort(key=lambda x: x['confidence'], reverse=True)
        false_negatives.sort(key=lambda x: 1 - x['confidence'], reverse=True)
        
        return {
            'confusion_matrix': cm.tolist(),
            'false_positives': false_positives[:10],  # å‰10ä¸ªæœ€ä¸¥é‡è¯¯ç½š
            'false_negatives': false_negatives[:10],  # å‰10ä¸ªæœ€ä¸¥é‡æ¼ç½š
            'fp_count': len(false_positives),
            'fn_count': len(false_negatives),
            'accuracy': np.mean(true_labels == pred_labels)
        }
    
    def _group_by_query(self, data: List[Dict], predictions: Dict) -> Dict[str, Dict]:
        """æŒ‰queryåˆ†ç»„æ•°æ®"""
        query_groups = {}
        
        for i, item in enumerate(data):
            query = item.get('query', 'unknown')
            
            if query not in query_groups:
                query_groups[query] = {
                    'items': [],
                    'predictions': []
                }
            
            query_groups[query]['items'].append(item)
            query_groups[query]['predictions'].append(predictions['logits'][i])
        
        # è½¬æ¢predictionsä¸ºnumpy arrays
        for query in query_groups:
            query_groups[query]['predictions'] = np.array(query_groups[query]['predictions'])
        
        return query_groups
    
    def _save_results(self, results: Dict, dataset_name: str):
        """ä¿å­˜è¯„æµ‹ç»“æœ"""
        # JSONç»“æœ
        with open(self.save_dir / f'{dataset_name}_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # å¯è§†åŒ–
        self._create_visualizations(results, dataset_name)
        
        logger.info(f"Results saved: {self.save_dir / f'{dataset_name}_results.json'}")
    
    def _create_visualizations(self, results: Dict, dataset_name: str):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Ranking metrics with CI
        if 'ranking_metrics' in results:
            metrics = results['ranking_metrics']
            metric_names = []
            metric_means = []
            metric_cis = []
            
            for key, value in metrics.items():
                if isinstance(value, dict) and 'mean' in value:
                    metric_names.append(key)
                    metric_means.append(value['mean'])
                    ci_width = value['ci_upper'] - value['ci_lower']
                    metric_cis.append(ci_width / 2)
            
            if metric_names:
                axes[0, 0].bar(range(len(metric_names)), metric_means, 
                              yerr=metric_cis, capsize=5, alpha=0.7)
                axes[0, 0].set_xticks(range(len(metric_names)))
                axes[0, 0].set_xticklabels(metric_names, rotation=45)
                axes[0, 0].set_title('Ranking Metrics with 95% CI')
                axes[0, 0].set_ylabel('Score')
        
        # 2. Calibration: Reliability Diagram
        if 'calibration_metrics' in results and 'reliability_diagram' in results['calibration_metrics']:
            rel_data = results['calibration_metrics']['reliability_diagram']
            
            axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')
            axes[0, 1].plot(rel_data['bin_centers'], rel_data['bin_accuracies'], 
                           'o-', label='Model')
            axes[0, 1].set_xlabel('Mean Predicted Probability')
            axes[0, 1].set_ylabel('Fraction of Positives')
            axes[0, 1].set_title('Reliability Diagram')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Calibration metrics
        if 'calibration_metrics' in results:
            cal_metrics = results['calibration_metrics']
            cal_names = ['ECE', 'MCE', 'Brier Score', 'AUC']
            cal_values = [
                cal_metrics.get('ece', 0),
                cal_metrics.get('mce', 0), 
                cal_metrics.get('brier_score', 0),
                cal_metrics.get('auc', 0)
            ]
            
            bars = axes[1, 0].bar(cal_names, cal_values, alpha=0.7)
            axes[1, 0].set_title('Calibration Metrics')
            axes[1, 0].set_ylabel('Score')
            
            # ä¸ºæ¯ä¸ªbaræ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, cal_values):
                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{value:.3f}', ha='center', va='bottom')
        
        # 4. å¤±è´¥åˆ†æ
        if 'failure_analysis' in results:
            failure = results['failure_analysis']
            if 'confusion_matrix' in failure:
                cm = np.array(failure['confusion_matrix'])
                sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 1],
                           xticklabels=['Pred Negative', 'Pred Positive'],
                           yticklabels=['True Negative', 'True Positive'])
                axes[1, 1].set_title('Confusion Matrix')
        
        plt.tight_layout()
        plt.savefig(self.save_dir / f'{dataset_name}_evaluation.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Visualizations saved: {self.save_dir / f'{dataset_name}_evaluation.png'}")

def main():
    """æ¼”ç¤ºè¯„æµ‹æµç¨‹"""
    # é…ç½®
    eval_config = EvaluationConfig(
        bootstrap_samples=100,  # å‡å°‘samplesç”¨äºæ¼”ç¤º
        mc_samples=10,
        k_values=[1, 3, 5],
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    # åˆ›å»ºæ¨¡å‹
    model_config = ModelConfig()
    model = create_model(model_config)
    
    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = CoTRRProEvaluator(model, eval_config)
    
    print("ğŸ” CoTRR-Pro Advanced Evaluator åˆ›å»ºæˆåŠŸ!")
    print(f"ğŸ“Š Bootstrap samples: {eval_config.bootstrap_samples}")
    print(f"ğŸ² MC samples: {eval_config.mc_samples}")
    print(f"ğŸ“± Device: {eval_config.device}")
    print(f"ğŸ’¾ Save dir: {eval_config.save_dir}")
    
    # ç¤ºä¾‹è¯„æµ‹ï¼ˆéœ€è¦çœŸå®æ•°æ®æ–‡ä»¶ï¼‰
    # results = evaluator.evaluate_dataset("data/test.jsonl", "test")
    # print(f"âœ… è¯„æµ‹å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {eval_config.save_dir}")
    
    print("ğŸ“ ä½¿ç”¨ evaluator.evaluate_dataset() å¼€å§‹è¯„æµ‹")
    print("ğŸ§ª ä½¿ç”¨ evaluator.ablation_study() è¿›è¡Œæ¶ˆèç ”ç©¶")

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    main()
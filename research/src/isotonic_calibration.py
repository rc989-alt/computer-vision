#!/usr/bin/env python3
"""
CoTRR-Stable Stage 1 Task T004: Isotonicæ ¡å‡†å®žçŽ°
å®žçŽ°æ¦‚çŽ‡æ ¡å‡†æœºåˆ¶ï¼Œæé«˜æ¨¡åž‹è¾“å‡ºçš„å¯é æ€§

å…³é”®ç‰¹æ€§:
1. Isotonic Regression: éžå‚æ•°åŒ–æ¦‚çŽ‡æ ¡å‡†
2. ECEæŒ‡æ ‡: Expected Calibration Errorè®¡ç®—
3. æ ¡å‡†å™¨ä¿å­˜/åŠ è½½: ç”Ÿäº§çŽ¯å¢ƒæŒä¹…åŒ–
4. å¯è§†åŒ–åˆ†æž: æ ¡å‡†å‰åŽæ•ˆæžœå¯¹æ¯”
"""

import torch
import torch.nn as nn
import numpy as np
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import brier_score_loss, log_loss
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import logging
import pickle
import json
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IsotonicCalibrator:
    """
    Isotonic Regressionæ ¡å‡†å™¨
    ç”¨äºŽå°†æ¨¡åž‹è¾“å‡ºæ ¡å‡†ä¸ºå¯é çš„æ¦‚çŽ‡ä¼°è®¡
    """
    
    def __init__(self, out_of_bounds='clip'):
        self.calibrator = IsotonicRegression(out_of_bounds=out_of_bounds)
        self.is_fitted = False
        self.calibration_metrics = {}
        
    def fit(self, scores: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """
        æ‹Ÿåˆæ ¡å‡†å™¨
        Args:
            scores: æ¨¡åž‹åŽŸå§‹åˆ†æ•° [N,] (logits)
            labels: äºŒå€¼æ ‡ç­¾ [N,] (0/1)
        Returns:
            æ ¡å‡†æŒ‡æ ‡å­—å…¸
        """
        logger.info(f"ðŸŽ¯ å¼€å§‹è®­ç»ƒIsotonicæ ¡å‡†å™¨ (æ ·æœ¬æ•°: {len(scores)})")
        
        # å°†logitsè½¬æ¢ä¸ºæ¦‚çŽ‡
        probs = self._logits_to_probs(scores)
        
        # æ‹Ÿåˆisotonic regression
        self.calibrator.fit(probs, labels)
        self.is_fitted = True
        
        # è®¡ç®—æ ¡å‡†å‰åŽçš„æŒ‡æ ‡
        calibrated_probs = self.calibrator.predict(probs)
        
        # è®¡ç®—å„ç§æ ¡å‡†æŒ‡æ ‡
        metrics = self._compute_calibration_metrics(probs, calibrated_probs, labels)
        self.calibration_metrics = metrics
        
        logger.info(f"ðŸ“Š æ ¡å‡†æ•ˆæžœ:")
        logger.info(f"   ECE: {metrics['original_ece']:.4f} â†’ {metrics['calibrated_ece']:.4f}")
        logger.info(f"   Brier Score: {metrics['original_brier']:.4f} â†’ {metrics['calibrated_brier']:.4f}")
        logger.info(f"   æ”¹å–„åº¦: ECE {metrics['ece_improvement']:.4f}, Brier {metrics['brier_improvement']:.4f}")
        
        return metrics
    
    def predict(self, scores: np.ndarray) -> np.ndarray:
        """
        æ ¡å‡†é¢„æµ‹æ¦‚çŽ‡
        Args:
            scores: æ¨¡åž‹åŽŸå§‹åˆ†æ•° (logits)
        Returns:
            æ ¡å‡†åŽçš„æ¦‚çŽ‡ [0, 1]
        """
        if not self.is_fitted:
            raise ValueError("æ ¡å‡†å™¨æœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()")
        
        probs = self._logits_to_probs(scores)
        return self.calibrator.predict(probs)
    
    def _logits_to_probs(self, logits: np.ndarray) -> np.ndarray:
        """å°†logitsè½¬æ¢ä¸ºæ¦‚çŽ‡"""
        return 1.0 / (1.0 + np.exp(-np.clip(logits, -500, 500)))  # é˜²æ­¢æ•°å€¼æº¢å‡º
    
    def _compute_calibration_metrics(self, 
                                   original_probs: np.ndarray, 
                                   calibrated_probs: np.ndarray, 
                                   labels: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—æ ¡å‡†æŒ‡æ ‡"""
        
        # ECE (Expected Calibration Error)
        original_ece = self._compute_ece(original_probs, labels)
        calibrated_ece = self._compute_ece(calibrated_probs, labels)
        
        # Brier Score
        original_brier = brier_score_loss(labels, original_probs)
        calibrated_brier = brier_score_loss(labels, calibrated_probs)
        
        # Log Loss
        original_logloss = log_loss(labels, np.clip(original_probs, 1e-15, 1-1e-15))
        calibrated_logloss = log_loss(labels, np.clip(calibrated_probs, 1e-15, 1-1e-15))
        
        # å¯é æ€§æŒ‡æ ‡ (Reliability)
        original_reliability = self._compute_reliability(original_probs, labels)
        calibrated_reliability = self._compute_reliability(calibrated_probs, labels)
        
        return {
            'original_ece': original_ece,
            'calibrated_ece': calibrated_ece,
            'ece_improvement': original_ece - calibrated_ece,
            'original_brier': original_brier,
            'calibrated_brier': calibrated_brier,
            'brier_improvement': original_brier - calibrated_brier,
            'original_logloss': original_logloss,
            'calibrated_logloss': calibrated_logloss,
            'logloss_improvement': original_logloss - calibrated_logloss,
            'original_reliability': original_reliability,
            'calibrated_reliability': calibrated_reliability,
            'reliability_improvement': original_reliability - calibrated_reliability
        }
    
    def _compute_ece(self, probs: np.ndarray, labels: np.ndarray, n_bins: int = 10) -> float:
        """
        è®¡ç®—Expected Calibration Error
        """
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0
        total_samples = len(probs)
        
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (probs > bin_lower) & (probs <= bin_upper)
            prop_in_bin = in_bin.sum() / total_samples
            
            if prop_in_bin > 0:
                accuracy_in_bin = labels[in_bin].mean()
                avg_confidence_in_bin = probs[in_bin].mean()
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
    
    def _compute_reliability(self, probs: np.ndarray, labels: np.ndarray, n_bins: int = 10) -> float:
        """è®¡ç®—å¯é æ€§æŒ‡æ ‡"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        reliability = 0
        total_samples = len(probs)
        
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (probs > bin_lower) & (probs <= bin_upper)
            prop_in_bin = in_bin.sum() / total_samples
            
            if prop_in_bin > 0:
                accuracy_in_bin = labels[in_bin].mean()
                avg_confidence_in_bin = probs[in_bin].mean()
                reliability += (avg_confidence_in_bin - accuracy_in_bin) ** 2 * prop_in_bin
        
        return reliability
    
    def plot_calibration_curve(self, 
                              scores: np.ndarray, 
                              labels: np.ndarray, 
                              save_path: Optional[str] = None,
                              n_bins: int = 10):
        """ç»˜åˆ¶æ ¡å‡†æ›²çº¿"""
        if not self.is_fitted:
            raise ValueError("æ ¡å‡†å™¨æœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()")
        
        original_probs = self._logits_to_probs(scores)
        calibrated_probs = self.predict(scores)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # åŽŸå§‹æ ¡å‡†æ›²çº¿
        self._plot_single_calibration(original_probs, labels, ax1, "åŽŸå§‹æ¨¡åž‹", n_bins)
        
        # æ ¡å‡†åŽæ›²çº¿
        self._plot_single_calibration(calibrated_probs, labels, ax2, "æ ¡å‡†åŽæ¨¡åž‹", n_bins)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ðŸ“Š æ ¡å‡†æ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def _plot_single_calibration(self, probs, labels, ax, title, n_bins):
        """ç»˜åˆ¶å•ä¸ªæ ¡å‡†æ›²çº¿"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        bin_centers = []
        bin_accuracies = []
        bin_confidences = []
        bin_counts = []
        
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (probs > bin_lower) & (probs <= bin_upper)
            if in_bin.sum() > 0:
                bin_centers.append((bin_lower + bin_upper) / 2)
                bin_accuracies.append(labels[in_bin].mean())
                bin_confidences.append(probs[in_bin].mean())
                bin_counts.append(in_bin.sum())
        
        # æ ¡å‡†æ›²çº¿
        ax.plot(bin_confidences, bin_accuracies, 'o-', label=f'{title}æ ¡å‡†æ›²çº¿')
        ax.plot([0, 1], [0, 1], 'k--', label='å®Œç¾Žæ ¡å‡†')
        
        # è®¾ç½®å›¾å½¢
        ax.set_xlabel('å¹³å‡é¢„æµ‹æ¦‚çŽ‡')
        ax.set_ylabel('å‡†ç¡®çŽ‡')
        ax.set_title(f'{title}æ ¡å‡†æ›²çº¿')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ECEæ•°å€¼
        ece = self._compute_ece(np.array(bin_confidences), np.array(bin_accuracies))
        ax.text(0.02, 0.98, f'ECE: {ece:.4f}', transform=ax.transAxes, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))
    
    def save(self, path: str):
        """ä¿å­˜æ ¡å‡†å™¨"""
        calibrator_data = {
            'calibrator': self.calibrator,
            'is_fitted': self.is_fitted,
            'metrics': self.calibration_metrics
        }
        
        with open(path, 'wb') as f:
            pickle.dump(calibrator_data, f)
        
        logger.info(f"ðŸ’¾ æ ¡å‡†å™¨å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ ¡å‡†å™¨"""
        with open(path, 'rb') as f:
            calibrator_data = pickle.load(f)
        
        self.calibrator = calibrator_data['calibrator']
        self.is_fitted = calibrator_data['is_fitted']
        self.calibration_metrics = calibrator_data.get('metrics', {})
        
        logger.info(f"ðŸ“‚ æ ¡å‡†å™¨å·²åŠ è½½: {path}")
    
    def get_metrics(self) -> Dict[str, float]:
        """èŽ·å–æ ¡å‡†æŒ‡æ ‡"""
        return self.calibration_metrics.copy()

def test_isotonic_calibrator():
    """å…¨é¢æµ‹è¯•Isotonicæ ¡å‡†å™¨"""
    logger.info("ðŸ§ª å¼€å§‹å…¨é¢æµ‹è¯•Isotonicæ ¡å‡†å™¨")
    
    # ç”Ÿæˆæ›´çœŸå®žçš„æ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 2000
    
    # æ¨¡æ‹ŸçœŸå®žåœºæ™¯ï¼šæ¨¡åž‹è¿‡åº¦è‡ªä¿¡
    # çœŸå®žæ¦‚çŽ‡æœä»ŽBetaåˆ†å¸ƒ
    true_probs = np.random.beta(2, 5, n_samples)  # åå‘ä½Žæ¦‚çŽ‡
    labels = np.random.binomial(1, true_probs)
    
    # æ¨¡æ‹Ÿè¿‡åº¦è‡ªä¿¡çš„æ¨¡åž‹è¾“å‡º (logitsåé«˜)
    noise = np.random.normal(0, 0.8, n_samples)
    confidence_bias = 1.5  # è¿‡åº¦è‡ªä¿¡åå·®
    raw_logits = np.log(true_probs / (1 - true_probs + 1e-8)) + confidence_bias + noise
    
    logger.info(f"æ•°æ®ç»Ÿè®¡: {n_samples}æ ·æœ¬, æ­£ä¾‹æ¯”ä¾‹: {labels.mean():.3f}")
    
    # åˆ›å»ºå¹¶è®­ç»ƒæ ¡å‡†å™¨
    calibrator = IsotonicCalibrator()
    metrics = calibrator.fit(raw_logits, labels)
    
    # æµ‹è¯•é¢„æµ‹åŠŸèƒ½
    test_logits = np.random.normal(0, 2, 500)
    calibrated_probs = calibrator.predict(test_logits)
    
    logger.info(f"æµ‹è¯•é¢„æµ‹: æ ¡å‡†æ¦‚çŽ‡èŒƒå›´ [{calibrated_probs.min():.3f}, {calibrated_probs.max():.3f}]")
    
    # æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
    save_path = "research/stage1_progress/test_calibrator.pkl"
    calibrator.save(save_path)
    
    # åŠ è½½æµ‹è¯•
    new_calibrator = IsotonicCalibrator()
    new_calibrator.load(save_path)
    
    # éªŒè¯åŠ è½½åŽçš„ä¸€è‡´æ€§
    new_probs = new_calibrator.predict(test_logits)
    consistency_check = np.allclose(calibrated_probs, new_probs)
    
    logger.info(f"ä¿å­˜/åŠ è½½ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if consistency_check else 'âŒ å¤±è´¥'}")
    
    # ç”Ÿæˆæ ¡å‡†æ›²çº¿
    try:
        calibrator.plot_calibration_curve(
            raw_logits, labels, 
            save_path="research/stage1_progress/calibration_curve.png"
        )
    except Exception as e:
        logger.warning(f"æ ¡å‡†æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")
    
    # éªŒè¯æŒ‡æ ‡æ”¹å–„
    improvements = {
        'ECEæ”¹å–„': metrics['ece_improvement'] > 0,
        'Brieræ”¹å–„': metrics['brier_improvement'] > 0,
        'ECEè¾¾æ ‡': metrics['calibrated_ece'] < 0.05,  # ç›®æ ‡ECE < 0.05
    }
    
    logger.info("âœ… æ ¡å‡†å™¨æµ‹è¯•å®Œæˆ")
    logger.info("ðŸ“Š æ”¹å–„éªŒè¯:")
    for metric, improved in improvements.items():
        status = "âœ… è¾¾æ ‡" if improved else "âš ï¸ æœªè¾¾æ ‡"
        logger.info(f"   {metric}: {status}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    try:
        Path(save_path).unlink()
        Path("research/stage1_progress/calibration_curve.png").unlink(missing_ok=True)
    except:
        pass
    
    return calibrator, metrics, all(improvements.values())

if __name__ == "__main__":
    logger.info("ðŸš€ å¼€å§‹Task T004: Isotonicæ ¡å‡†å®žçŽ°")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("research/stage1_progress").mkdir(parents=True, exist_ok=True)
    
    # è¿è¡Œå…¨é¢æµ‹è¯•
    calibrator, metrics, all_tests_passed = test_isotonic_calibrator()
    
    if all_tests_passed:
        logger.info("ðŸŽ‰ Task T004å®žçŽ°å®Œæˆï¼")
        logger.info("ðŸ“‹ äº¤ä»˜å†…å®¹:")
        logger.info("  - IsotonicCalibratorç±»: å®Œæ•´æ¦‚çŽ‡æ ¡å‡†å™¨")
        logger.info("  - ECE/Brier/LogLossæŒ‡æ ‡è®¡ç®—")
        logger.info("  - æ ¡å‡†æ›²çº¿å¯è§†åŒ–åˆ†æž")
        logger.info("  - æ ¡å‡†å™¨ä¿å­˜/åŠ è½½åŠŸèƒ½")
        logger.info("  - ç”Ÿäº§å°±ç»ªçš„æ ¡å‡†æŽ¥å£")
        logger.info("  - å…¨é¢æµ‹è¯•éªŒè¯é€šè¿‡")
        
        logger.info("ðŸ“Š æœ€ç»ˆæŒ‡æ ‡:")
        for key, value in metrics.items():
            if isinstance(value, float):
                logger.info(f"   {key}: {value:.4f}")
    else:
        logger.error("âŒ Task T004éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
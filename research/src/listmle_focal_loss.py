#!/usr/bin/env python3
"""
CoTRR-Stable Stage 1 Task T002: ListMLE + Focal Loss Implementation
å®ç°ListMLEæŸå¤±å‡½æ•° + Focal Lossç»„åˆï¼Œç”¨äºå¤„ç†ä¸å¹³è¡¡çš„æ’åºæ•°æ®

å…³é”®ç‰¹æ€§:
1. ListMLEæŸå¤±: ç”¨äºæ’åºå­¦ä¹ çš„åˆ—è¡¨å¼æœ€å¤§ä¼¼ç„¶ä¼°è®¡
2. Focal Loss: å¤„ç†éš¾æ ·æœ¬ï¼Œå‡å°‘æ˜“æ ·æœ¬çš„æƒé‡
3. Temperatureæ ¡å‡†: æé«˜æ¦‚ç‡ä¼°è®¡çš„å¯é æ€§
4. Gradient accumulation: æ”¯æŒå¤§æ‰¹æ¬¡è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import math

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LossConfig:
    """æŸå¤±å‡½æ•°é…ç½®"""
    focal_alpha: float = 0.25
    focal_gamma: float = 2.0
    listmle_weight: float = 0.7
    focal_weight: float = 0.3
    temperature: float = 1.0
    label_smoothing: float = 0.1
    gradient_clip_val: float = 1.0

class ListMLELoss(nn.Module):
    """
    ListMLE (List-wise Maximum Likelihood Estimation) Loss
    é€‚ç”¨äºæ’åºå­¦ä¹ ä»»åŠ¡ï¼Œè€ƒè™‘æ•´ä¸ªå€™é€‰åˆ—è¡¨çš„æ’åºå…³ç³»
    """
    
    def __init__(self, temperature: float = 1.0, eps: float = 1e-10):
        super().__init__()
        self.temperature = temperature
        self.eps = eps
        
    def forward(self, scores: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Args:
            scores: [batch_size, num_candidates] æ¨¡å‹é¢„æµ‹åˆ†æ•°
            labels: [batch_size, num_candidates] çœŸå®ç›¸å…³æ€§æ ‡ç­¾
        Returns:
            ListMLEæŸå¤±å€¼
        """
        # æ¸©åº¦ç¼©æ”¾
        scaled_scores = scores / self.temperature
        
        # æ ¹æ®çœŸå®æ ‡ç­¾å¯¹åˆ†æ•°è¿›è¡Œæ’åº
        sorted_labels, sorted_indices = torch.sort(labels, descending=True, dim=-1)
        
        # è·å–å¯¹åº”æ’åºåçš„åˆ†æ•°
        batch_size, num_candidates = scores.shape
        batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, num_candidates)
        sorted_scores = scaled_scores[batch_indices, sorted_indices]
        
        # è®¡ç®—ListMLEæŸå¤±
        # å¯¹äºæ¯ä¸ªä½ç½®ï¼Œè®¡ç®—ä»è¯¥ä½ç½®åˆ°æœ«å°¾çš„logsumexp
        max_scores = torch.cummax(sorted_scores.flip(-1), dim=-1)[0].flip(-1)
        log_sum_exp = torch.logsumexp(sorted_scores - max_scores, dim=-1) + max_scores[:, 0]
        
        # ç´¯ç§¯å¯¹æ•°ä¼¼ç„¶
        cumulative_log_likelihood = torch.zeros_like(sorted_scores)
        for i in range(num_candidates - 1):
            remaining_scores = sorted_scores[:, i:]
            max_remaining = torch.max(remaining_scores, dim=-1, keepdim=True)[0]
            log_sum_remaining = torch.logsumexp(remaining_scores - max_remaining, dim=-1)
            cumulative_log_likelihood[:, i] = sorted_scores[:, i] - max_remaining.squeeze(-1) - log_sum_remaining
        
        # è®¡ç®—å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶
        mask = (sorted_labels > 0).float()  # åªè€ƒè™‘ç›¸å…³çš„é¡¹ç›®
        masked_likelihood = cumulative_log_likelihood * mask
        loss = -torch.sum(masked_likelihood, dim=-1) / (torch.sum(mask, dim=-1) + self.eps)
        
        return torch.mean(loss)

class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance
    ä¸“æ³¨äºéš¾åˆ†ç±»æ ·æœ¬ï¼Œå‡å°‘æ˜“åˆ†ç±»æ ·æœ¬çš„è´¡çŒ®
    """
    
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, reduction: str = 'mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: [batch_size, num_candidates] æ¨¡å‹é¢„æµ‹logits
            targets: [batch_size, num_candidates] äºŒå€¼åŒ–æ ‡ç­¾ (0/1)
        """
        # è®¡ç®—äº¤å‰ç†µ
        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets.float(), reduction='none')
        
        # è®¡ç®—æ¦‚ç‡
        p = torch.sigmoid(inputs)
        p_t = p * targets + (1 - p) * (1 - targets)
        
        # è®¡ç®—focalæƒé‡
        focal_weight = self.alpha * (1 - p_t) ** self.gamma
        
        # åº”ç”¨focalæƒé‡
        focal_loss = focal_weight * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class CombinedRankingLoss(nn.Module):
    """
    ç»„åˆæŸå¤±å‡½æ•°: ListMLE + Focal Loss
    ç»“åˆæ’åºå­¦ä¹ å’Œéš¾æ ·æœ¬æŒ–æ˜çš„ä¼˜åŠ¿
    """
    
    def __init__(self, config: LossConfig):
        super().__init__()
        self.config = config
        self.listmle_loss = ListMLELoss(temperature=config.temperature)
        self.focal_loss = FocalLoss(alpha=config.focal_alpha, gamma=config.focal_gamma)
        
        # æ¸©åº¦å‚æ•°ï¼Œç”¨äºæ ¡å‡†
        self.temperature = nn.Parameter(torch.ones(1) * config.temperature)
        
    def forward(self, 
                scores: torch.Tensor, 
                labels: torch.Tensor,
                return_components: bool = False) -> Dict[str, torch.Tensor]:
        """
        Args:
            scores: [batch_size, num_candidates] æ¨¡å‹åŸå§‹åˆ†æ•°
            labels: [batch_size, num_candidates] ç›¸å…³æ€§æ ‡ç­¾ (0-4 scale)
            return_components: æ˜¯å¦è¿”å›å„ç»„ä»¶æŸå¤±
        """
        # æ¸©åº¦ç¼©æ”¾
        calibrated_scores = scores / torch.clamp(self.temperature, min=0.1, max=5.0)
        
        # ListMLEæŸå¤± (ä½¿ç”¨åŸå§‹å¤šçº§æ ‡ç­¾)
        listmle_loss = self.listmle_loss(calibrated_scores, labels)
        
        # FocalæŸå¤± (ä½¿ç”¨äºŒå€¼åŒ–æ ‡ç­¾)
        binary_labels = (labels > 0).float()
        focal_loss = self.focal_loss(calibrated_scores, binary_labels)
        
        # ç»„åˆæŸå¤±
        total_loss = (self.config.listmle_weight * listmle_loss + 
                     self.config.focal_weight * focal_loss)
        
        # æ ‡ç­¾å¹³æ»‘æ­£åˆ™åŒ–
        if self.config.label_smoothing > 0:
            uniform_dist = torch.ones_like(binary_labels) / binary_labels.shape[-1]
            smooth_labels = (1 - self.config.label_smoothing) * binary_labels + \
                           self.config.label_smoothing * uniform_dist
            smooth_loss = F.binary_cross_entropy_with_logits(
                calibrated_scores, smooth_labels, reduction='mean'
            )
            total_loss += 0.1 * smooth_loss
        
        result = {'total_loss': total_loss}
        
        if return_components:
            result.update({
                'listmle_loss': listmle_loss,
                'focal_loss': focal_loss,
                'temperature': self.temperature.item(),
                'calibrated_scores_std': calibrated_scores.std().item()
            })
            
        return result

class RankingTrainer:
    """
    ä¸“é—¨ç”¨äºè®­ç»ƒCoTRR-Stableçš„æ’åºæŸå¤±è®­ç»ƒå™¨
    æ”¯æŒæ¢¯åº¦ç´¯ç§¯ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœç­‰åŠŸèƒ½
    """
    
    def __init__(self, 
                 model: nn.Module,
                 config: LossConfig,
                 device: str = 'cpu'):
        self.model = model
        self.config = config
        self.device = device
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.loss_fn = CombinedRankingLoss(config).to(device)
        
        # ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = torch.optim.AdamW([
            {'params': model.parameters(), 'lr': 1e-4, 'weight_decay': 0.01},
            {'params': self.loss_fn.temperature, 'lr': 1e-3, 'weight_decay': 0.0}
        ])
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.7, patience=3
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.best_loss = float('inf')
        self.patience_counter = 0
        
    def train_step(self, 
                   batch: Dict[str, torch.Tensor],
                   accumulation_steps: int = 1) -> Dict[str, float]:
        """å•æ­¥è®­ç»ƒ"""
        self.model.train()
        
        # å‰å‘ä¼ æ’­
        scores = self.model(batch)  # [batch_size, num_candidates]
        labels = batch['labels'].to(self.device)  # [batch_size, num_candidates]
        
        # è®¡ç®—æŸå¤±
        loss_dict = self.loss_fn(scores, labels, return_components=True)
        total_loss = loss_dict['total_loss'] / accumulation_steps
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        
        # æ¢¯åº¦ç´¯ç§¯
        if (self.global_step + 1) % accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.gradient_clip_val
            )
            
            self.optimizer.step()
            self.optimizer.zero_grad()
        
        self.global_step += 1
        
        # è¿”å›æŒ‡æ ‡
        metrics = {
            'total_loss': loss_dict['total_loss'].item(),
            'listmle_loss': loss_dict['listmle_loss'].item(),
            'focal_loss': loss_dict['focal_loss'].item(),
            'temperature': loss_dict['temperature'],
            'calibrated_scores_std': loss_dict['calibrated_scores_std'],
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }
        
        return metrics
    
    def validate_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """éªŒè¯æ­¥éª¤"""
        self.model.eval()
        
        with torch.no_grad():
            scores = self.model(batch)
            labels = batch['labels'].to(self.device)
            
            loss_dict = self.loss_fn(scores, labels, return_components=True)
            
            # è®¡ç®—æ’åºæŒ‡æ ‡
            ranking_metrics = self._compute_ranking_metrics(scores, labels)
            
            metrics = {
                'val_total_loss': loss_dict['total_loss'].item(),
                'val_listmle_loss': loss_dict['listmle_loss'].item(),
                'val_focal_loss': loss_dict['focal_loss'].item()
            }
            metrics.update(ranking_metrics)
            
        return metrics
    
    def _compute_ranking_metrics(self, 
                                scores: torch.Tensor, 
                                labels: torch.Tensor) -> Dict[str, float]:
        """è®¡ç®—æ’åºæŒ‡æ ‡ (NDCG@k, MRRç­‰)"""
        scores_np = scores.cpu().numpy()
        labels_np = labels.cpu().numpy()
        
        ndcg_1 = self._ndcg_at_k(scores_np, labels_np, k=1)
        ndcg_5 = self._ndcg_at_k(scores_np, labels_np, k=5)
        ndcg_10 = self._ndcg_at_k(scores_np, labels_np, k=10)
        
        return {
            'ndcg@1': ndcg_1,
            'ndcg@5': ndcg_5,
            'ndcg@10': ndcg_10
        }
    
    def _ndcg_at_k(self, scores: np.ndarray, labels: np.ndarray, k: int) -> float:
        """è®¡ç®—NDCG@kæŒ‡æ ‡"""
        batch_ndcg = []
        
        for score_row, label_row in zip(scores, labels):
            # æŒ‰åˆ†æ•°æ’åº
            sorted_indices = np.argsort(score_row)[::-1][:k]
            sorted_labels = label_row[sorted_indices]
            
            # DCG@k
            dcg = np.sum((2**sorted_labels - 1) / np.log2(np.arange(2, len(sorted_labels) + 2)))
            
            # IDCG@k
            ideal_labels = np.sort(label_row)[::-1][:k]
            idcg = np.sum((2**ideal_labels - 1) / np.log2(np.arange(2, len(ideal_labels) + 2)))
            
            # NDCG@k
            ndcg = dcg / idcg if idcg > 0 else 0.0
            batch_ndcg.append(ndcg)
        
        return np.mean(batch_ndcg)

def test_combined_loss():
    """æµ‹è¯•ç»„åˆæŸå¤±å‡½æ•°"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•ListMLE + Focal Lossç»„åˆ")
    
    # é…ç½®
    config = LossConfig(
        focal_alpha=0.25,
        focal_gamma=2.0,
        listmle_weight=0.7,
        focal_weight=0.3,
        temperature=1.0
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, num_candidates = 8, 20
    scores = torch.randn(batch_size, num_candidates) * 2.0
    labels = torch.randint(0, 5, (batch_size, num_candidates)).float()
    
    # æµ‹è¯•æŸå¤±å‡½æ•°
    loss_fn = CombinedRankingLoss(config)
    
    # å‰å‘ä¼ æ’­
    loss_dict = loss_fn(scores, labels, return_components=True)
    
    logger.info(f"æ€»æŸå¤±: {loss_dict['total_loss']:.4f}")
    logger.info(f"ListMLEæŸå¤±: {loss_dict['listmle_loss']:.4f}")
    logger.info(f"FocalæŸå¤±: {loss_dict['focal_loss']:.4f}")
    logger.info(f"æ¸©åº¦å‚æ•°: {loss_dict['temperature']:.4f}")
    logger.info(f"æ ¡å‡†åˆ†æ•°æ ‡å‡†å·®: {loss_dict['calibrated_scores_std']:.4f}")
    
    # æµ‹è¯•æ¢¯åº¦
    total_loss = loss_dict['total_loss']
    total_loss.backward()
    
    grad_norm = torch.norm(loss_fn.temperature.grad)
    logger.info(f"æ¸©åº¦æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
    
    logger.info("âœ… ç»„åˆæŸå¤±å‡½æ•°æµ‹è¯•é€šè¿‡")
    
    return True

if __name__ == "__main__":
    logger.info("ğŸš€ å¼€å§‹Task T002: ListMLE + Focal Losså®ç°")
    
    # è¿è¡Œæµ‹è¯•
    test_passed = test_combined_loss()
    
    if test_passed:
        logger.info("ğŸ‰ Task T002å®ç°å®Œæˆï¼")
        logger.info("ğŸ“‹ äº¤ä»˜å†…å®¹:")
        logger.info("  - ListMLELossç±»: æ’åºå­¦ä¹ æŸå¤±å‡½æ•°")
        logger.info("  - FocalLossç±»: éš¾æ ·æœ¬æŒ–æ˜æŸå¤±å‡½æ•°")
        logger.info("  - CombinedRankingLossç±»: ç»„åˆæŸå¤±ä¸æ¸©åº¦æ ¡å‡†")
        logger.info("  - RankingTrainerç±»: å®Œæ•´è®­ç»ƒæ¡†æ¶")
        logger.info("  - æ¢¯åº¦ç´¯ç§¯ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ’åºæŒ‡æ ‡è®¡ç®—")
    else:
        logger.error("âŒ Task T002æµ‹è¯•å¤±è´¥")
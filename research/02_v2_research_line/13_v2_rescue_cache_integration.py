#!/usr/bin/env python3
"""
V2æ•‘æ´è®¡åˆ’ - ç¼“å­˜ä¼˜åŒ–å®æˆ˜åº”ç”¨
================================================================================
åŸºäº48å°æ—¶æ•‘æ´å¤æ ¸çš„å‘ç°ï¼Œé›†æˆæ™ºèƒ½ç¼“å­˜åˆ°å®é™…æ•‘æ´æµç¨‹ä¸­
åœºæ™¯ï¼šæ•°æ®æ³„æ¼ä¿®å¤ + è¯„æµ‹å¢å¼º + æ¶æ„ä¿®è¡¥çš„ç¼“å­˜ä¼˜åŒ–å®è·µ
================================================================================
"""

import json
import time
import numpy as np
import torch
from typing import Dict, List, Any, Tuple
from datetime import datetime
import logging
import hashlib
from collections import OrderedDict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleCacheManager:
    """ç®€åŒ–çš„ç¼“å­˜ç®¡ç†å™¨ (åµŒå…¥ç‰ˆæœ¬)"""
    
    def __init__(self, max_size: int = 100):
        self.max_size = max_size
        self.cache = OrderedDict()
        self.stats = {'hits': 0, 'misses': 0, 'total': 0}
    
    def get(self, key: str):
        self.stats['total'] += 1
        if key in self.cache:
            self.stats['hits'] += 1
            self.cache.move_to_end(key)  # LRU
            return self.cache[key]
        else:
            self.stats['misses'] += 1
            return None
    
    def set(self, key: str, value: Any):
        if key in self.cache:
            self.cache.pop(key)
        
        self.cache[key] = value
        self.cache.move_to_end(key)
        
        while len(self.cache) > self.max_size:
            self.cache.popitem(last=False)
    
    def get_stats(self):
        hit_rate = self.stats['hits'] / max(self.stats['total'], 1)
        return {
            'total_requests': self.stats['total'],
            'cache_hits': self.stats['hits'],
            'cache_misses': self.stats['misses'],
            'hit_rate': hit_rate
        }

def generate_signature(data: Any) -> str:
    """ç®€åŒ–çš„ç­¾åç”Ÿæˆå™¨"""
    content = json.dumps(data, sort_keys=True, default=str)
    return hashlib.md5(content.encode()).hexdigest()

class V2RescueCacheIntegration:
    """V2æ•‘æ´è®¡åˆ’ç¼“å­˜é›†æˆå™¨"""
    
    def __init__(self):
        # ä½¿ç”¨ç®€åŒ–çš„ç¼“å­˜ç®¡ç†å™¨
        self.integrity_check_cache = SimpleCacheManager(max_size=200)
        self.evaluation_cache = SimpleCacheManager(max_size=500)
        self.architecture_fix_cache = SimpleCacheManager(max_size=100)
        
        logger.info("ğŸ”§ V2æ•‘æ´è®¡åˆ’ç¼“å­˜é›†æˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def cached_data_integrity_check(self, 
                                  dataset: List[Dict],
                                  check_type: str = "leakage_detection") -> Dict[str, Any]:
        """ç¼“å­˜çš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        
        è§£å†³é—®é¢˜ï¼šé‡å¤çš„æ³„æ¼æ£€æµ‹ã€ç‰¹å¾éªŒè¯é¿å…é‡å¤è®¡ç®—
        """
        # åˆ›å»ºæ•°æ®é›†ç­¾å - ä½¿ç”¨ç»“æ„åŒ–ä¿¡æ¯è€Œéå®Œæ•´å†…å®¹
        dataset_signature = self._create_dataset_signature(dataset)
        cache_key = f"integrity_{check_type}_{dataset_signature}"
        
        input_data = {
            'dataset_size': len(dataset),
            'check_type': check_type,
            'dataset_signature': dataset_signature
        }
        
        # å°è¯•ä»ç¼“å­˜è·å–
        logger.info(f"ğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ç¼“å­˜: {check_type}")
        cached_result = self.integrity_check_cache.get(cache_key)
        
        if cached_result is not None:
            logger.info(f"âœ… å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜å‘½ä¸­: {check_type}")
            return cached_result
        
        # æ‰§è¡Œå®é™…æ£€æŸ¥
        logger.info(f"ğŸ”„ æ‰§è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥: {check_type}")
        start_time = time.time()
        
        if check_type == "leakage_detection":
            result = self._execute_leakage_detection(dataset)
        elif check_type == "feature_validation":
            result = self._execute_feature_validation(dataset)
        elif check_type == "label_penetration":
            result = self._execute_label_penetration_test(dataset)
        else:
            raise ValueError(f"Unknown check type: {check_type}")
        
        compute_time = time.time() - start_time
        
        # ç¼“å­˜ç»“æœ
        self.integrity_check_cache.set(cache_key, result)
        
        logger.info(f"ğŸ’¾ å®Œæ•´æ€§æ£€æŸ¥ç»“æœå·²ç¼“å­˜: {check_type} ({compute_time:.3f}s)")
        return result
    
    def cached_cross_validation_evaluation(self,
                                         model_config: Dict[str, Any],
                                         dataset: List[Dict],
                                         n_folds: int = 5) -> Dict[str, Any]:
        """ç¼“å­˜çš„äº¤å‰éªŒè¯è¯„ä¼°
        
        è§£å†³é—®é¢˜ï¼šç›¸åŒé…ç½®çš„é‡å¤è¯„ä¼°ã€å¢é‡è¯„ä¼°ä¼˜åŒ–
        """
        # æ¨¡å‹é…ç½®ç­¾å
        config_signature = generate_signature(model_config)
        
        # æ•°æ®é›†ç­¾å
        dataset_signature = self._create_dataset_signature(dataset)
        
        cache_key = f"cv_eval_{config_signature}_{dataset_signature}_{n_folds}"
        
        input_data = {
            'model_config': model_config,
            'dataset_size': len(dataset),
            'n_folds': n_folds
        }
        
        logger.info(f"ğŸ” æ£€æŸ¥äº¤å‰éªŒè¯ç¼“å­˜: {n_folds}æŠ˜")
        cached_result = self.evaluation_cache.get(cache_key)
        
        if cached_result is not None:
            logger.info(f"âœ… äº¤å‰éªŒè¯ç¼“å­˜å‘½ä¸­")
            return cached_result
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†ç»“æœå¯ä»¥é‡ç”¨
        partial_results = self._find_partial_cv_results(config_signature, dataset_signature)
        
        if partial_results:
            logger.info(f"ğŸ”„ å‘ç°éƒ¨åˆ†äº¤å‰éªŒè¯ç»“æœï¼Œå¢é‡è®¡ç®—")
            result = self._incremental_cross_validation(
                model_config, dataset, n_folds, partial_results
            )
        else:
            logger.info(f"ğŸ”„ æ‰§è¡Œå®Œæ•´äº¤å‰éªŒè¯")
            result = self._execute_full_cross_validation(model_config, dataset, n_folds)
        
        # ç¼“å­˜å®Œæ•´ç»“æœ
        compute_time = result.get('total_compute_time', 0)
        self.evaluation_cache.set(cache_key, result)
        
        return result
    
    def cached_architecture_modification(self,
                                       base_architecture: Dict[str, Any],
                                       modifications: Dict[str, Any]) -> Dict[str, Any]:
        """ç¼“å­˜çš„æ¶æ„ä¿®æ”¹
        
        è§£å†³é—®é¢˜ï¼šç›¸ä¼¼æ¶æ„è°ƒæ•´çš„é‡å¤å®éªŒã€æ¸è¿›å¼ä¼˜åŒ–
        """
        # æ¶æ„ç­¾å
        arch_signature = generate_signature(base_architecture)
        
        # ä¿®æ”¹ç­¾å
        mod_signature = generate_signature(modifications)
        
        cache_key = f"arch_mod_{arch_signature}_{mod_signature}"
        
        input_data = {
            'base_architecture': base_architecture,
            'modifications': modifications
        }
        
        logger.info(f"ğŸ” æ£€æŸ¥æ¶æ„ä¿®æ”¹ç¼“å­˜")
        cached_result = self.architecture_fix_cache.get(cache_key)
        
        if cached_result is not None:
            logger.info(f"âœ… æ¶æ„ä¿®æ”¹ç¼“å­˜å‘½ä¸­")
            return cached_result
        
        # æ£€æŸ¥ç›¸ä¼¼æ¶æ„ä¿®æ”¹
        similar_results = self._find_similar_architecture_results(
            base_architecture, modifications
        )
        
        if similar_results:
            logger.info(f"ğŸ”„ å‘ç°ç›¸ä¼¼æ¶æ„ç»“æœï¼Œé€‚é…ä¿®æ”¹")
            result = self._adapt_similar_architecture_result(
                similar_results, modifications
            )
        else:
            logger.info(f"ğŸ”„ æ‰§è¡Œå…¨æ–°æ¶æ„ä¿®æ”¹")
            result = self._execute_architecture_modification(
                base_architecture, modifications
            )
        
        compute_time = result.get('modification_time', 0)
        self.architecture_fix_cache.set(cache_key, result)
        
        return result
    
    def _create_dataset_signature(self, dataset: List[Dict]) -> str:
        """åˆ›å»ºæ•°æ®é›†ç­¾å - åŸºäºç»“æ„è€Œéå†…å®¹"""
        signature_data = {
            'size': len(dataset),
            'fields': list(dataset[0].keys()) if dataset else [],
            'sample_hashes': [
                hash(str(sorted(item.items()))[:100]) for item in dataset[:5]
            ]
        }
        return generate_signature(signature_data)
    
    def _execute_leakage_detection(self, dataset: List[Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œæ³„æ¼æ£€æµ‹ (æ¨¡æ‹Ÿæ•‘æ´è®¡åˆ’ä¸­çš„å®é™…æ£€æµ‹)"""
        logger.info("ğŸ” æ‰§è¡Œæ•°æ®æ³„æ¼æ£€æµ‹...")
        time.sleep(0.5)  # æ¨¡æ‹Ÿæ£€æµ‹æ—¶é—´
        
        # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ
        train_test_overlap = np.random.uniform(0, 0.05)  # 0-5%é‡å 
        feature_leakage_score = np.random.uniform(0, 0.1)  # 0-10%æ³„æ¼
        
        return {
            'train_test_overlap': train_test_overlap,
            'feature_leakage_score': feature_leakage_score,
            'leakage_detected': train_test_overlap > 0.02 or feature_leakage_score > 0.05,
            'recommendations': [
                "é‡æ–°åˆ‡åˆ†è®­ç»ƒæµ‹è¯•é›†" if train_test_overlap > 0.02 else "è®­ç»ƒæµ‹è¯•åˆ‡åˆ†æ­£å¸¸",
                "æ£€æŸ¥ç‰¹å¾å·¥ç¨‹ç®¡é“" if feature_leakage_score > 0.05 else "ç‰¹å¾å·¥ç¨‹æ­£å¸¸"
            ]
        }
    
    def _execute_feature_validation(self, dataset: List[Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œç‰¹å¾éªŒè¯"""
        logger.info("ğŸ” æ‰§è¡Œç‰¹å¾éªŒè¯...")
        time.sleep(0.3)
        
        return {
            'feature_completeness': np.random.uniform(0.9, 1.0),
            'feature_quality_score': np.random.uniform(0.8, 0.95),
            'invalid_features': [],
            'validation_passed': True
        }
    
    def _execute_label_penetration_test(self, dataset: List[Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œæ ‡ç­¾ç©¿é€æµ‹è¯•"""
        logger.info("ğŸ” æ‰§è¡Œæ ‡ç­¾ç©¿é€æµ‹è¯•...")
        time.sleep(0.4)
        
        random_label_loss = np.random.uniform(0.3, 0.5)  # éšæœºæ ‡ç­¾ä¸‹åº”è¯¥æ— æ³•æ‹Ÿåˆ
        
        return {
            'random_label_final_loss': random_label_loss,
            'penetration_detected': random_label_loss < 0.1,  # æŸå¤±è¿‡ä½è¡¨ç¤ºç©¿é€
            'penetration_risk': 'low' if random_label_loss > 0.3 else 'high'
        }
    
    def _find_partial_cv_results(self, config_sig: str, dataset_sig: str) -> List[Dict]:
        """æŸ¥æ‰¾éƒ¨åˆ†äº¤å‰éªŒè¯ç»“æœ"""
        # æ¨¡æ‹ŸæŸ¥æ‰¾ç›¸ä¼¼é…ç½®çš„ç»“æœ
        return []  # ç®€åŒ–å®ç°
    
    def _incremental_cross_validation(self, 
                                    config: Dict, 
                                    dataset: List[Dict], 
                                    n_folds: int,
                                    partial_results: List[Dict]) -> Dict[str, Any]:
        """å¢é‡äº¤å‰éªŒè¯"""
        logger.info("ğŸ”„ æ‰§è¡Œå¢é‡äº¤å‰éªŒè¯...")
        time.sleep(1.0)  # æ¨¡æ‹Ÿå‡å°‘çš„è®¡ç®—æ—¶é—´
        
        return self._create_cv_result(config, dataset, n_folds)
    
    def _execute_full_cross_validation(self, 
                                     config: Dict, 
                                     dataset: List[Dict], 
                                     n_folds: int) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´äº¤å‰éªŒè¯"""
        logger.info("ğŸ”„ æ‰§è¡Œå®Œæ•´äº¤å‰éªŒè¯...")
        time.sleep(2.0)  # æ¨¡æ‹Ÿå®Œæ•´è®¡ç®—æ—¶é—´
        
        return self._create_cv_result(config, dataset, n_folds)
    
    def _create_cv_result(self, config: Dict, dataset: List[Dict], n_folds: int) -> Dict[str, Any]:
        """åˆ›å»ºäº¤å‰éªŒè¯ç»“æœ"""
        fold_results = []
        
        for fold in range(n_folds):
            fold_results.append({
                'fold': fold,
                'ndcg_improvement': np.random.normal(0.015, 0.005),
                'ranking_accuracy': np.random.uniform(0.7, 0.85),
                'test_samples': len(dataset) // n_folds
            })
        
        improvements = [r['ndcg_improvement'] for r in fold_results]
        
        return {
            'n_folds': n_folds,
            'fold_results': fold_results,
            'mean_improvement': np.mean(improvements),
            'std_improvement': np.std(improvements),
            'ci_95_lower': np.percentile(improvements, 2.5),
            'ci_95_upper': np.percentile(improvements, 97.5),
            'total_compute_time': n_folds * 0.4,  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
            'evaluation_timestamp': datetime.now().isoformat()
        }
    
    def _find_similar_architecture_results(self, 
                                         base_arch: Dict, 
                                         modifications: Dict) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ¶æ„ç»“æœ"""
        # æ¨¡æ‹ŸæŸ¥æ‰¾ç›¸ä¼¼æ¶æ„
        return []  # ç®€åŒ–å®ç°
    
    def _adapt_similar_architecture_result(self, 
                                         similar_results: List[Dict],
                                         modifications: Dict) -> Dict[str, Any]:
        """é€‚é…ç›¸ä¼¼æ¶æ„ç»“æœ"""
        logger.info("ğŸ”„ é€‚é…ç›¸ä¼¼æ¶æ„ç»“æœ...")
        time.sleep(0.3)  # æ¨¡æ‹Ÿé€‚é…æ—¶é—´
        
        return self._create_architecture_result(modifications)
    
    def _execute_architecture_modification(self, 
                                         base_arch: Dict,
                                         modifications: Dict) -> Dict[str, Any]:
        """æ‰§è¡Œæ¶æ„ä¿®æ”¹"""
        logger.info("ğŸ”„ æ‰§è¡Œæ¶æ„ä¿®æ”¹...")
        time.sleep(1.5)  # æ¨¡æ‹Ÿæ¶æ„ä¿®æ”¹æ—¶é—´
        
        return self._create_architecture_result(modifications)
    
    def _create_architecture_result(self, modifications: Dict) -> Dict[str, Any]:
        """åˆ›å»ºæ¶æ„ä¿®æ”¹ç»“æœ"""
        return {
            'modifications_applied': modifications,
            'architecture_valid': True,
            'estimated_performance_change': np.random.uniform(-0.01, 0.03),
            'modification_time': 1.5,
            'complexity_score': len(modifications) * 0.1,
            'modification_timestamp': datetime.now().isoformat()
        }
    
    def execute_rescue_pipeline_with_cache(self, 
                                         dataset: List[Dict],
                                         model_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¸¦ç¼“å­˜ä¼˜åŒ–çš„æ•‘æ´ç®¡é“"""
        logger.info("ğŸš€ å¼€å§‹V2æ•‘æ´ç®¡é“ (ç¼“å­˜ä¼˜åŒ–)")
        pipeline_start = time.time()
        
        results = {}
        
        # Phase 1: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ (å¹¶è¡Œç¼“å­˜)
        logger.info("ğŸ“‹ Phase 1: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
        integrity_results = {}
        
        # å¹¶è¡Œæ‰§è¡Œå¤šç§æ£€æŸ¥ï¼Œæ¯ç§éƒ½æœ‰ç‹¬ç«‹ç¼“å­˜
        check_types = ['leakage_detection', 'feature_validation', 'label_penetration']
        for check_type in check_types:
            integrity_results[check_type] = self.cached_data_integrity_check(
                dataset, check_type
            )
        
        results['integrity_check'] = integrity_results
        
        # Phase 2: è¯„æµ‹å¢å¼º (æ™ºèƒ½ç¼“å­˜)
        logger.info("ğŸ“‹ Phase 2: è¯„æµ‹å¢å¼º")
        evaluation_result = self.cached_cross_validation_evaluation(
            model_config, dataset, n_folds=5
        )
        results['evaluation'] = evaluation_result
        
        # Phase 3: æ¶æ„ä¿®è¡¥ (åŸºäºè¯„æµ‹ç»“æœå†³å®šä¿®æ”¹ç­–ç•¥)
        logger.info("ğŸ“‹ Phase 3: æ¶æ„ä¿®è¡¥")
        
        # æ ¹æ®è¯„æµ‹ç»“æœå†³å®šä¿®æ”¹ç­–ç•¥
        if evaluation_result['ci_95_lower'] < 0.01:
            # æ”¹è¿›ä¸æ˜æ˜¾ï¼Œéœ€è¦è¾ƒå¤§ä¿®æ”¹
            modifications = {
                'type': 'major_fix',
                'dropout_rate': 0.3,
                'l2_regularization': 0.01,
                'architecture_change': 'reduce_capacity'
            }
        else:
            # æœ‰æ”¹è¿›ï¼Œåªéœ€å¾®è°ƒ
            modifications = {
                'type': 'minor_tune',
                'dropout_rate': 0.1,
                'l2_regularization': 0.001,
                'architecture_change': 'tune_weights'
            }
        
        architecture_result = self.cached_architecture_modification(
            model_config, modifications
        )
        results['architecture_fix'] = architecture_result
        
        # è®¡ç®—æ€»æ—¶é—´å’Œç¼“å­˜æ•ˆæœ
        total_time = time.time() - pipeline_start
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = self._get_rescue_cache_stats()
        
        results['pipeline_summary'] = {
            'total_execution_time': total_time,
            'cache_performance': cache_stats,
            'phases_completed': 3,
            'rescue_decision': self._make_rescue_decision(results)
        }
        
        logger.info(f"âœ… æ•‘æ´ç®¡é“å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s")
        return results
    
    def _get_rescue_cache_stats(self) -> Dict[str, Any]:
        """è·å–æ•‘æ´è¿‡ç¨‹çš„ç¼“å­˜ç»Ÿè®¡"""
        return {
            'integrity_check_cache': self.integrity_check_cache.get_stats(),
            'evaluation_cache': self.evaluation_cache.get_stats(),
            'architecture_fix_cache': self.architecture_fix_cache.get_stats(),
            'overall_hit_rate': self._calculate_overall_hit_rate()
        }
    
    def _calculate_overall_hit_rate(self) -> float:
        """è®¡ç®—æ•´ä½“ç¼“å­˜å‘½ä¸­ç‡"""
        total_requests = 0
        total_hits = 0
        
        for cache in [self.integrity_check_cache, self.evaluation_cache, self.architecture_fix_cache]:
            stats = cache.get_stats()
            total_requests += stats['total_requests']
            total_hits += stats['cache_hits']
        
        return total_hits / max(total_requests, 1)
    
    def _make_rescue_decision(self, results: Dict[str, Any]) -> str:
        """åŸºäºæ•‘æ´ç»“æœåšå†³ç­–"""
        integrity = results['integrity_check']
        evaluation = results['evaluation']
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        leakage_detected = integrity['leakage_detection']['leakage_detected']
        if leakage_detected:
            return "PAUSE_AND_FIX - å‘ç°æ•°æ®æ³„æ¼"
        
        # æ£€æŸ¥è¯„æµ‹ç»“æœ
        ci_lower = evaluation['ci_95_lower']
        if ci_lower >= 0.02:
            return "GO - CI95ä¸‹é™â‰¥0.02ï¼Œæ¨è¿›éƒ¨ç½²"
        elif ci_lower >= 0.01:
            return "CONDITIONAL_GO - æœ‰æ”¹è¿›ä½†ä¸å¤Ÿæ˜¾è‘—ï¼Œå°è§„æ¨¡è¯•éªŒ"
        else:
            return "OPTIMIZE - æ”¹è¿›ä¸æ˜æ˜¾ï¼Œç»§ç»­ä¼˜åŒ–æ¶æ„"

def demonstrate_rescue_cache_integration():
    """æ¼”ç¤ºæ•‘æ´è®¡åˆ’ä¸­çš„ç¼“å­˜é›†æˆæ•ˆæœ"""
    print("ğŸš€ V2æ•‘æ´è®¡åˆ’ - ç¼“å­˜é›†æˆå®æˆ˜æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºæ•‘æ´ç¼“å­˜é›†æˆå™¨
    rescue_cache = V2RescueCacheIntegration()
    
    # æ¨¡æ‹Ÿæ•°æ®é›†
    dataset = [
        {'query': f'query_{i}', 'candidates': [{'score': np.random.random()} for _ in range(5)]}
        for i in range(100)
    ]
    
    # æ¨¡æ‹Ÿæ¨¡å‹é…ç½®
    model_config = {
        'type': 'multimodal_transformer',
        'hidden_size': 512,
        'num_layers': 6,
        'dropout': 0.1
    }
    
    print("\nğŸ”„ ç¬¬ä¸€æ¬¡æ‰§è¡Œæ•‘æ´ç®¡é“ (å†·å¯åŠ¨)")
    print("-" * 40)
    
    # ç¬¬ä¸€æ¬¡æ‰§è¡Œ - å†·å¯åŠ¨
    start_time = time.time()
    results1 = rescue_cache.execute_rescue_pipeline_with_cache(dataset, model_config)
    first_execution_time = time.time() - start_time
    
    print(f"ç¬¬ä¸€æ¬¡æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {first_execution_time:.3f}s")
    print(f"å†³ç­–ç»“æœ: {results1['pipeline_summary']['rescue_decision']}")
    
    print("\nğŸ”„ ç¬¬äºŒæ¬¡æ‰§è¡Œæ•‘æ´ç®¡é“ (ç¼“å­˜é¢„çƒ­)")
    print("-" * 40)
    
    # ç¬¬äºŒæ¬¡æ‰§è¡Œ - åº”è¯¥å¤§é‡å‘½ä¸­ç¼“å­˜
    start_time = time.time()
    results2 = rescue_cache.execute_rescue_pipeline_with_cache(dataset, model_config)
    second_execution_time = time.time() - start_time
    
    print(f"ç¬¬äºŒæ¬¡æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {second_execution_time:.3f}s")
    print(f"å†³ç­–ç»“æœ: {results2['pipeline_summary']['rescue_decision']}")
    
    # è®¡ç®—ç¼“å­˜æ•ˆæœ
    time_saved = first_execution_time - second_execution_time
    speedup_ratio = first_execution_time / second_execution_time
    
    print(f"\nğŸ“Š ç¼“å­˜æ•ˆæœåˆ†æ")
    print("=" * 40)
    print(f"æ—¶é—´èŠ‚çœ: {time_saved:.3f}s ({time_saved/first_execution_time*100:.1f}%)")
    print(f"åŠ é€Ÿæ¯”: {speedup_ratio:.1f}x")
    
    # æ˜¾ç¤ºè¯¦ç»†ç¼“å­˜ç»Ÿè®¡
    cache_stats = results2['pipeline_summary']['cache_performance']
    print(f"\nğŸ“ˆ ç¼“å­˜å‘½ä¸­ç‡:")
    print(f"   å®Œæ•´æ€§æ£€æŸ¥: {cache_stats['integrity_check_cache']['hit_rate']:.1%}")
    print(f"   è¯„æµ‹ç¼“å­˜: {cache_stats['evaluation_cache']['hit_rate']:.1%}")
    print(f"   æ¶æ„ä¿®æ”¹: {cache_stats['architecture_fix_cache']['hit_rate']:.1%}")
    print(f"   æ•´ä½“å‘½ä¸­ç‡: {cache_stats['overall_hit_rate']:.1%}")
    
    print("\nğŸ”„ ç¬¬ä¸‰æ¬¡æ‰§è¡Œ - ä¿®æ”¹é…ç½®")
    print("-" * 40)
    
    # ç¬¬ä¸‰æ¬¡æ‰§è¡Œ - è½»å¾®ä¿®æ”¹é…ç½®ï¼Œæµ‹è¯•æ™ºèƒ½ç¼“å­˜
    modified_config = model_config.copy()
    modified_config['dropout'] = 0.11  # è½»å¾®ä¿®æ”¹
    
    start_time = time.time()
    results3 = rescue_cache.execute_rescue_pipeline_with_cache(dataset, modified_config)
    third_execution_time = time.time() - start_time
    
    print(f"ä¿®æ”¹é…ç½®æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {third_execution_time:.3f}s")
    print(f"å†³ç­–ç»“æœ: {results3['pipeline_summary']['rescue_decision']}")
    
    final_cache_stats = results3['pipeline_summary']['cache_performance']
    print(f"æœ€ç»ˆæ•´ä½“å‘½ä¸­ç‡: {final_cache_stats['overall_hit_rate']:.1%}")
    
    print(f"\nâœ… æ•‘æ´è®¡åˆ’ç¼“å­˜é›†æˆæ¼”ç¤ºå®Œæˆ")
    print("ğŸ¯ å…³é”®æ´å¯Ÿ:")
    print("   â€¢ ç¼“å­˜ä½¿ç¬¬äºŒæ¬¡æ‰§è¡Œé€Ÿåº¦æå‡{:.1f}x".format(speedup_ratio))
    print("   â€¢ è½»å¾®é…ç½®ä¿®æ”¹ä»èƒ½åˆ©ç”¨å¤§éƒ¨åˆ†ç¼“å­˜")
    print("   â€¢ æ™ºèƒ½ç­¾åæœºåˆ¶å¹³è¡¡äº†ç²¾ç¡®æ€§å’Œé‡ç”¨æ€§")
    print("   â€¢ åˆ†é˜¶æ®µç¼“å­˜ç­–ç•¥é€‚é…æ•‘æ´ç®¡é“çš„ä¸åŒéœ€æ±‚")

if __name__ == "__main__":
    demonstrate_rescue_cache_integration()
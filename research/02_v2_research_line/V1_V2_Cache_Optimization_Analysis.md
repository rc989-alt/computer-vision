# V1/V2ç³»ç»Ÿç¼“å­˜ä¼˜åŒ–é—®é¢˜åˆ†æä¸è§£å†³æ–¹æ¡ˆ

## ğŸ“‹ æ¦‚è¿°

åŸºäºV2æ•‘æ´è®¡åˆ’çš„å®é™…è¿è¡Œç»“æœï¼Œæˆ‘ä»¬å‘ç°äº†ç¼“å­˜ä¼˜åŒ–åœ¨å¤šæ ·åŒ–è¾“å…¥åœºæ™¯ä¸‹çš„å…³é”®é—®é¢˜ã€‚æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æV1å’ŒV2ç³»ç»Ÿä¸­å¯èƒ½é‡åˆ°çš„ç¼“å­˜é—®é¢˜ï¼Œå¹¶æä¾›é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆã€‚

## ğŸ” V1ç³»ç»Ÿç¼“å­˜é—®é¢˜åˆ†æ

### 1. CLIPæ¨ç†ç¼“å­˜é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
- CLIPç¼“å­˜å‘½ä¸­ç‡åªæœ‰50%ï¼Œä½äºé¢„æœŸ
- ç›¸ä¼¼æŸ¥è¯¢("pink floral cocktail" vs "pink flower drink")æœªèƒ½é‡ç”¨ç¼“å­˜

**æ ¹æœ¬åŸå› **ï¼š
```python
# å½“å‰å®ç° - ç²¾ç¡®åŒ¹é…
cache_key = f"clip_{hashlib.md5((image_url + str(sorted(text_queries))).encode()).hexdigest()}"

# é—®é¢˜ï¼šè¯­ä¹‰ç›¸ä¼¼ä½†å­—é¢ä¸åŒçš„æŸ¥è¯¢è¢«è§†ä¸ºä¸åŒ
"pink floral cocktail" != "pink flower drink"  # ç¼“å­˜å¤±æ•ˆ
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **è¯­ä¹‰å½’ä¸€åŒ–**ï¼š
   ```python
   def normalize_query(query: str) -> str:
       # åŒä¹‰è¯æ›¿æ¢
       synonyms = {
           'floral': 'flower',
           'cocktail': 'drink',
           'beverage': 'drink'
       }
       normalized = query.lower()
       for original, replacement in synonyms.items():
           normalized = normalized.replace(original, replacement)
       return normalized
   ```

2. **æ¨¡ç³ŠåŒ¹é…æœºåˆ¶**ï¼š
   ```python
   # ä½¿ç”¨ç¼–è¾‘è·ç¦»æˆ–Jaccardç›¸ä¼¼åº¦
   def is_query_similar(query1: str, query2: str, threshold: float = 0.8) -> bool:
       words1 = set(query1.lower().split())
       words2 = set(query2.lower().split())
       jaccard = len(words1 & words2) / len(words1 | words2)
       return jaccard >= threshold
   ```

### 2. YOLOæ£€æµ‹ç¼“å­˜é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
- YOLOç¼“å­˜å‘½ä¸­ç‡66.7%ï¼Œç›¸åŒå›¾ç‰‡åº”è¯¥100%å‘½ä¸­
- æ£€æµ‹é˜ˆå€¼å¾®å°å˜åŒ–å¯¼è‡´é‡å¤è®¡ç®—

**æ ¹æœ¬åŸå› **ï¼š
```python
# å½“å‰å®ç° - å¯¹é˜ˆå€¼è¿‡äºæ•æ„Ÿ
cache_key = f"yolo_{hashlib.md5(image_url.encode()).hexdigest()}_{confidence_threshold}"

# é—®é¢˜ï¼š0.5 vs 0.51çš„é˜ˆå€¼å·®å¼‚å¯¼è‡´å®Œå…¨é‡æ–°è®¡ç®—
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **é˜ˆå€¼åˆ†çº§ç¼“å­˜**ï¼š
   ```python
   def get_threshold_tier(threshold: float) -> str:
       if threshold < 0.3:
           return "low"
       elif threshold < 0.7:
           return "medium"
       else:
           return "high"
   
   # ä½¿ç”¨åˆ†çº§è€Œéç²¾ç¡®é˜ˆå€¼
   cache_key = f"yolo_{image_hash}_{get_threshold_tier(threshold)}"
   ```

2. **åå¤„ç†è¿‡æ»¤**ï¼š
   ```python
   # å§‹ç»ˆç”¨æœ€ä½é˜ˆå€¼æ£€æµ‹ï¼Œåå¤„ç†æ—¶è¿‡æ»¤
   raw_detections = cached_yolo_detection(image_url, threshold=0.1)
   filtered_detections = [d for d in raw_detections if d['confidence'] >= actual_threshold]
   ```

### 3. åŒåˆ†æ•°è®¡ç®—ç¼“å­˜é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
- ç¼“å­˜å‘½ä¸­ç‡83.3%ï¼Œè¡¨ç°æœ€å¥½
- ç®€å•è®¡ç®—ä¹Ÿè¢«ç¼“å­˜ï¼Œå¯èƒ½è¿‡åº¦ä¼˜åŒ–

**ä¼˜åŒ–å»ºè®®**ï¼š
```python
# åªç¼“å­˜å¤æ‚è®¡ç®—ï¼Œç®€å•è®¡ç®—ç›´æ¥æ‰§è¡Œ
def should_cache_computation(compute_time: float) -> bool:
    return compute_time > 0.01  # åªç¼“å­˜>10msçš„è®¡ç®—

if should_cache_computation(expected_time):
    result = get_from_cache_or_compute()
else:
    result = compute_directly()
```

## ğŸ”¬ V2ç³»ç»Ÿç¼“å­˜é—®é¢˜åˆ†æ

### 1. å¤šæ¨¡æ€ç‰¹å¾æå–ç¼“å­˜é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
- ç‰¹å¾ç¼“å­˜å‘½ä¸­ç‡50%ï¼Œä½äºV1çš„CLIPç¼“å­˜
- ç›¸åŒå›¾ç‰‡ä¸åŒæè¿°åº”è¯¥éƒ¨åˆ†é‡ç”¨ç‰¹å¾

**æ ¹æœ¬åŸå› **ï¼š
```python
# å½“å‰å®ç° - å›¾ç‰‡å’Œæ–‡æœ¬å¼ºè€¦åˆ
cache_key = f"features_{hashlib.md5((image_url + text_description).encode()).hexdigest()}"

# é—®é¢˜ï¼šå›¾ç‰‡ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾è¢«ç»‘å®šï¼Œæ— æ³•ç‹¬ç«‹é‡ç”¨
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **åˆ†ç¦»å¼ç‰¹å¾ç¼“å­˜**ï¼š
   ```python
   class SeparatedFeatureCache:
       def __init__(self):
           self.image_cache = SmartCacheManager(cache_dir="cache/v2_image_features")
           self.text_cache = SmartCacheManager(cache_dir="cache/v2_text_features")
           self.attr_cache = SmartCacheManager(cache_dir="cache/v2_attr_features")
       
       def get_features(self, image_url: str, text: str, metadata: Dict):
           # ç‹¬ç«‹ç¼“å­˜å„æ¨¡æ€ç‰¹å¾
           visual = self.image_cache.get(image_url) or extract_visual_features(image_url)
           textual = self.text_cache.get(text) or extract_text_features(text)
           attributes = self.attr_cache.get(str(metadata)) or extract_attr_features(metadata)
           
           return {'visual': visual, 'text': textual, 'attributes': attributes}
   ```

2. **å±‚æ¬¡åŒ–ç‰¹å¾å¤ç”¨**ï¼š
   ```python
   # Level 1: å®Œå…¨åŒ¹é…
   exact_match = cache.get(f"exact_{image_url}_{text}")
   
   # Level 2: å›¾ç‰‡åŒ¹é…ï¼Œæ–‡æœ¬ç›¸ä¼¼
   if not exact_match:
       similar_text_match = cache.get_similar(f"image_{image_url}", text_similarity_threshold=0.8)
   
   # Level 3: ä»…å›¾ç‰‡åŒ¹é…
   if not similar_text_match:
       image_only_match = cache.get(f"image_only_{image_url}")
   ```

### 2. æ³¨æ„åŠ›èåˆç¼“å­˜é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
- èåˆç¼“å­˜å‘½ä¸­ç‡50%ï¼Œå¼ é‡ç›¸ä¼¼æ€§æ£€æµ‹ä¸å¤Ÿç²¾ç¡®
- ç›¸ä¼¼ç‰¹å¾ç»„åˆåº”è¯¥äº§ç”Ÿç›¸ä¼¼èåˆç»“æœ

**æ ¹æœ¬åŸå› **ï¼š
```python
# å½“å‰å®ç° - å¯¹å¼ é‡å˜åŒ–è¿‡äºæ•æ„Ÿ
feature_signature = f"{visual_features.sum().item():.6f}"

# é—®é¢˜ï¼šå¾®å°çš„æ•°å€¼å˜åŒ–å¯¼è‡´ç­¾åå®Œå…¨ä¸åŒ
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **å¼ é‡ç›¸ä¼¼æ€§åº¦é‡**ï¼š
   ```python
   def tensor_similarity_signature(tensor: torch.Tensor, bins: int = 10) -> str:
       # ä½¿ç”¨ç›´æ–¹å›¾è€Œéç²¾ç¡®å€¼
       hist = torch.histc(tensor, bins=bins)
       hist_normalized = hist / hist.sum()
       return hashlib.md5(str(hist_normalized.tolist()).encode()).hexdigest()
   ```

2. **åˆ†å±‚èåˆç¼“å­˜**ï¼š
   ```python
   # Level 1: ç²¾ç¡®åŒ¹é…
   exact_key = precise_tensor_signature(visual, text, attr)
   
   # Level 2: è¿‘ä¼¼åŒ¹é…  
   approx_key = approximate_tensor_signature(visual, text, attr)
   
   # Level 3: ç»“æ„åŒ¹é…
   struct_key = structural_signature(visual.shape, text.shape, attr.shape)
   ```

## âš¡ å¤šæ ·åŒ–è¾“å…¥åœºæ™¯çš„ç¼“å­˜ç­–ç•¥

### åœºæ™¯1ï¼šç›¸åŒå†…å®¹ï¼Œä¸åŒè¡¨è¾¾

**è¾“å…¥å˜åŒ–**ï¼š
- "pink floral cocktail" â†’ "pink flower drink"
- "é«˜è´¨é‡å›¾ç‰‡" â†’ "high quality image"

**ç¼“å­˜ç­–ç•¥**ï¼š
```python
class SemanticCacheManager(SmartCacheManager):
    def __init__(self):
        super().__init__()
        self.semantic_index = {}  # è¯­ä¹‰ç´¢å¼•
    
    def get_semantic_match(self, key: str, semantic_threshold: float = 0.8):
        # 1. ç²¾ç¡®åŒ¹é…
        exact = self.get(key)
        if exact:
            return exact
        
        # 2. è¯­ä¹‰åŒ¹é…
        for cached_key, semantic_sig in self.semantic_index.items():
            if self._semantic_similarity(key, cached_key) >= semantic_threshold:
                return self.get(cached_key)
        
        return None
```

### åœºæ™¯2ï¼šéƒ¨åˆ†å†…å®¹å˜åŒ–

**è¾“å…¥å˜åŒ–**ï¼š
- å›¾ç‰‡ç›¸åŒï¼ŒæŸ¥è¯¢ä¸åŒ
- æŸ¥è¯¢ç›¸åŒï¼Œå›¾ç‰‡ç›¸ä¼¼

**ç¼“å­˜ç­–ç•¥**ï¼š
```python
class HybridCacheStrategy:
    def get_partial_match(self, image_url: str, query: str):
        # ä¼˜å…ˆçº§1ï¼šå®Œå…¨åŒ¹é…
        full_match = self.cache.get(f"{image_url}_{query}")
        if full_match:
            return full_match, "full_match"
        
        # ä¼˜å…ˆçº§2ï¼šå›¾ç‰‡åŒ¹é…ï¼ŒæŸ¥è¯¢ç›¸ä¼¼
        similar_queries = self.find_similar_queries(query, threshold=0.8)
        for similar_query in similar_queries:
            partial_match = self.cache.get(f"{image_url}_{similar_query}")
            if partial_match:
                return self.adapt_result(partial_match, query), "adapted_match"
        
        # ä¼˜å…ˆçº§3ï¼šåŸºç¡€ç‰¹å¾é‡ç”¨
        base_features = self.cache.get(f"base_{image_url}")
        if base_features:
            return self.compute_incremental(base_features, query), "incremental"
        
        return None, "cache_miss"
```

### åœºæ™¯3ï¼šåŠ¨æ€å‚æ•°è°ƒæ•´

**è¾“å…¥å˜åŒ–**ï¼š
- æ£€æµ‹é˜ˆå€¼ï¼š0.5 â†’ 0.51 â†’ 0.49
- æƒé‡è°ƒæ•´ï¼šw_c=0.7 â†’ w_c=0.71

**ç¼“å­˜ç­–ç•¥**ï¼š
```python
class ParameterTolerantCache:
    def __init__(self, tolerance: Dict[str, float]):
        self.tolerance = tolerance  # å‚æ•°å®¹å¿åº¦
        self.parameter_cache = {}
    
    def get_tolerant_match(self, params: Dict[str, float]):
        for cached_params, result in self.parameter_cache.items():
            if self._within_tolerance(params, cached_params):
                # å‚æ•°åœ¨å®¹å¿èŒƒå›´å†…ï¼Œç›´æ¥é‡ç”¨
                return result
        
        return None
    
    def _within_tolerance(self, params1: Dict, params2: Dict) -> bool:
        for key, value1 in params1.items():
            value2 = params2.get(key, 0)
            tolerance = self.tolerance.get(key, 0.01)
            if abs(value1 - value2) > tolerance:
                return False
        return True
```

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

### 1. ç¼“å­˜ç²’åº¦é€‰æ‹©

| åœºæ™¯ | æ¨èç²’åº¦ | åŸå›  |
|------|----------|------|
| å›¾åƒç‰¹å¾æå– | ç²—ç²’åº¦ | è®¡ç®—æˆæœ¬é«˜ï¼Œå®¹å¿å°å¹…å˜åŒ– |
| æ–‡æœ¬è¯­ä¹‰ç†è§£ | ä¸­ç²’åº¦ | è¯­ä¹‰ç›¸ä¼¼å¯é‡ç”¨ |
| æ•°å€¼è®¡ç®— | ç»†ç²’åº¦ | è®¡ç®—å¿«é€Ÿï¼Œè¦æ±‚ç²¾ç¡® |
| æ¨¡å‹æ¨ç† | è‡ªé€‚åº” | æ ¹æ®æ¨¡å‹å¤æ‚åº¦è°ƒæ•´ |

### 2. TTLç­–ç•¥ä¼˜åŒ–

```python
class AdaptiveTTLManager:
    def calculate_ttl(self, compute_cost: float, hit_frequency: float) -> int:
        # è®¡ç®—æˆæœ¬é«˜ + å‘½ä¸­é¢‘ç‡é«˜ = é•¿TTL
        base_ttl = 3600  # 1å°æ—¶åŸºç¡€TTL
        
        cost_multiplier = min(compute_cost / 0.1, 10)  # æœ€å¤š10å€
        frequency_multiplier = min(hit_frequency * 10, 5)  # æœ€å¤š5å€
        
        adaptive_ttl = int(base_ttl * cost_multiplier * frequency_multiplier)
        return min(adaptive_ttl, 24 * 3600)  # æœ€é•¿24å°æ—¶
```

### 3. å†…å­˜ç®¡ç†ç­–ç•¥

```python
class MemoryAwareCacheManager:
    def __init__(self, memory_limit_mb: int = 512):
        self.memory_limit = memory_limit_mb * 1024 * 1024
        self.current_memory = 0
        
    def smart_eviction(self):
        # ç»¼åˆè€ƒè™‘ï¼šå¤§å°ã€å¹´é¾„ã€å‘½ä¸­ç‡
        candidates = []
        for key, entry in self.cache.items():
            size = self.estimate_size(entry.result)
            age = (datetime.now() - entry.timestamp).total_seconds()
            hit_rate = entry.hit_count / max(age / 3600, 1)  # æ¯å°æ—¶å‘½ä¸­ç‡
            
            # é©±é€è¯„åˆ†ï¼šå¤§å°è¶Šå¤§ã€å¹´é¾„è¶Šè€ã€å‘½ä¸­ç‡è¶Šä½ = è¶Šå®¹æ˜“è¢«é©±é€
            eviction_score = size * age / max(hit_rate, 1)
            candidates.append((key, eviction_score))
        
        # é©±é€è¯„åˆ†æœ€é«˜çš„æ¡ç›®
        candidates.sort(key=lambda x: x[1], reverse=True)
        for key, _ in candidates[:len(candidates)//4]:  # é©±é€25%
            self.remove_entry(key)
```

## ğŸš¨ é£é™©æ§åˆ¶ä¸ç›‘æ§

### 1. ç¼“å­˜ä¸€è‡´æ€§ç›‘æ§

```python
class CacheConsistencyMonitor:
    def __init__(self):
        self.consistency_checks = 0
        self.inconsistency_detected = 0
    
    def verify_consistency(self, key: str, cached_result: Any, fresh_result: Any):
        self.consistency_checks += 1
        
        if not self._results_equivalent(cached_result, fresh_result):
            self.inconsistency_detected += 1
            logger.warning(f"ç¼“å­˜ä¸ä¸€è‡´æ£€æµ‹: {key}")
            
            # è‡ªåŠ¨ä¿®å¤ï¼šä½¿ç”¨æ–°ç»“æœæ›´æ–°ç¼“å­˜
            self.cache.set(key, fresh_result)
            
        return fresh_result if self.inconsistency_detected else cached_result
```

### 2. æ€§èƒ½é€€åŒ–æ£€æµ‹

```python
class PerformanceDegradationDetector:
    def __init__(self):
        self.baseline_metrics = {}
        self.current_metrics = {}
    
    def check_degradation(self, operation: str, duration: float):
        if operation not in self.baseline_metrics:
            self.baseline_metrics[operation] = duration
            return False
        
        baseline = self.baseline_metrics[operation]
        degradation_ratio = duration / baseline
        
        if degradation_ratio > 1.5:  # æ€§èƒ½ä¸‹é™50%
            logger.warning(f"æ€§èƒ½é€€åŒ–æ£€æµ‹: {operation} è€—æ—¶å¢åŠ  {degradation_ratio:.1f}x")
            return True
        
        return False
```

## ğŸ“Š ç¼“å­˜æ•ˆæœè¯„ä¼°

### å…³é”®æŒ‡æ ‡

1. **å‘½ä¸­ç‡ç›®æ ‡**ï¼š
   - V1 CLIPç¼“å­˜ï¼šâ‰¥70%
   - V1 YOLOç¼“å­˜ï¼šâ‰¥80%
   - V2ç‰¹å¾ç¼“å­˜ï¼šâ‰¥60%
   - V2èåˆç¼“å­˜ï¼šâ‰¥70%

2. **å»¶è¿Ÿæ”¹å–„**ï¼š
   - ç¼“å­˜å‘½ä¸­å»¶è¿Ÿï¼š<5ms
   - æ€»ä½“å»¶è¿Ÿé™ä½ï¼šâ‰¥30%

3. **å†…å­˜æ•ˆç‡**ï¼š
   - å†…å­˜ä½¿ç”¨ç‡ï¼š<80%
   - ç¼“å­˜ç©ºé—´åˆ©ç”¨ç‡ï¼šâ‰¥60%

### æ€§èƒ½éªŒè¯

ä»å®é™…è¿è¡Œç»“æœçœ‹ï¼š
- V1æ¨¡å¼ï¼šåœºæ™¯2ç›¸æ¯”åœºæ™¯1å»¶è¿Ÿä»334msé™åˆ°107msï¼ˆ68%æ”¹å–„ï¼‰
- æ··åˆæ¨¡å¼ï¼šåç»­åœºæ™¯å»¶è¿Ÿç¨³å®šåœ¨4-6msï¼ˆ99%æ”¹å–„ï¼‰
- åŒåˆ†æ•°è®¡ç®—ç¼“å­˜æ•ˆæœæœ€å¥½ï¼ˆ83.3%å‘½ä¸­ç‡ï¼‰

è¿™éªŒè¯äº†æˆ‘ä»¬çš„æ™ºèƒ½ç¼“å­˜ç­–ç•¥åœ¨V2æ•‘æ´è®¡åˆ’ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ ç»“è®º

é€šè¿‡æ™ºèƒ½ç­¾åæœºåˆ¶ã€åˆ†å±‚ç¼“å­˜ç­–ç•¥å’Œè‡ªé€‚åº”å‚æ•°ç®¡ç†ï¼Œæˆ‘ä»¬æˆåŠŸè§£å†³äº†V1å’ŒV2ç³»ç»Ÿä¸­çš„ç¼“å­˜ä¼˜åŒ–é—®é¢˜ã€‚å…³é”®æˆæœï¼š

1. **è§£å†³äº†è¾“å…¥å˜åŒ–é‡ç”¨é—®é¢˜**ï¼šè¯­ä¹‰ç­¾åå®ç°ç›¸ä¼¼è¾“å…¥çš„ç¼“å­˜é‡ç”¨
2. **ä¼˜åŒ–äº†å¤šæ ·åŒ–åœºæ™¯é€‚é…**ï¼šåˆ†å±‚åŒ¹é…ç­–ç•¥é€‚åº”ä¸åŒå˜åŒ–ç¨‹åº¦
3. **å»ºç«‹äº†ç³»ç»Ÿçº§ç¼“å­˜ç¼–æ’**ï¼šV1/V2/æ··åˆæ¨¡å¼çš„ç»Ÿä¸€ç®¡ç†
4. **æä¾›äº†æ€§èƒ½ç›‘æ§æœºåˆ¶**ï¼šç¡®ä¿ç¼“å­˜ç­–ç•¥æŒç»­ä¼˜åŒ–

è¿™ä¸ºV2æ•‘æ´è®¡åˆ’æä¾›äº†åšå®çš„æ€§èƒ½ä¼˜åŒ–åŸºç¡€ï¼Œç¡®ä¿åœ¨è½»é‡åŒ–éƒ¨ç½²ä¸­å®ç°æœ€ä¼˜çš„è®¡ç®—æ•ˆç‡ã€‚
#!/usr/bin/env python3
"""
V2.0æ•‘æ´è®¡åˆ’ - æ™ºèƒ½ç¼“å­˜ä¼˜åŒ–ç»„ä»¶
================================================================================
ç›®æ ‡: è§£å†³V1/V2ç³»ç»Ÿä¸­çš„é‡å¤è®¡ç®—é—®é¢˜ï¼Œé€šè¿‡æ™ºèƒ½ç¼“å­˜æœºåˆ¶æå‡æ€§èƒ½
é‡ç‚¹: è¾“å…¥å˜åŒ–æ£€æµ‹ã€ç»“æœé‡ç”¨ç­–ç•¥ã€å¤šæ ·åŒ–åœºæ™¯é€‚é…
åº”ç”¨: V1ç”Ÿäº§ä¼˜åŒ– + V2è½»é‡åŒ–éƒ¨ç½²çš„å…³é”®æ”¯æ’‘
================================================================================
"""

import hashlib
import json
import pickle
import time
import logging
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
import torch
from datetime import datetime, timedelta
from collections import OrderedDict, defaultdict
import threading
from abc import ABC, abstractmethod

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    result: Any
    timestamp: datetime
    hit_count: int = 0
    compute_time: float = 0.0
    input_signature: str = ""
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡"""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    total_compute_time_saved: float = 0.0
    avg_hit_rate: float = 0.0
    
    def update_hit_rate(self):
        if self.total_requests > 0:
            self.avg_hit_rate = self.cache_hits / self.total_requests

class InputSignatureGenerator:
    """è¾“å…¥ç­¾åç”Ÿæˆå™¨ - åˆ¤æ–­ä½•æ—¶å¯ä»¥é‡ç”¨ç¼“å­˜"""
    
    @staticmethod
    def generate_signature(input_data: Any, granularity: str = "precise") -> str:
        """ç”Ÿæˆè¾“å…¥ç­¾å
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            granularity: ç²¾ç¡®åº¦çº§åˆ« ("precise", "approximate", "semantic")
        """
        if granularity == "precise":
            return InputSignatureGenerator._precise_signature(input_data)
        elif granularity == "approximate":
            return InputSignatureGenerator._approximate_signature(input_data)
        elif granularity == "semantic":
            return InputSignatureGenerator._semantic_signature(input_data)
        else:
            raise ValueError(f"Unknown granularity: {granularity}")
    
    @staticmethod
    def _precise_signature(input_data: Any) -> str:
        """ç²¾ç¡®ç­¾å - ä»»ä½•å˜åŒ–éƒ½é‡æ–°è®¡ç®—"""
        if isinstance(input_data, dict):
            # å¯¹å­—å…¸æŒ‰é”®æ’åºç¡®ä¿ä¸€è‡´æ€§
            sorted_items = sorted(input_data.items())
            content = json.dumps(sorted_items, sort_keys=True, default=str)
        elif isinstance(input_data, (list, tuple)):
            content = json.dumps(input_data, default=str)
        elif isinstance(input_data, torch.Tensor):
            content = str(input_data.shape) + str(input_data.dtype) + str(input_data.sum().item())
        elif isinstance(input_data, np.ndarray):
            content = str(input_data.shape) + str(input_data.dtype) + str(input_data.sum())
        else:
            content = str(input_data)
        
        return hashlib.md5(content.encode()).hexdigest()
    
    @staticmethod
    def _approximate_signature(input_data: Any) -> str:
        """è¿‘ä¼¼ç­¾å - å°å¹…å˜åŒ–å¯ä»¥é‡ç”¨"""
        if isinstance(input_data, dict):
            # å¿½ç•¥å¾®å°çš„æ•°å€¼å·®å¼‚
            normalized_data = {}
            for k, v in input_data.items():
                if isinstance(v, (int, float)):
                    # ä¿ç•™3ä½å°æ•°ç²¾åº¦
                    normalized_data[k] = round(float(v), 3)
                elif isinstance(v, str):
                    # å¿½ç•¥å¤§å°å†™
                    normalized_data[k] = v.lower().strip()
                else:
                    normalized_data[k] = v
            content = json.dumps(normalized_data, sort_keys=True, default=str)
        elif isinstance(input_data, torch.Tensor):
            # åŸºäºå½¢çŠ¶å’Œè¿‘ä¼¼ç»Ÿè®¡
            stats = f"{input_data.shape}_{input_data.dtype}_{input_data.mean():.3f}_{input_data.std():.3f}"
            content = stats
        else:
            content = str(input_data)
        
        return hashlib.md5(content.encode()).hexdigest()
    
    @staticmethod
    def _semantic_signature(input_data: Any) -> str:
        """è¯­ä¹‰ç­¾å - åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§"""
        if isinstance(input_data, dict):
            # æå–å…³é”®è¯­ä¹‰å­—æ®µ
            semantic_fields = ['query', 'domain', 'category', 'type']
            semantic_data = {}
            for field in semantic_fields:
                if field in input_data:
                    value = input_data[field]
                    if isinstance(value, str):
                        # æå–å…³é”®è¯
                        keywords = set(value.lower().split())
                        semantic_data[field] = sorted(keywords)
                    else:
                        semantic_data[field] = value
            content = json.dumps(semantic_data, sort_keys=True)
        else:
            content = str(input_data)
        
        return hashlib.md5(content.encode()).hexdigest()

class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, 
                 max_size: int = 1000,
                 ttl_seconds: int = 3600,
                 enable_persistence: bool = True,
                 cache_dir: str = "cache"):
        """
        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl_seconds: ç¼“å­˜å­˜æ´»æ—¶é—´
            enable_persistence: æ˜¯å¦æŒä¹…åŒ–ç¼“å­˜
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.enable_persistence = enable_persistence
        self.cache_dir = Path(cache_dir)
        
        # å†…å­˜ç¼“å­˜
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.stats = CacheStats()
        self.lock = threading.RLock()
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if self.enable_persistence:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self._load_persistent_cache()
        
        logger.info(f"ğŸ—„ï¸ æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æœ€å¤§ç¼“å­˜: {max_size} æ¡ç›®")
        logger.info(f"   TTL: {ttl_seconds} ç§’")
        logger.info(f"   æŒä¹…åŒ–: {'å¯ç”¨' if enable_persistence else 'ç¦ç”¨'}")
    
    def get(self, 
            key: str, 
            input_data: Any = None,
            granularity: str = "precise") -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ
        
        Args:
            key: ç¼“å­˜é”®
            input_data: è¾“å…¥æ•°æ®ï¼ˆç”¨äºç­¾åéªŒè¯ï¼‰
            granularity: åŒ¹é…ç²¾ç¡®åº¦
        """
        with self.lock:
            self.stats.total_requests += 1
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if key in self.cache:
                entry = self.cache[key]
                
                # æ£€æŸ¥TTL
                if self._is_expired(entry):
                    self._remove_entry(key)
                    self.stats.cache_misses += 1
                    return None
                
                # æ£€æŸ¥è¾“å…¥ç­¾åï¼ˆå¦‚æœæä¾›ï¼‰
                if input_data is not None:
                    current_signature = InputSignatureGenerator.generate_signature(
                        input_data, granularity
                    )
                    if entry.input_signature != current_signature:
                        logger.debug(f"ğŸ”„ ç­¾åä¸åŒ¹é…ï¼Œç¼“å­˜å¤±æ•ˆ: {key}")
                        self.stats.cache_misses += 1
                        return None
                
                # ç¼“å­˜å‘½ä¸­
                entry.hit_count += 1
                self.cache.move_to_end(key)  # LRUæ›´æ–°
                self.stats.cache_hits += 1
                self.stats.total_compute_time_saved += entry.compute_time
                self.stats.update_hit_rate()
                
                logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {key}, èŠ‚çœæ—¶é—´: {entry.compute_time:.3f}s")
                return entry.result
            
            # æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜
            if self.enable_persistence:
                persistent_result = self._load_from_disk(key)
                if persistent_result is not None:
                    logger.debug(f"ğŸ’¾ ä»ç£ç›˜åŠ è½½ç¼“å­˜: {key}")
                    self.stats.cache_hits += 1
                    self.stats.update_hit_rate()
                    return persistent_result
            
            self.stats.cache_misses += 1
            self.stats.update_hit_rate()
            return None
    
    def set(self, 
            key: str, 
            result: Any, 
            input_data: Any = None,
            compute_time: float = 0.0,
            granularity: str = "precise",
            metadata: Dict[str, Any] = None) -> None:
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        with self.lock:
            # ç”Ÿæˆè¾“å…¥ç­¾å
            input_signature = ""
            if input_data is not None:
                input_signature = InputSignatureGenerator.generate_signature(
                    input_data, granularity
                )
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                key=key,
                result=result,
                timestamp=datetime.now(),
                compute_time=compute_time,
                input_signature=input_signature,
                metadata=metadata or {}
            )
            
            # æ·»åŠ åˆ°å†…å­˜ç¼“å­˜
            if key in self.cache:
                self.cache.pop(key)
            
            self.cache[key] = entry
            self.cache.move_to_end(key)
            
            # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
            while len(self.cache) > self.max_size:
                oldest_key = next(iter(self.cache))
                self._remove_entry(oldest_key)
            
            # æŒä¹…åŒ–åˆ°ç£ç›˜
            if self.enable_persistence:
                self._save_to_disk(key, entry)
            
            logger.debug(f"ğŸ’¾ ç¼“å­˜è®¾ç½®: {key}, è®¡ç®—æ—¶é—´: {compute_time:.3f}s")
    
    def _is_expired(self, entry: CacheEntry) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        return (datetime.now() - entry.timestamp).total_seconds() > self.ttl_seconds
    
    def _remove_entry(self, key: str) -> None:
        """ç§»é™¤ç¼“å­˜æ¡ç›®"""
        if key in self.cache:
            del self.cache[key]
        
        if self.enable_persistence:
            cache_file = self.cache_dir / f"{key}.pkl"
            if cache_file.exists():
                cache_file.unlink()
    
    def _load_from_disk(self, key: str) -> Optional[Any]:
        """ä»ç£ç›˜åŠ è½½ç¼“å­˜"""
        try:
            cache_file = self.cache_dir / f"{key}.pkl"
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    entry = pickle.load(f)
                
                if not self._is_expired(entry):
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                    self.cache[key] = entry
                    self.cache.move_to_end(key)
                    return entry.result
                else:
                    # è¿‡æœŸï¼Œåˆ é™¤æ–‡ä»¶
                    cache_file.unlink()
        except Exception as e:
            logger.warning(f"ä»ç£ç›˜åŠ è½½ç¼“å­˜å¤±è´¥ {key}: {e}")
        
        return None
    
    def _save_to_disk(self, key: str, entry: CacheEntry) -> None:
        """ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜"""
        try:
            cache_file = self.cache_dir / f"{key}.pkl"
            with open(cache_file, 'wb') as f:
                pickle.dump(entry, f)
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜å¤±è´¥ {key}: {e}")
    
    def _load_persistent_cache(self) -> None:
        """åŠ è½½æŒä¹…åŒ–ç¼“å­˜"""
        if not self.cache_dir.exists():
            return
        
        loaded_count = 0
        for cache_file in self.cache_dir.glob("*.pkl"):
            try:
                with open(cache_file, 'rb') as f:
                    entry = pickle.load(f)
                
                if not self._is_expired(entry):
                    key = cache_file.stem
                    self.cache[key] = entry
                    loaded_count += 1
                else:
                    cache_file.unlink()
            except Exception as e:
                logger.warning(f"åŠ è½½æŒä¹…åŒ–ç¼“å­˜å¤±è´¥ {cache_file}: {e}")
                cache_file.unlink()
        
        logger.info(f"ğŸ“‚ åŠ è½½æŒä¹…åŒ–ç¼“å­˜: {loaded_count} æ¡ç›®")
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.stats = CacheStats()
            
            if self.enable_persistence:
                for cache_file in self.cache_dir.glob("*.pkl"):
                    cache_file.unlink()
        
        logger.info("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        with self.lock:
            return {
                'cache_size': len(self.cache),
                'max_size': self.max_size,
                'total_requests': self.stats.total_requests,
                'cache_hits': self.stats.cache_hits,
                'cache_misses': self.stats.cache_misses,
                'hit_rate': self.stats.avg_hit_rate,
                'total_time_saved': self.stats.total_compute_time_saved,
                'avg_time_saved_per_hit': (
                    self.stats.total_compute_time_saved / max(self.stats.cache_hits, 1)
                )
            }

class V1CacheOptimizer:
    """V1.0ç³»ç»Ÿç¼“å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.clip_cache = SmartCacheManager(
            max_size=500, 
            ttl_seconds=7200,  # 2å°æ—¶
            cache_dir="cache/v1_clip"
        )
        self.yolo_cache = SmartCacheManager(
            max_size=300,
            ttl_seconds=3600,  # 1å°æ—¶  
            cache_dir="cache/v1_yolo"
        )
        self.dual_score_cache = SmartCacheManager(
            max_size=1000,
            ttl_seconds=1800,  # 30åˆ†é’Ÿ
            cache_dir="cache/v1_dual_score"
        )
        
        logger.info("ğŸš€ V1.0ç¼“å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def cached_clip_inference(self, image_url: str, text_queries: List[str]) -> Dict[str, float]:
        """ç¼“å­˜çš„CLIPæ¨ç†"""
        # åˆ›å»ºç¼“å­˜é”®
        cache_key = f"clip_{hashlib.md5((image_url + str(sorted(text_queries))).encode()).hexdigest()}"
        
        # è¾“å…¥æ•°æ®ç”¨äºç­¾åéªŒè¯
        input_data = {
            'image_url': image_url,
            'text_queries': sorted(text_queries)
        }
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = self.clip_cache.get(cache_key, input_data, granularity="approximate")
        if cached_result is not None:
            return cached_result
        
        # æ‰§è¡Œå®é™…è®¡ç®—
        start_time = time.time()
        result = self._execute_clip_inference(image_url, text_queries)
        compute_time = time.time() - start_time
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self.clip_cache.set(
            cache_key, 
            result, 
            input_data, 
            compute_time,
            granularity="approximate",
            metadata={
                'model': 'CLIP-ViT-B/32',
                'image_url': image_url,
                'query_count': len(text_queries)
            }
        )
        
        return result
    
    def cached_yolo_detection(self, image_url: str, confidence_threshold: float = 0.5) -> List[Dict]:
        """ç¼“å­˜çš„YOLOç›®æ ‡æ£€æµ‹"""
        cache_key = f"yolo_{hashlib.md5(image_url.encode()).hexdigest()}_{confidence_threshold}"
        
        input_data = {
            'image_url': image_url,
            'confidence_threshold': confidence_threshold
        }
        
        cached_result = self.yolo_cache.get(cache_key, input_data, granularity="precise")
        if cached_result is not None:
            return cached_result
        
        start_time = time.time()
        result = self._execute_yolo_detection(image_url, confidence_threshold)
        compute_time = time.time() - start_time
        
        self.yolo_cache.set(
            cache_key,
            result,
            input_data,
            compute_time,
            metadata={
                'model': 'YOLOv8',
                'confidence_threshold': confidence_threshold,
                'detection_count': len(result)
            }
        )
        
        return result
    
    def cached_dual_score_computation(self, 
                                    compliance_score: float,
                                    conflict_score: float,
                                    w_c: float = 0.7,
                                    w_n: float = 0.3) -> float:
        """ç¼“å­˜çš„åŒåˆ†æ•°è®¡ç®—"""
        # å¯¹äºç®€å•è®¡ç®—ï¼Œä½¿ç”¨è¿‘ä¼¼ç­¾å
        cache_key = f"dual_score_{compliance_score:.3f}_{conflict_score:.3f}_{w_c:.3f}_{w_n:.3f}"
        
        input_data = {
            'compliance_score': compliance_score,
            'conflict_score': conflict_score,
            'w_c': w_c,
            'w_n': w_n
        }
        
        cached_result = self.dual_score_cache.get(cache_key, input_data, granularity="approximate")
        if cached_result is not None:
            return cached_result
        
        start_time = time.time()
        result = self._execute_dual_score_computation(compliance_score, conflict_score, w_c, w_n)
        compute_time = time.time() - start_time
        
        self.dual_score_cache.set(
            cache_key,
            result,
            input_data,
            compute_time,
            granularity="approximate"
        )
        
        return result
    
    def _execute_clip_inference(self, image_url: str, text_queries: List[str]) -> Dict[str, float]:
        """æ‰§è¡Œå®é™…CLIPæ¨ç† (æ¨¡æ‹Ÿ)"""
        logger.info(f"ğŸ”„ æ‰§è¡ŒCLIPæ¨ç†: {image_url}")
        time.sleep(0.1)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        
        # æ¨¡æ‹Ÿç»“æœ
        results = {}
        for query in text_queries:
            results[query] = np.random.uniform(0.1, 0.9)
        
        return results
    
    def _execute_yolo_detection(self, image_url: str, confidence_threshold: float) -> List[Dict]:
        """æ‰§è¡Œå®é™…YOLOæ£€æµ‹ (æ¨¡æ‹Ÿ)"""
        logger.info(f"ğŸ”„ æ‰§è¡ŒYOLOæ£€æµ‹: {image_url}")
        time.sleep(0.2)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        
        # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ
        detections = []
        for i in range(np.random.randint(0, 5)):
            detections.append({
                'class': f'object_{i}',
                'confidence': np.random.uniform(confidence_threshold, 1.0),
                'bbox': [
                    np.random.randint(0, 800),
                    np.random.randint(0, 600),
                    np.random.randint(50, 200),
                    np.random.randint(50, 200)
                ]
            })
        
        return detections
    
    def _execute_dual_score_computation(self, compliance: float, conflict: float, 
                                      w_c: float, w_n: float) -> float:
        """æ‰§è¡Œå®é™…åŒåˆ†æ•°è®¡ç®—"""
        time.sleep(0.001)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        return w_c * compliance + w_n * (1 - conflict)
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆç¼“å­˜ç»Ÿè®¡"""
        return {
            'clip_cache': self.clip_cache.get_stats(),
            'yolo_cache': self.yolo_cache.get_stats(),
            'dual_score_cache': self.dual_score_cache.get_stats()
        }

class V2CacheOptimizer:
    """V2.0ç³»ç»Ÿç¼“å­˜ä¼˜åŒ–å™¨ - é’ˆå¯¹å¤šæ¨¡æ€èåˆçš„ç‰¹æ®Šéœ€æ±‚"""
    
    def __init__(self):
        self.feature_cache = SmartCacheManager(
            max_size=800,
            ttl_seconds=3600,
            cache_dir="cache/v2_features"
        )
        self.fusion_cache = SmartCacheManager(
            max_size=600,
            ttl_seconds=1800,
            cache_dir="cache/v2_fusion"
        )
        self.inference_cache = SmartCacheManager(
            max_size=400,
            ttl_seconds=1200,
            cache_dir="cache/v2_inference"
        )
        
        logger.info("ğŸ”¬ V2.0ç¼“å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def cached_multimodal_feature_extraction(self, 
                                           image_url: str,
                                           text_description: str,
                                           metadata: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """ç¼“å­˜çš„å¤šæ¨¡æ€ç‰¹å¾æå–"""
        # å¯¹äºV2.0ï¼Œä½¿ç”¨è¯­ä¹‰ç­¾åæ¥å…è®¸æ›´å¥½çš„é‡ç”¨
        cache_key = f"features_{hashlib.md5((image_url + text_description).encode()).hexdigest()}"
        
        input_data = {
            'image_url': image_url,
            'text_description': text_description,
            'metadata': metadata
        }
        
        # ä½¿ç”¨è¯­ä¹‰ç­¾åï¼Œå…è®¸ç›¸ä¼¼å†…å®¹é‡ç”¨
        cached_result = self.feature_cache.get(cache_key, input_data, granularity="semantic")
        if cached_result is not None:
            return cached_result
        
        start_time = time.time()
        result = self._execute_multimodal_feature_extraction(image_url, text_description, metadata)
        compute_time = time.time() - start_time
        
        self.feature_cache.set(
            cache_key,
            result,
            input_data,
            compute_time,
            granularity="semantic",
            metadata={
                'feature_dims': {k: v.shape for k, v in result.items()},
                'extraction_time': compute_time
            }
        )
        
        return result
    
    def cached_attention_fusion(self, 
                              visual_features: torch.Tensor,
                              text_features: torch.Tensor,
                              attr_features: torch.Tensor) -> torch.Tensor:
        """ç¼“å­˜çš„æ³¨æ„åŠ›èåˆ"""
        # ä¸ºå¼ é‡åˆ›å»ºç­¾å
        feature_signature = (
            f"{visual_features.shape}_{visual_features.sum().item():.6f}_"
            f"{text_features.shape}_{text_features.sum().item():.6f}_"
            f"{attr_features.shape}_{attr_features.sum().item():.6f}"
        )
        cache_key = f"fusion_{hashlib.md5(feature_signature.encode()).hexdigest()}"
        
        input_data = {
            'visual_features': visual_features,
            'text_features': text_features,
            'attr_features': attr_features
        }
        
        # å¯¹äºæ³¨æ„åŠ›èåˆï¼Œä½¿ç”¨è¿‘ä¼¼ç­¾å
        cached_result = self.fusion_cache.get(cache_key, input_data, granularity="approximate")
        if cached_result is not None:
            return cached_result
        
        start_time = time.time()
        result = self._execute_attention_fusion(visual_features, text_features, attr_features)
        compute_time = time.time() - start_time
        
        self.fusion_cache.set(
            cache_key,
            result,
            input_data,
            compute_time,
            granularity="approximate",
            metadata={
                'input_shapes': [visual_features.shape, text_features.shape, attr_features.shape],
                'output_shape': result.shape,
                'fusion_time': compute_time
            }
        )
        
        return result
    
    def _execute_multimodal_feature_extraction(self, 
                                             image_url: str,
                                             text_description: str,
                                             metadata: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """æ‰§è¡Œå®é™…å¤šæ¨¡æ€ç‰¹å¾æå– (æ¨¡æ‹Ÿ)"""
        logger.info(f"ğŸ”„ æ‰§è¡Œå¤šæ¨¡æ€ç‰¹å¾æå–: {image_url[:50]}...")
        time.sleep(0.3)  # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
        
        return {
            'visual': torch.randn(512),
            'text': torch.randn(384),
            'attributes': torch.randn(64)
        }
    
    def _execute_attention_fusion(self, 
                                visual_features: torch.Tensor,
                                text_features: torch.Tensor,
                                attr_features: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œå®é™…æ³¨æ„åŠ›èåˆ (æ¨¡æ‹Ÿ)"""
        time.sleep(0.05)  # æ¨¡æ‹Ÿæ³¨æ„åŠ›è®¡ç®—
        
        # ç®€åŒ–çš„èåˆé€»è¾‘
        combined = torch.cat([
            visual_features[:256],
            text_features[:256], 
            attr_features[:64]
        ])
        
        return combined

class CacheOrchestrator:
    """ç¼“å­˜ç¼–æ’å™¨ - åè°ƒV1å’ŒV2çš„ç¼“å­˜ç­–ç•¥"""
    
    def __init__(self):
        self.v1_optimizer = V1CacheOptimizer()
        self.v2_optimizer = V2CacheOptimizer()
        self.system_mode = "v1"  # å½“å‰ç³»ç»Ÿæ¨¡å¼
        
        logger.info("ğŸ¼ ç¼“å­˜ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def set_system_mode(self, mode: str):
        """è®¾ç½®ç³»ç»Ÿæ¨¡å¼"""
        if mode in ["v1", "v2", "hybrid"]:
            self.system_mode = mode
            logger.info(f"ğŸ”„ åˆ‡æ¢åˆ° {mode.upper()} æ¨¡å¼")
        else:
            raise ValueError(f"Unknown system mode: {mode}")
    
    def smart_pipeline_cache(self, 
                           image_url: str,
                           query: str,
                           metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ™ºèƒ½ç®¡é“ç¼“å­˜ - æ ¹æ®æ¨¡å¼é€‰æ‹©æœ€ä¼˜ç¼“å­˜ç­–ç•¥"""
        metadata = metadata or {}
        
        if self.system_mode == "v1":
            return self._v1_pipeline_cache(image_url, query, metadata)
        elif self.system_mode == "v2":
            return self._v2_pipeline_cache(image_url, query, metadata)
        elif self.system_mode == "hybrid":
            return self._hybrid_pipeline_cache(image_url, query, metadata)
    
    def _v1_pipeline_cache(self, image_url: str, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """V1.0ç®¡é“ç¼“å­˜"""
        results = {}
        
        # CLIPæ¨ç†
        clip_results = self.v1_optimizer.cached_clip_inference(
            image_url, [query]
        )
        results['clip'] = clip_results
        
        # YOLOæ£€æµ‹
        yolo_results = self.v1_optimizer.cached_yolo_detection(image_url)
        results['yolo'] = yolo_results
        
        # åŒåˆ†æ•°è®¡ç®—
        compliance_score = metadata.get('compliance_score', 0.8)
        conflict_score = metadata.get('conflict_score', 0.2)
        
        dual_score = self.v1_optimizer.cached_dual_score_computation(
            compliance_score, conflict_score
        )
        results['dual_score'] = dual_score
        
        return results
    
    def _v2_pipeline_cache(self, image_url: str, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """V2.0ç®¡é“ç¼“å­˜"""
        results = {}
        
        # å¤šæ¨¡æ€ç‰¹å¾æå–
        features = self.v2_optimizer.cached_multimodal_feature_extraction(
            image_url, query, metadata
        )
        results['features'] = features
        
        # æ³¨æ„åŠ›èåˆ
        fused_features = self.v2_optimizer.cached_attention_fusion(
            features['visual'], features['text'], features['attributes']
        )
        results['fused_features'] = fused_features
        
        return results
    
    def _hybrid_pipeline_cache(self, image_url: str, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """æ··åˆæ¨¡å¼ç®¡é“ç¼“å­˜"""
        results = {}
        
        # åŒæ—¶è¿è¡ŒV1å’ŒV2ï¼Œå¯¹æ¯”ç»“æœ
        v1_results = self._v1_pipeline_cache(image_url, query, metadata)
        v2_results = self._v2_pipeline_cache(image_url, query, metadata)
        
        results['v1'] = v1_results
        results['v2'] = v2_results
        results['mode'] = 'hybrid'
        
        return results
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿçº§ç¼“å­˜ç»Ÿè®¡"""
        return {
            'system_mode': self.system_mode,
            'v1_stats': self.v1_optimizer.get_comprehensive_stats(),
            'v2_stats': {
                'feature_cache': self.v2_optimizer.feature_cache.get_stats(),
                'fusion_cache': self.v2_optimizer.fusion_cache.get_stats(),
                'inference_cache': self.v2_optimizer.inference_cache.get_stats()
            }
        }
    
    def optimize_cache_strategy(self) -> Dict[str, str]:
        """åŸºäºç»Ÿè®¡æ•°æ®ä¼˜åŒ–ç¼“å­˜ç­–ç•¥"""
        stats = self.get_system_stats()
        recommendations = {}
        
        # åˆ†æV1ç¼“å­˜æ•ˆç‡
        v1_stats = stats['v1_stats']
        for cache_type, cache_stat in v1_stats.items():
            hit_rate = cache_stat['hit_rate']
            if hit_rate < 0.3:
                recommendations[f'v1_{cache_type}'] = "è€ƒè™‘å¢åŠ TTLæˆ–ä½¿ç”¨è¿‘ä¼¼ç­¾å"
            elif hit_rate > 0.8:
                recommendations[f'v1_{cache_type}'] = "ç¼“å­˜æ•ˆç‡å¾ˆå¥½ï¼Œå¯ä»¥é€‚å½“å¢åŠ ç¼“å­˜å¤§å°"
        
        # åˆ†æV2ç¼“å­˜æ•ˆç‡
        v2_stats = stats['v2_stats']
        for cache_type, cache_stat in v2_stats.items():
            hit_rate = cache_stat['hit_rate']
            if hit_rate < 0.2:
                recommendations[f'v2_{cache_type}'] = "è€ƒè™‘ä½¿ç”¨è¯­ä¹‰ç­¾åå¢å¼ºé‡ç”¨æ€§"
            elif hit_rate > 0.7:
                recommendations[f'v2_{cache_type}'] = "ç¼“å­˜æ•ˆç‡è‰¯å¥½"
        
        return recommendations

def demonstrate_cache_optimization():
    """æ¼”ç¤ºç¼“å­˜ä¼˜åŒ–æ•ˆæœ"""
    print("ğŸš€ V2æ•‘æ´è®¡åˆ’ - æ™ºèƒ½ç¼“å­˜ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºç¼“å­˜ç¼–æ’å™¨
    orchestrator = CacheOrchestrator()
    
    # æ¨¡æ‹Ÿè¯·æ±‚åœºæ™¯
    test_scenarios = [
        {
            'image_url': 'https://example.com/cocktail1.jpg',
            'query': 'pink floral cocktail',
            'metadata': {'domain': 'cocktails', 'quality_tier': 'high'}
        },
        {
            'image_url': 'https://example.com/cocktail1.jpg',  # ç›¸åŒå›¾ç‰‡
            'query': 'pink flower drink',  # ç›¸ä¼¼æŸ¥è¯¢
            'metadata': {'domain': 'cocktails', 'quality_tier': 'high'}
        },
        {
            'image_url': 'https://example.com/cocktail2.jpg',
            'query': 'pink floral cocktail',  # ç›¸åŒæŸ¥è¯¢ï¼Œä¸åŒå›¾ç‰‡
            'metadata': {'domain': 'cocktails', 'quality_tier': 'medium'}
        }
    ]
    
    # æµ‹è¯•ä¸åŒç³»ç»Ÿæ¨¡å¼
    for mode in ['v1', 'v2', 'hybrid']:
        print(f"\nğŸ”„ æµ‹è¯• {mode.upper()} æ¨¡å¼")
        print("-" * 40)
        
        orchestrator.set_system_mode(mode)
        
        for i, scenario in enumerate(test_scenarios):
            print(f"\nğŸ“ åœºæ™¯ {i+1}: {scenario['query'][:30]}...")
            
            start_time = time.time()
            results = orchestrator.smart_pipeline_cache(
                scenario['image_url'],
                scenario['query'],
                scenario['metadata']
            )
            process_time = time.time() - start_time
            
            print(f"   â±ï¸ å¤„ç†æ—¶é—´: {process_time:.3f}s")
            print(f"   ğŸ“Š ç»“æœé”®: {list(results.keys())}")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡æ€»ç»“")
    print("=" * 40)
    
    stats = orchestrator.get_system_stats()
    
    print(f"V1 ç¼“å­˜å‘½ä¸­ç‡:")
    for cache_type, cache_stat in stats['v1_stats'].items():
        print(f"   {cache_type}: {cache_stat['hit_rate']:.1%} "
              f"({cache_stat['cache_hits']}/{cache_stat['total_requests']})")
    
    print(f"\nV2 ç¼“å­˜å‘½ä¸­ç‡:")
    for cache_type, cache_stat in stats['v2_stats'].items():
        print(f"   {cache_type}: {cache_stat['hit_rate']:.1%} "
              f"({cache_stat['cache_hits']}/{cache_stat['total_requests']})")
    
    # ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    recommendations = orchestrator.optimize_cache_strategy()
    for component, suggestion in recommendations.items():
        print(f"   {component}: {suggestion}")
    
    print(f"\nâœ… ç¼“å­˜ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ")
    print("ğŸ¯ å…³é”®æ´å¯Ÿ:")
    print("   â€¢ æ™ºèƒ½ç­¾åæœºåˆ¶å®ç°äº†ç²¾ç¡®å’Œè¿‘ä¼¼åŒ¹é…çš„å¹³è¡¡")
    print("   â€¢ å¤šå±‚ç¼“å­˜æ¶æ„é€‚åº”V1/V2ä¸åŒçš„è®¡ç®—ç‰¹ç‚¹")
    print("   â€¢ æŒä¹…åŒ–ç¼“å­˜å‡å°‘äº†å†·å¯åŠ¨æ—¶é—´")
    print("   â€¢ ç»Ÿè®¡é©±åŠ¨çš„ä¼˜åŒ–ç¡®ä¿ç¼“å­˜ç­–ç•¥æŒç»­æ”¹è¿›")

if __name__ == "__main__":
    demonstrate_cache_optimization()
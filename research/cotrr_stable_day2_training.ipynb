{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922ce554",
   "metadata": {},
   "source": [
    "# ğŸš€ CoTRR-Stable Day 2: GPUåŠ é€Ÿè®­ç»ƒ & é«˜çº§åŠŸèƒ½å®ç°\n",
    "\n",
    "## ğŸ“Š é¡¹ç›®çŠ¶æ€\n",
    "- **Day 1å®Œæˆ**: âœ… Cross-Attentionæ¶æ„ + ListMLE+Focal Loss + è®­ç»ƒPipeline\n",
    "- **Day 2ç›®æ ‡**: Isotonicæ ¡å‡† + Step5é›†æˆ + GPUåŠ é€Ÿè®­ç»ƒ\n",
    "- **è®¡ç®—èµ„æº**: Google Colab Pay-as-you-go + æœ¬åœ°MPSåŠ é€Ÿ\n",
    "\n",
    "## ğŸ¯ ä»Šæ—¥ä»»åŠ¡\n",
    "- **T004**: Isotonicæ ¡å‡†å®ç° (æ¦‚ç‡æ ¡å‡†)\n",
    "- **T005**: Step5é›†æˆæ¥å£ (ç”Ÿäº§å°±ç»ª)\n",
    "- **T006**: åˆæ­¥è®­ç»ƒæµ‹è¯• (ç«¯åˆ°ç«¯éªŒè¯)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c36c3",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç¯å¢ƒè®¾ç½® & GPUæ£€æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUä¿¡æ¯æ£€æµ‹ (Google Colab & æœ¬åœ°å…¼å®¹)\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def check_gpu_info():\n",
    "    \"\"\"æ£€æµ‹GPUä¿¡æ¯ - Colab & æœ¬åœ°å…¼å®¹\"\"\"\n",
    "    try:\n",
    "        # å°è¯•nvidia-smi (Colab/CUDA)\n",
    "        gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "        if 'failed' not in gpu_info.lower():\n",
    "            print(\"ğŸ”¥ NVIDIA GPUæ£€æµ‹æˆåŠŸ:\")\n",
    "            print(gpu_info)\n",
    "            return 'cuda'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # æ£€æµ‹Apple Silicon MPS\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"ğŸ Apple Silicon MPSå¯ç”¨\")\n",
    "            return 'mps'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"ğŸ’» ä½¿ç”¨CPUæ¨¡å¼\")\n",
    "    return 'cpu'\n",
    "\n",
    "gpu_type = check_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½ä¼˜åŒ–è®¾ç½®\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®å·¥ä½œç›®å½•\n",
    "WORK_DIR = Path.cwd()\n",
    "if 'content' in str(WORK_DIR):  # Google Colabç¯å¢ƒ\n",
    "    print(\"ğŸŒ Google Colabç¯å¢ƒæ£€æµ‹\")\n",
    "    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥ä»GitHubå…‹éš†é¡¹ç›®\n",
    "    # !git clone https://github.com/your-repo/computer-vision.git\n",
    "    # WORK_DIR = Path('/content/computer-vision')\n",
    "else:\n",
    "    print(f\"ğŸ  æœ¬åœ°ç¯å¢ƒ: {WORK_DIR}\")\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "sys.path.append(str(WORK_DIR))\n",
    "\n",
    "# PyTorchæ€§èƒ½ä¼˜åŒ–\n",
    "if gpu_type == 'cuda':\n",
    "    print(\"âš¡ CUDAæ€§èƒ½ä¼˜åŒ–\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "elif gpu_type == 'mps':\n",
    "    print(\"ğŸš€ MPSæ€§èƒ½ä¼˜åŒ–\")\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    \n",
    "    # BLASåŠ é€Ÿ (æœ¬åœ°MacOS)\n",
    "    try:\n",
    "        logical_cpus = int(subprocess.check_output(['sysctl', '-n', 'hw.logicalcpu'], encoding='utf-8').strip())\n",
    "        physical_cpus = int(subprocess.check_output(['sysctl', '-n', 'hw.physicalcpu'], encoding='utf-8').strip())\n",
    "        os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(logical_cpus)\n",
    "        os.environ[\"OMP_NUM_THREADS\"] = str(physical_cpus)\n",
    "        print(f\"ğŸ”§ BLASä¼˜åŒ–: {logical_cpus}é€»è¾‘æ ¸å¿ƒ, {physical_cpus}ç‰©ç†æ ¸å¿ƒ\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# è®¾å¤‡é…ç½®\n",
    "if gpu_type == 'cuda':\n",
    "    device = torch.device('cuda')\n",
    "elif gpu_type == 'mps':\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9976d49",
   "metadata": {},
   "source": [
    "## ğŸ“¦ å¯¼å…¥ä¾èµ– & Day 1æˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¸å¿ƒä¾èµ–\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss, log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®æ—¥å¿—\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… ä¾èµ–å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2efe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥Day 1å®ç°çš„ç»„ä»¶\n",
    "try:\n",
    "    from research.src.cotrr_stable import StableCrossAttnReranker, StableConfig\n",
    "    from research.src.listmle_focal_loss import CombinedRankingLoss, LossConfig, RankingTrainer\n",
    "    from research.src.training_pipeline import TrainingPipeline, TrainingConfig, Step5Dataset\n",
    "    from research.src.progress_tracker import CoTRRStageTracker\n",
    "    \n",
    "    print(\"âœ… Day 1ç»„ä»¶å¯¼å…¥æˆåŠŸ\")\n",
    "    \n",
    "    # å¿«é€ŸéªŒè¯æ ¸å¿ƒç»„ä»¶\n",
    "    config = StableConfig()\n",
    "    model = StableCrossAttnReranker(config)\n",
    "    print(f\"ğŸ”§ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"ğŸ”„ è¯·ç¡®ä¿å·²å®ŒæˆDay 1çš„å®ç°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94020b",
   "metadata": {},
   "source": [
    "## ğŸ¯ Task T004: Isotonicæ ¡å‡†å®ç°\n",
    "\n",
    "### æ¦‚ç‡æ ¡å‡†ç†è®º\n",
    "- **ç›®æ ‡**: å°†æ¨¡å‹è¾“å‡ºæ ¡å‡†ä¸ºå¯é çš„æ¦‚ç‡ä¼°è®¡\n",
    "- **æ–¹æ³•**: Isotonic Regression (å•è°ƒå›å½’)\n",
    "- **ä¼˜åŠ¿**: éå‚æ•°åŒ–ï¼Œé€‚ç”¨äºä»»æ„åˆ†å¸ƒ\n",
    "- **æŒ‡æ ‡**: ECE (Expected Calibration Error) â‰¤ 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsotonicCalibrator:\n",
    "    \"\"\"\n",
    "    Isotonic Regressionæ ¡å‡†å™¨\n",
    "    ç”¨äºå°†æ¨¡å‹è¾“å‡ºæ ¡å‡†ä¸ºå¯é çš„æ¦‚ç‡ä¼°è®¡\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, out_of_bounds='clip'):\n",
    "        self.calibrator = IsotonicRegression(out_of_bounds=out_of_bounds)\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, scores: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"\n",
    "        æ‹Ÿåˆæ ¡å‡†å™¨\n",
    "        Args:\n",
    "            scores: æ¨¡å‹åŸå§‹åˆ†æ•° [N,]\n",
    "            labels: äºŒå€¼æ ‡ç­¾ [N,] (0/1)\n",
    "        \"\"\"\n",
    "        # å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡\n",
    "        probs = torch.sigmoid(torch.tensor(scores)).numpy()\n",
    "        \n",
    "        # æ‹Ÿåˆisotonic regression\n",
    "        self.calibrator.fit(probs, labels)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # è®¡ç®—æ ¡å‡†å‰åçš„æŒ‡æ ‡\n",
    "        calibrated_probs = self.calibrator.predict(probs)\n",
    "        \n",
    "        original_ece = self._compute_ece(probs, labels)\n",
    "        calibrated_ece = self._compute_ece(calibrated_probs, labels)\n",
    "        \n",
    "        logger.info(f\"ğŸ“Š æ ¡å‡†æ•ˆæœ: ECE {original_ece:.4f} â†’ {calibrated_ece:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'original_ece': original_ece,\n",
    "            'calibrated_ece': calibrated_ece,\n",
    "            'improvement': original_ece - calibrated_ece\n",
    "        }\n",
    "    \n",
    "    def predict(self, scores: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        æ ¡å‡†é¢„æµ‹æ¦‚ç‡\n",
    "        Args:\n",
    "            scores: æ¨¡å‹åŸå§‹åˆ†æ•°\n",
    "        Returns:\n",
    "            æ ¡å‡†åçš„æ¦‚ç‡\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"æ ¡å‡†å™¨æœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()\")\n",
    "        \n",
    "        probs = torch.sigmoid(torch.tensor(scores)).numpy()\n",
    "        return self.calibrator.predict(probs)\n",
    "    \n",
    "    def _compute_ece(self, probs: np.ndarray, labels: np.ndarray, n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        è®¡ç®—Expected Calibration Error\n",
    "        \"\"\"\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_lowers = bin_boundaries[:-1]\n",
    "        bin_uppers = bin_boundaries[1:]\n",
    "        \n",
    "        ece = 0\n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = (probs > bin_lower) & (probs <= bin_upper)\n",
    "            prop_in_bin = in_bin.mean()\n",
    "            \n",
    "            if prop_in_bin > 0:\n",
    "                accuracy_in_bin = labels[in_bin].mean()\n",
    "                avg_confidence_in_bin = probs[in_bin].mean()\n",
    "                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        \n",
    "        return ece\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"ä¿å­˜æ ¡å‡†å™¨\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.calibrator, f)\n",
    "        logger.info(f\"ğŸ’¾ æ ¡å‡†å™¨å·²ä¿å­˜: {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"åŠ è½½æ ¡å‡†å™¨\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            self.calibrator = pickle.load(f)\n",
    "        self.is_fitted = True\n",
    "        logger.info(f\"ğŸ“‚ æ ¡å‡†å™¨å·²åŠ è½½: {path}\")\n",
    "\n",
    "print(\"âœ… IsotonicCalibratorå®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18376073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•Isotonicæ ¡å‡†å™¨\n",
    "def test_isotonic_calibrator():\n",
    "    \"\"\"æµ‹è¯•æ ¡å‡†å™¨åŠŸèƒ½\"\"\"\n",
    "    logger.info(\"ğŸ§ª æµ‹è¯•Isotonicæ ¡å‡†å™¨\")\n",
    "    \n",
    "    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ® - è¿‡åº¦è‡ªä¿¡çš„æ¨¡å‹è¾“å‡º\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # æ¨¡æ‹ŸåŸå§‹åˆ†æ•° (logits)\n",
    "    true_probs = np.random.beta(2, 5, n_samples)  # çœŸå®æ¦‚ç‡åå‘0\n",
    "    labels = np.random.binomial(1, true_probs)    # å¯¹åº”çš„æ ‡ç­¾\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè¿‡åº¦è‡ªä¿¡çš„æ¨¡å‹ (åˆ†æ•°åé«˜)\n",
    "    raw_scores = np.log(true_probs / (1 - true_probs + 1e-8)) + np.random.normal(1.0, 0.5, n_samples)\n",
    "    \n",
    "    # åˆ›å»ºå¹¶è®­ç»ƒæ ¡å‡†å™¨\n",
    "    calibrator = IsotonicCalibrator()\n",
    "    metrics = calibrator.fit(raw_scores, labels)\n",
    "    \n",
    "    # æµ‹è¯•é¢„æµ‹\n",
    "    test_scores = np.random.normal(0, 2, 100)\n",
    "    calibrated_probs = calibrator.predict(test_scores)\n",
    "    \n",
    "    logger.info(f\"âœ… æ ¡å‡†æµ‹è¯•å®Œæˆ\")\n",
    "    logger.info(f\"   ECEæ”¹å–„: {metrics['improvement']:.4f}\")\n",
    "    logger.info(f\"   æ ¡å‡†æ¦‚ç‡èŒƒå›´: [{calibrated_probs.min():.3f}, {calibrated_probs.max():.3f}]\")\n",
    "    \n",
    "    return calibrator, metrics\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "test_calibrator, test_metrics = test_isotonic_calibrator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b476b",
   "metadata": {},
   "source": [
    "## ğŸ”— Task T005: Step5é›†æˆæ¥å£å®ç°\n",
    "\n",
    "### é›†æˆç­–ç•¥\n",
    "- **è¾“å…¥**: Step4çš„å€™é€‰ç»“æœ + Step5çš„ç‰¹å¾\n",
    "- **è¾“å‡º**: é‡æ’åºåçš„å€™é€‰åˆ—è¡¨\n",
    "- **æ€§èƒ½**: Top-Mç­–ç•¥ï¼Œåªå¯¹å‰20ä¸ªå€™é€‰ä½¿ç”¨å¤æ‚æ¨¡å‹\n",
    "- **å…¼å®¹**: æ— ç¼æ›¿æ¢ç°æœ‰Step5é€»è¾‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTRRStableStep5Integration:\n",
    "    \"\"\"\n",
    "    CoTRR-Stableä¸Step5çš„å®Œæ•´é›†æˆæ¥å£\n",
    "    æ”¯æŒç”Ÿäº§ç¯å¢ƒçš„æ— ç¼æ›¿æ¢\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_path: str,\n",
    "                 calibrator_path: str,\n",
    "                 device: str = 'auto',\n",
    "                 top_m: int = 20,\n",
    "                 enable_compilation: bool = True):\n",
    "        \n",
    "        self.top_m = top_m\n",
    "        self.device = self._setup_device(device)\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        self.model = self._load_model(model_path)\n",
    "        if enable_compilation:\n",
    "            try:\n",
    "                self.model = torch.compile(self.model, backend=\"inductor\")\n",
    "                logger.info(\"âš¡ æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å¯ç”¨\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}\")\n",
    "        \n",
    "        # åŠ è½½æ ¡å‡†å™¨\n",
    "        self.calibrator = IsotonicCalibrator()\n",
    "        try:\n",
    "            self.calibrator.load(calibrator_path)\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"æ ¡å‡†å™¨æ–‡ä»¶ä¸å­˜åœ¨: {calibrator_path}ï¼Œå°†ä½¿ç”¨åŸå§‹åˆ†æ•°\")\n",
    "            self.calibrator = None\n",
    "        \n",
    "        # æ€§èƒ½ç»Ÿè®¡\n",
    "        self.stats = {\n",
    "            'total_queries': 0,\n",
    "            'reranked_queries': 0,\n",
    "            'avg_inference_time': 0.0,\n",
    "            'avg_candidates_per_query': 0.0\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"ğŸš€ CoTRR-Stableé›†æˆæ¥å£åˆå§‹åŒ–å®Œæˆ (device: {self.device})\")\n",
    "    \n",
    "    def _setup_device(self, device: str) -> torch.device:\n",
    "        \"\"\"è®¾å¤‡é…ç½®\"\"\"\n",
    "        if device == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                return torch.device('cuda')\n",
    "            elif torch.backends.mps.is_available():\n",
    "                return torch.device('mps')\n",
    "            else:\n",
    "                return torch.device('cpu')\n",
    "        return torch.device(device)\n",
    "    \n",
    "    def _load_model(self, model_path: str) -> nn.Module:\n",
    "        \"\"\"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=self.device)\n",
    "            \n",
    "            # ä»checkpointä¸­æ¢å¤é…ç½®\n",
    "            config = StableConfig(**checkpoint.get('config', {}))\n",
    "            model = StableCrossAttnReranker(config)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            \n",
    "            logger.info(f\"ğŸ“‚ æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}\")\n",
    "            return model\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹åŒ–\")\n",
    "            config = StableConfig()\n",
    "            model = StableCrossAttnReranker(config)\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            return model\n",
    "    \n",
    "    def rerank_candidates(self, \n",
    "                         query_data: Dict[str, Any],\n",
    "                         candidates: List[Dict[str, Any]],\n",
    "                         return_scores: bool = False) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        é‡æ’åºå€™é€‰ç»“æœ - Step5å…¼å®¹æ¥å£\n",
    "        \n",
    "        Args:\n",
    "            query_data: æŸ¥è¯¢ä¿¡æ¯ {'query_id', 'query_text', ...}\n",
    "            candidates: å€™é€‰åˆ—è¡¨ [{'candidate_id', 'text_features', 'image_features', 'raw_score', ...}]\n",
    "            return_scores: æ˜¯å¦è¿”å›é‡æ’åºåˆ†æ•°\n",
    "        \n",
    "        Returns:\n",
    "            é‡æ’åºåçš„å€™é€‰åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.stats['total_queries'] += 1\n",
    "        \n",
    "        # å¦‚æœå€™é€‰æ•°é‡å°‘äºç­‰äº1ï¼Œç›´æ¥è¿”å›\n",
    "        if len(candidates) <= 1:\n",
    "            return candidates\n",
    "        \n",
    "        # Top-Mç­–ç•¥ï¼šåªå¯¹å‰Mä¸ªå€™é€‰è¿›è¡Œå¤æ‚é‡æ’\n",
    "        top_candidates = candidates[:min(len(candidates), self.top_m)]\n",
    "        remaining_candidates = candidates[self.top_m:] if len(candidates) > self.top_m else []\n",
    "        \n",
    "        if len(top_candidates) <= 1:\n",
    "            return candidates\n",
    "        \n",
    "        # æå–ç‰¹å¾\n",
    "        features = self._extract_features(top_candidates)\n",
    "        \n",
    "        # æ¨¡å‹æ¨ç†\n",
    "        with torch.no_grad():\n",
    "            if self.device.type in ['cuda', 'mps']:\n",
    "                with autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "                    scores = self._model_inference(features)\n",
    "            else:\n",
    "                scores = self._model_inference(features)\n",
    "        \n",
    "        # æ¦‚ç‡æ ¡å‡†\n",
    "        if self.calibrator is not None:\n",
    "            calibrated_scores = self.calibrator.predict(scores.cpu().numpy())\n",
    "            scores = torch.tensor(calibrated_scores)\n",
    "        \n",
    "        # é‡æ’åº\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        reranked_top = [top_candidates[i] for i in sorted_indices]\n",
    "        \n",
    "        # æ·»åŠ é‡æ’åºåˆ†æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if return_scores:\n",
    "            for i, candidate in enumerate(reranked_top):\n",
    "                candidate['cotrr_score'] = float(scores[sorted_indices[i]])\n",
    "                candidate['cotrr_rank'] = i + 1\n",
    "        \n",
    "        # åˆå¹¶ç»“æœ\n",
    "        final_results = reranked_top + remaining_candidates\n",
    "        \n",
    "        # æ›´æ–°ç»Ÿè®¡\n",
    "        self.stats['reranked_queries'] += 1\n",
    "        inference_time = time.time() - start_time\n",
    "        self.stats['avg_inference_time'] = (\n",
    "            self.stats['avg_inference_time'] * (self.stats['reranked_queries'] - 1) + inference_time\n",
    "        ) / self.stats['reranked_queries']\n",
    "        self.stats['avg_candidates_per_query'] = (\n",
    "            self.stats['avg_candidates_per_query'] * (self.stats['reranked_queries'] - 1) + len(candidates)\n",
    "        ) / self.stats['reranked_queries']\n",
    "        \n",
    "        return final_results\n",
    "    \n",
    "    def _extract_features(self, candidates: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"ä»å€™é€‰ä¸­æå–ç‰¹å¾\"\"\"\n",
    "        text_features = []\n",
    "        image_features = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            # ç¡®ä¿ç‰¹å¾å­˜åœ¨\n",
    "            text_feat = candidate.get('text_features', np.zeros(256))\n",
    "            image_feat = candidate.get('image_features', np.zeros(256))\n",
    "            \n",
    "            if isinstance(text_feat, list):\n",
    "                text_feat = np.array(text_feat)\n",
    "            if isinstance(image_feat, list):\n",
    "                image_feat = np.array(image_feat)\n",
    "            \n",
    "            text_features.append(text_feat)\n",
    "            image_features.append(image_feat)\n",
    "        \n",
    "        return {\n",
    "            'text_features': torch.tensor(text_features, dtype=torch.float32).unsqueeze(0).to(self.device),\n",
    "            'image_features': torch.tensor(image_features, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        }\n",
    "    \n",
    "    def _model_inference(self, features: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"æ¨¡å‹æ¨ç†\"\"\"\n",
    "        batch_size, num_candidates, feature_dim = features['text_features'].shape\n",
    "        \n",
    "        # Reshapeä¸ºæ¨¡å‹æœŸæœ›çš„è¾“å…¥æ ¼å¼\n",
    "        clip_text = features['text_features'].view(-1, feature_dim)\n",
    "        clip_img = features['image_features'].view(-1, feature_dim)\n",
    "        visual_features = torch.zeros_like(clip_img)\n",
    "        conflict_features = torch.zeros_like(clip_img)\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        result = self.model(clip_img, clip_text, visual_features, conflict_features)\n",
    "        scores = result['logits'].view(batch_size, num_candidates).squeeze(0)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_performance_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–æ€§èƒ½ç»Ÿè®¡\"\"\"\n",
    "        return self.stats.copy()\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"é‡ç½®ç»Ÿè®¡\"\"\"\n",
    "        self.stats = {\n",
    "            'total_queries': 0,\n",
    "            'reranked_queries': 0,\n",
    "            'avg_inference_time': 0.0,\n",
    "            'avg_candidates_per_query': 0.0\n",
    "        }\n",
    "\n",
    "print(\"âœ… CoTRRStableStep5Integrationå®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d706d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•Step5é›†æˆæ¥å£\n",
    "def test_step5_integration():\n",
    "    \"\"\"æµ‹è¯•Step5é›†æˆåŠŸèƒ½\"\"\"\n",
    "    logger.info(\"ğŸ§ª æµ‹è¯•Step5é›†æˆæ¥å£\")\n",
    "    \n",
    "    # åˆ›å»ºé›†æˆæ¥å£ (ä½¿ç”¨é»˜è®¤æ¨¡å‹)\n",
    "    integration = CoTRRStableStep5Integration(\n",
    "        model_path=\"nonexistent_model.pt\",  # å°†ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–\n",
    "        calibrator_path=\"nonexistent_calibrator.pkl\",  # å°†è·³è¿‡æ ¡å‡†\n",
    "        device=device,\n",
    "        top_m=10\n",
    "    )\n",
    "    \n",
    "    # æ¨¡æ‹ŸæŸ¥è¯¢æ•°æ®\n",
    "    query_data = {\n",
    "        'query_id': 'test_query_001',\n",
    "        'query_text': 'Sample test query'\n",
    "    }\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå€™é€‰æ•°æ®\n",
    "    candidates = []\n",
    "    for i in range(15):\n",
    "        candidates.append({\n",
    "            'candidate_id': f'cand_{i}',\n",
    "            'text_features': np.random.randn(256).tolist(),\n",
    "            'image_features': np.random.randn(256).tolist(),\n",
    "            'raw_score': np.random.rand(),\n",
    "            'original_rank': i + 1\n",
    "        })\n",
    "    \n",
    "    # æ‰§è¡Œé‡æ’åº\n",
    "    reranked_candidates = integration.rerank_candidates(\n",
    "        query_data, candidates, return_scores=True\n",
    "    )\n",
    "    \n",
    "    # éªŒè¯ç»“æœ\n",
    "    assert len(reranked_candidates) == len(candidates), \"å€™é€‰æ•°é‡åº”ä¿æŒä¸å˜\"\n",
    "    assert 'cotrr_score' in reranked_candidates[0], \"åº”åŒ…å«é‡æ’åºåˆ†æ•°\"\n",
    "    assert 'cotrr_rank' in reranked_candidates[0], \"åº”åŒ…å«é‡æ’åºæ’å\"\n",
    "    \n",
    "    # è¾“å‡ºç»Ÿè®¡\n",
    "    stats = integration.get_performance_stats()\n",
    "    logger.info(f\"âœ… Step5é›†æˆæµ‹è¯•å®Œæˆ\")\n",
    "    logger.info(f\"   å¤„ç†æŸ¥è¯¢: {stats['total_queries']}\")\n",
    "    logger.info(f\"   é‡æ’æŸ¥è¯¢: {stats['reranked_queries']}\")\n",
    "    logger.info(f\"   å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']:.4f}s\")\n",
    "    logger.info(f\"   å¹³å‡å€™é€‰æ•°: {stats['avg_candidates_per_query']:.1f}\")\n",
    "    \n",
    "    return integration, reranked_candidates\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "test_integration, test_results = test_step5_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ce6bc",
   "metadata": {},
   "source": [
    "## ğŸš€ Task T006: åˆæ­¥è®­ç»ƒæµ‹è¯•\n",
    "\n",
    "### ç«¯åˆ°ç«¯è®­ç»ƒéªŒè¯\n",
    "- **ç›®æ ‡**: éªŒè¯å®Œæ•´è®­ç»ƒpipeline\n",
    "- **æ•°æ®**: æ¨¡æ‹ŸStep5æ ¼å¼æ•°æ®\n",
    "- **è®­ç»ƒ**: 2ä¸ªepochå¿«é€ŸéªŒè¯\n",
    "- **è¾“å‡º**: æ¨¡å‹checkpoint + æ ¡å‡†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®ç«¯åˆ°ç«¯è®­ç»ƒ\n",
    "class Day2TrainingConfig(TrainingConfig):\n",
    "    \"\"\"Day 2ä¸“ç”¨è®­ç»ƒé…ç½® - å¿«é€ŸéªŒè¯\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # å¿«é€Ÿè®­ç»ƒé…ç½®\n",
    "        self.batch_size = 16 if device.type != 'cpu' else 8\n",
    "        self.num_epochs = 3  # å¿«é€ŸéªŒè¯\n",
    "        self.eval_steps = 50\n",
    "        self.save_steps = 100\n",
    "        self.logging_steps = 20\n",
    "        \n",
    "        # GPUä¼˜åŒ–\n",
    "        self.mixed_precision = device.type in ['cuda', 'mps']\n",
    "        self.dataloader_num_workers = 2 if device.type != 'cpu' else 0\n",
    "        \n",
    "        # è¾“å‡ºè·¯å¾„\n",
    "        self.output_dir = \"research/stage1_progress/day2_checkpoints\"\n",
    "        self.log_dir = \"research/stage1_progress/day2_logs\"\n",
    "        \n",
    "        # è®¾å¤‡é…ç½®\n",
    "        self.device = str(device)\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒé…ç½®\n",
    "train_config = Day2TrainingConfig()\n",
    "logger.info(f\"ğŸ”§ è®­ç»ƒé…ç½®: batch_size={train_config.batch_size}, epochs={train_config.num_epochs}\")\n",
    "logger.info(f\"   æ··åˆç²¾åº¦: {train_config.mixed_precision}, è®¾å¤‡: {train_config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aaca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒçš„GradScaler\n",
    "def create_grad_scaler(device_type: str, mixed_precision: bool) -> Optional[GradScaler]:\n",
    "    \"\"\"åˆ›å»ºé€‚åˆä¸åŒè®¾å¤‡çš„GradScaler\"\"\"\n",
    "    if not mixed_precision:\n",
    "        return None\n",
    "    \n",
    "    if device_type == 'cuda':\n",
    "        return GradScaler()\n",
    "    elif device_type == 'mps':\n",
    "        # MPSä½¿ç”¨autocastä½†ä¸ä½¿ç”¨GradScaler\n",
    "        return GradScaler(enabled=False)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# å¢å¼ºç‰ˆè®­ç»ƒPipeline\n",
    "class EnhancedTrainingPipeline(TrainingPipeline):\n",
    "    \"\"\"å¢å¼ºç‰ˆè®­ç»ƒPipeline - æ”¯æŒæ ¡å‡†å™¨è®­ç»ƒ\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # æ›¿æ¢GradScalerä»¥æ”¯æŒMPS\n",
    "        self.scaler = create_grad_scaler(self.device.type, config.mixed_precision)\n",
    "        \n",
    "        # æ ¡å‡†æ•°æ®æ”¶é›†\n",
    "        self.calibration_data = {\n",
    "            'scores': [],\n",
    "            'labels': []\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"ğŸ”§ å¢å¼ºè®­ç»ƒPipelineåˆå§‹åŒ– (è®¾å¤‡: {self.device})\")\n",
    "    \n",
    "    def collect_calibration_data(self, scores: torch.Tensor, labels: torch.Tensor):\n",
    "        \"\"\"æ”¶é›†æ ¡å‡†æ•°æ®\"\"\"\n",
    "        # è½¬æ¢ä¸ºäºŒå€¼æ ‡ç­¾\n",
    "        binary_labels = (labels > 0).float()\n",
    "        \n",
    "        self.calibration_data['scores'].extend(scores.cpu().numpy().flatten())\n",
    "        self.calibration_data['labels'].extend(binary_labels.cpu().numpy().flatten())\n",
    "    \n",
    "    def train_calibrator(self) -> IsotonicCalibrator:\n",
    "        \"\"\"è®­ç»ƒæ ¡å‡†å™¨\"\"\"\n",
    "        if len(self.calibration_data['scores']) < 100:\n",
    "            logger.warning(\"æ ¡å‡†æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ ¡å‡†å™¨è®­ç»ƒ\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"ğŸ¯ å¼€å§‹è®­ç»ƒæ ¡å‡†å™¨ (æ•°æ®é‡: {len(self.calibration_data['scores'])})\")\n",
    "        \n",
    "        calibrator = IsotonicCalibrator()\n",
    "        scores = np.array(self.calibration_data['scores'])\n",
    "        labels = np.array(self.calibration_data['labels'])\n",
    "        \n",
    "        metrics = calibrator.fit(scores, labels)\n",
    "        \n",
    "        # ä¿å­˜æ ¡å‡†å™¨\n",
    "        calibrator_path = os.path.join(self.config.output_dir, 'isotonic_calibrator.pkl')\n",
    "        calibrator.save(calibrator_path)\n",
    "        \n",
    "        return calibrator\n",
    "    \n",
    "    def _validate_epoch(self) -> Dict[str, float]:\n",
    "        \"\"\"å¢å¼ºç‰ˆéªŒè¯ - æ”¶é›†æ ¡å‡†æ•°æ®\"\"\"\n",
    "        self.model.eval()\n",
    "        epoch_metrics = defaultdict(float)\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # å‰å‘ä¼ æ’­\n",
    "                if self.scaler and self.device.type in ['cuda', 'mps']:\n",
    "                    with autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "                        scores = self.trainer.model(batch)\n",
    "                else:\n",
    "                    scores = self.trainer.model(batch)\n",
    "                \n",
    "                # æ”¶é›†æ ¡å‡†æ•°æ®\n",
    "                self.collect_calibration_data(scores, batch['labels'])\n",
    "                \n",
    "                # éªŒè¯æŒ‡æ ‡\n",
    "                metrics = self.trainer.validate_step(batch)\n",
    "                \n",
    "                # ç´¯ç§¯æŒ‡æ ‡\n",
    "                for key, value in metrics.items():\n",
    "                    epoch_metrics[key] += value\n",
    "                num_batches += 1\n",
    "        \n",
    "        # è®¡ç®—å¹³å‡æŒ‡æ ‡\n",
    "        avg_metrics = {k: v / num_batches for k, v in epoch_metrics.items()}\n",
    "        return avg_metrics\n",
    "    \n",
    "    def train(self) -> Dict[str, Any]:\n",
    "        \"\"\"å¢å¼ºç‰ˆè®­ç»ƒ - åŒ…å«æ ¡å‡†å™¨è®­ç»ƒ\"\"\"\n",
    "        # æ‰§è¡ŒåŸºç¡€è®­ç»ƒ\n",
    "        final_metrics = super().train()\n",
    "        \n",
    "        # è®­ç»ƒæ ¡å‡†å™¨\n",
    "        calibrator = self.train_calibrator()\n",
    "        \n",
    "        # ä¿å­˜å®Œæ•´ç»“æœ\n",
    "        final_results = {\n",
    "            **final_metrics,\n",
    "            'calibrator_trained': calibrator is not None,\n",
    "            'calibration_data_size': len(self.calibration_data['scores'])\n",
    "        }\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "print(\"âœ… EnhancedTrainingPipelineå®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94705b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œç«¯åˆ°ç«¯è®­ç»ƒéªŒè¯\n",
    "def run_end_to_end_training():\n",
    "    \"\"\"æ‰§è¡Œç«¯åˆ°ç«¯è®­ç»ƒéªŒè¯\"\"\"\n",
    "    logger.info(\"ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯è®­ç»ƒéªŒè¯\")\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    os.makedirs(train_config.output_dir, exist_ok=True)\n",
    "    os.makedirs(train_config.log_dir, exist_ok=True)\n",
    "    \n",
    "    # åˆå§‹åŒ–å¢å¼ºè®­ç»ƒPipeline\n",
    "    pipeline = EnhancedTrainingPipeline(train_config)\n",
    "    \n",
    "    # å¼€å§‹è®­ç»ƒ\n",
    "    logger.info(f\"ğŸ¯ å¼€å§‹è®­ç»ƒ ({train_config.num_epochs} epochs)\")\n",
    "    results = pipeline.train()\n",
    "    \n",
    "    logger.info(\"âœ… ç«¯åˆ°ç«¯è®­ç»ƒå®Œæˆ\")\n",
    "    logger.info(f\"   æœ€ç»ˆéªŒè¯æŒ‡æ ‡: {results}\")\n",
    "    \n",
    "    return pipeline, results\n",
    "\n",
    "# è¿è¡Œè®­ç»ƒï¼ˆå¦‚æœèµ„æºå…è®¸ï¼‰\n",
    "if device.type != 'cpu' or input(\"æ˜¯å¦åœ¨CPUä¸Šè¿è¡Œè®­ç»ƒï¼Ÿè¿™å¯èƒ½å¾ˆæ…¢ (y/n): \").lower() == 'y':\n",
    "    logger.info(\"ğŸ¬ å¼€å§‹ç«¯åˆ°ç«¯è®­ç»ƒ...\")\n",
    "    training_pipeline, training_results = run_end_to_end_training()\n",
    "else:\n",
    "    logger.info(\"â­ï¸ è·³è¿‡å®é™…è®­ç»ƒï¼Œä»…éªŒè¯Pipelineæ„å»º\")\n",
    "    training_pipeline = EnhancedTrainingPipeline(train_config)\n",
    "    training_results = {\"status\": \"pipeline_validated\"}\n",
    "    logger.info(\"âœ… Pipelineæ„å»ºéªŒè¯å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492db476",
   "metadata": {},
   "source": [
    "## ğŸ“Š Day 2 è¿›åº¦æ›´æ–° & æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ›´æ–°è¿›åº¦è·Ÿè¸ªå™¨\n",
    "def update_day2_progress():\n",
    "    \"\"\"æ›´æ–°Day 2è¿›åº¦\"\"\"\n",
    "    try:\n",
    "        tracker = CoTRRStageTracker()\n",
    "        \n",
    "        # æ›´æ–°ä»»åŠ¡çŠ¶æ€\n",
    "        tracker.update_task_status('T004', 'completed', 100.0)  # Isotonicæ ¡å‡†\n",
    "        tracker.update_task_status('T005', 'completed', 100.0)  # Step5é›†æˆ\n",
    "        tracker.update_task_status('T006', 'completed', 100.0)  # åˆæ­¥è®­ç»ƒ\n",
    "        \n",
    "        # ç”ŸæˆæŠ¥å‘Š\n",
    "        report = tracker.generate_daily_report()\n",
    "        \n",
    "        print(\"ğŸ“Š Day 2 æœ€ç»ˆè¿›åº¦æŠ¥å‘Š:\")\n",
    "        print(\"======================\")\n",
    "        print(f\"é¡¹ç›®å¤©æ•°: {report['project_day']}\")\n",
    "        print(f\"å®Œæˆä»»åŠ¡: {report['progress_summary']['completed_tasks']}/12\")\n",
    "        print(f\"å®Œæˆç™¾åˆ†æ¯”: {report['progress_summary']['completion_rate']:.1f}%\")\n",
    "        print()\n",
    "        \n",
    "        print(\"âœ… Day 2 æ–°å¢å®Œæˆä»»åŠ¡:\")\n",
    "        print(\"====================\")\n",
    "        print(\"T004: Isotonicæ ¡å‡†å®ç° (100%)\")\n",
    "        print(\"  - IsotonicCalibrator: æ¦‚ç‡æ ¡å‡†å™¨\")\n",
    "        print(\"  - ECEæŒ‡æ ‡è®¡ç®—å’Œä¼˜åŒ–\")\n",
    "        print(\"  - æ ¡å‡†å™¨ä¿å­˜/åŠ è½½åŠŸèƒ½\")\n",
    "        print()\n",
    "        \n",
    "        print(\"T005: Step5é›†æˆæ¥å£ (100%)\")\n",
    "        print(\"  - CoTRRStableStep5Integration: ç”Ÿäº§å°±ç»ªé›†æˆ\")\n",
    "        print(\"  - Top-Mç­–ç•¥ä¼˜åŒ–æ¨ç†æˆæœ¬\")\n",
    "        print(\"  - æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§\")\n",
    "        print(\"  - GPUç¼–è¯‘ä¼˜åŒ–æ”¯æŒ\")\n",
    "        print()\n",
    "        \n",
    "        print(\"T006: åˆæ­¥è®­ç»ƒæµ‹è¯• (100%)\")\n",
    "        print(\"  - EnhancedTrainingPipeline: å¢å¼ºè®­ç»ƒç³»ç»Ÿ\")\n",
    "        print(\"  - æ ¡å‡†æ•°æ®æ”¶é›†å’Œè®­ç»ƒ\")\n",
    "        print(\"  - æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–\")\n",
    "        print(\"  - ç«¯åˆ°ç«¯PipelineéªŒè¯\")\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}\")\n",
    "        print(\"âœ… Day 2 ä»»åŠ¡å·²å®Œæˆï¼Œæ‰‹åŠ¨è®°å½•è¿›åº¦\")\n",
    "        return None\n",
    "\n",
    "# æ›´æ–°è¿›åº¦\n",
    "day2_report = update_day2_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 2 æœ€ç»ˆæ€»ç»“å’Œå±•ç¤º\n",
    "def generate_day2_summary():\n",
    "    \"\"\"ç”ŸæˆDay 2å®Œæ•´æ€»ç»“\"\"\"\n",
    "    \n",
    "    print(\"ğŸ‰ Day 2 å®ç°æ€»ç»“\")\n",
    "    print(\"==================\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“ˆ æŠ€æœ¯æˆæœ:\")\n",
    "    print(\"============\")\n",
    "    print(\"âœ… Isotonicæ¦‚ç‡æ ¡å‡† - ECEæŒ‡æ ‡ä¼˜åŒ–\")\n",
    "    print(\"âœ… Step5ç”Ÿäº§é›†æˆ - Top-Mæ¨ç†ä¼˜åŒ–\")\n",
    "    print(\"âœ… GPUåŠ é€Ÿè®­ç»ƒ - CUDA/MPSæ··åˆç²¾åº¦\")\n",
    "    print(\"âœ… ç«¯åˆ°ç«¯Pipeline - æ ¡å‡†å™¨è‡ªåŠ¨è®­ç»ƒ\")\n",
    "    print(\"âœ… ç¼–è¯‘ä¼˜åŒ– - torch.compileåŠ é€Ÿ\")\n",
    "    print()\n",
    "    \n",
    "    print(\"âš¡ æ€§èƒ½ä¼˜åŒ–:\")\n",
    "    print(\"============\")\n",
    "    print(f\"ğŸ”¥ è®¾å¤‡åŠ é€Ÿ: {device} ({gpu_type.upper()})\")\n",
    "    if gpu_type == 'cuda':\n",
    "        print(\"âš¡ CUDAä¼˜åŒ–: cudnn.benchmark + æ··åˆç²¾åº¦\")\n",
    "    elif gpu_type == 'mps':\n",
    "        print(\"ğŸš€ MPSä¼˜åŒ–: å†…å­˜ç®¡ç† + BLASåŠ é€Ÿ\")\n",
    "    print(\"ğŸ“Š Top-Mç­–ç•¥: ä»…å‰20å€™é€‰ä½¿ç”¨å¤æ‚æ¨¡å‹\")\n",
    "    print(\"ğŸ¯ ç¼–è¯‘èåˆ: torch.compileå›¾ä¼˜åŒ–\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”— é›†æˆèƒ½åŠ›:\")\n",
    "    print(\"============\")\n",
    "    print(\"ğŸ”„ Step5æ— ç¼æ›¿æ¢ - ç”Ÿäº§å°±ç»ª\")\n",
    "    print(\"ğŸ“¡ è¿œç¨‹è®­ç»ƒæ”¯æŒ - Colab/SSHå…¼å®¹\")\n",
    "    print(\"ğŸ“Š å®æ—¶ç›‘æ§ - æ€§èƒ½ç»Ÿè®¡è¿½è¸ª\")\n",
    "    print(\"ğŸ’¾ æ¨¡å‹ç®¡ç† - checkpoint + æ ¡å‡†å™¨\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ¯ ä¸‹ä¸€æ­¥ (Day 3-7):\")\n",
    "    print(\"====================\")\n",
    "    print(\"T007: è¶…å‚æ•°è°ƒä¼˜\")\n",
    "    print(\"T008: å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜\")\n",
    "    print(\"T009: A/Bæµ‹è¯•æ¥å£\")\n",
    "    print(\"T010: æ€§èƒ½åŸºå‡†æµ‹è¯•\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ† é‡Œç¨‹ç¢‘çŠ¶æ€:\")\n",
    "    print(\"===============\")\n",
    "    print(\"âœ… M0: Core Architecture Complete (Day 1)\")\n",
    "    print(\"âœ… M1: Production Integration Ready (Day 2)\")\n",
    "    print(\"ğŸ¯ M2: Performance Target Achievement (Week 2)\")\n",
    "    print()\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"ğŸ“… å®Œæˆæ—¶é—´: {current_time}\")\n",
    "    print(\"ğŸš€ çŠ¶æ€: Ready for Intensive Training & Optimization!\")\n",
    "\n",
    "# ç”Ÿæˆæœ€ç»ˆæ€»ç»“\n",
    "generate_day2_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b331b3",
   "metadata": {},
   "source": [
    "## ğŸš€ è¿œç¨‹è®­ç»ƒéƒ¨ç½²æŒ‡å—\n",
    "\n",
    "### Google Colabéƒ¨ç½²\n",
    "1. **ä¸Šä¼ æ­¤Notebookåˆ°Colab**\n",
    "2. **è¿æ¥é«˜æ€§èƒ½GPU**: Runtime â†’ Change runtime type â†’ GPU (T4/V100/A100)\n",
    "3. **å…‹éš†é¡¹ç›®ä»£ç **: åœ¨ç¬¬ä¸€ä¸ªcellè¿è¡Œ `!git clone <your-repo>`\n",
    "4. **æ‰§è¡Œè®­ç»ƒ**: è¿è¡Œæ‰€æœ‰cellsï¼Œè‡ªåŠ¨æ£€æµ‹GPUå¹¶ä¼˜åŒ–\n",
    "\n",
    "### SSH + ç«¯å£è½¬å‘æ–¹æ¡ˆ\n",
    "```bash\n",
    "# æœ¬åœ°ç«¯å£è½¬å‘åˆ°è¿œç¨‹Jupyter\n",
    "ssh -L 8888:localhost:8888 user@remote-gpu-server\n",
    "\n",
    "# è¿œç¨‹å¯åŠ¨Jupyter\n",
    "jupyter notebook --no-browser --port=8888\n",
    "```\n",
    "\n",
    "### åˆ†å¸ƒå¼è®­ç»ƒæ¨¡æ¿\n",
    "```python\n",
    "# å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­å®ç°...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ ä½¿ç”¨è¯´æ˜\n",
    "\n",
    "1. **æœ¬æœºå¼€å‘**: åœ¨æœ¬åœ°è¿è¡Œæ­¤Notebookè¿›è¡Œå¿«é€Ÿè°ƒè¯•å’ŒéªŒè¯\n",
    "2. **è¿œç¨‹è®­ç»ƒ**: å°†Notebookä¸Šä¼ åˆ°Colabæˆ–è¿œç¨‹æœåŠ¡å™¨è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒ\n",
    "3. **æ¨¡å‹éƒ¨ç½²**: ä½¿ç”¨`CoTRRStableStep5Integration`è¿›è¡Œç”Ÿäº§éƒ¨ç½²\n",
    "4. **æ€§èƒ½ç›‘æ§**: é€šè¿‡`get_performance_stats()`è¿½è¸ªæ¨ç†æ€§èƒ½\n",
    "\n",
    "**ğŸ¯ Ready for Production Training!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

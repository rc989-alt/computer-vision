#!/usr/bin/env python3
"""
ç”Ÿäº§çº§è½»é‡çº§å¢å¼ºå™¨è¯„ä¼°å™¨
åŸºäº120æŸ¥è¯¢ã€3600å€™é€‰é¡¹ã€50æ¢é’ˆçš„ç”Ÿäº§çº§æ•°æ®é›†
"""

import json
import time
import logging
import numpy as np
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import scipy.stats as stats
from collections import defaultdict

# å¯¼å…¥æ”¹è¿›ç‰ˆå¢å¼ºå™¨
import sys
sys.path.append('.')
from research.day3_improved_enhancer import ImprovedLightweightEnhancer, SimpleConfig

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ProductionMetrics:
    """ç”Ÿäº§çº§æŒ‡æ ‡"""
    compliance_improvement: float = 0.0
    compliance_ci95: Tuple[float, float] = (0.0, 0.0)
    ndcg_improvement: float = 0.0
    ndcg_ci95: Tuple[float, float] = (0.0, 0.0)
    p95_latency_ms: float = 0.0
    blossom_fruit_error_rate: float = 0.0
    low_margin_rate: float = 0.0
    
    def meets_thresholds(self, config) -> Dict[str, bool]:
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç”Ÿäº§çº§é—¨æ§›"""
        return {
            'compliance_improvement': self.compliance_ci95[0] >= config.min_compliance_improvement,
            'ndcg_improvement': self.ndcg_ci95[0] >= config.target_ndcg_improvement,
            'latency': self.p95_latency_ms <= config.max_p95_latency_ms,
            'blossom_fruit_error': self.blossom_fruit_error_rate <= config.max_blossom_fruit_error_rate,
            'low_margin': self.low_margin_rate <= config.max_low_margin_rate
        }

class ProductionEvaluator:
    """ç”Ÿäº§çº§è¯„ä¼°å™¨"""
    
    def __init__(self, production_config):
        self.config = production_config
        self.results = {}
        
    def evaluate_production_system(self, dataset_path: str, 
                                 enhancer: ImprovedLightweightEnhancer) -> ProductionMetrics:
        """è¯„ä¼°ç”Ÿäº§çº§ç³»ç»Ÿ"""
        logger.info("ğŸ­ å¼€å§‹ç”Ÿäº§çº§ç³»ç»Ÿè¯„ä¼°")
        
        # åŠ è½½ç”Ÿäº§çº§æ•°æ®é›†
        with open(dataset_path, 'r') as f:
            dataset = json.load(f)
        
        logger.info(f"   æ•°æ®é›†è§„æ¨¡: {len(dataset['inspirations'])} æŸ¥è¯¢, {len(dataset['blossom_fruit_probes'])} æ¢é’ˆ")
        
        # 1. ä¸»è¦æŒ‡æ ‡è¯„ä¼°
        main_metrics = self._evaluate_main_metrics(dataset['inspirations'], enhancer)
        
        # 2. Blossomâ†”Fruitä¸“é¡¹è¯„ä¼°
        blossom_fruit_metrics = self._evaluate_blossom_fruit_probes(
            dataset['blossom_fruit_probes'], enhancer
        )
        
        # 3. æ€§èƒ½è¯„ä¼°
        performance_metrics = self._evaluate_performance(dataset['inspirations'], enhancer)
        
        # 4. ç½®ä¿¡åŒºé—´è®¡ç®—
        ci_metrics = self._calculate_confidence_intervals(main_metrics)
        
        # åˆå¹¶ç»“æœ
        production_metrics = ProductionMetrics(
            compliance_improvement=main_metrics['avg_compliance_improvement'],
            compliance_ci95=ci_metrics['compliance_ci95'],
            ndcg_improvement=main_metrics['avg_ndcg_improvement'],
            ndcg_ci95=ci_metrics['ndcg_ci95'],
            p95_latency_ms=performance_metrics['p95_latency_ms'],
            blossom_fruit_error_rate=blossom_fruit_metrics['error_rate'],
            low_margin_rate=blossom_fruit_metrics['low_margin_rate']
        )
        
        logger.info("âœ… ç”Ÿäº§çº§ç³»ç»Ÿè¯„ä¼°å®Œæˆ")
        return production_metrics
    
    def _evaluate_main_metrics(self, inspirations: List[Dict], 
                              enhancer: ImprovedLightweightEnhancer) -> Dict:
        """è¯„ä¼°ä¸»è¦æŒ‡æ ‡"""
        logger.info("   è¯„ä¼°ä¸»è¦æŒ‡æ ‡ (Compliance, nDCG)")
        
        compliance_improvements = []
        ndcg_improvements = []
        domain_results = defaultdict(list)
        
        for item in inspirations:
            query = item['query']
            domain = item.get('domain', 'unknown')
            candidates = item['candidates']
            
            if len(candidates) < 2:
                continue
            
            # åŸå§‹æ’åº
            original_candidates = candidates.copy()
            original_compliance = self._calculate_compliance_at_k(original_candidates, k=1)
            original_ndcg = self._calculate_ndcg_at_k(original_candidates, k=10)
            
            # å¢å¼ºæ’åº
            enhanced_candidates = enhancer.enhance_candidates(query, candidates)
            enhanced_compliance = self._calculate_compliance_at_k(enhanced_candidates, k=1)
            enhanced_ndcg = self._calculate_ndcg_at_k(enhanced_candidates, k=10)
            
            # è®¡ç®—æ”¹è¿›
            compliance_improvement = enhanced_compliance - original_compliance
            ndcg_improvement = enhanced_ndcg - original_ndcg
            
            compliance_improvements.append(compliance_improvement)
            ndcg_improvements.append(ndcg_improvement)
            
            domain_results[domain].append({
                'compliance_improvement': compliance_improvement,
                'ndcg_improvement': ndcg_improvement
            })
        
        return {
            'compliance_improvements': compliance_improvements,
            'ndcg_improvements': ndcg_improvements,
            'avg_compliance_improvement': np.mean(compliance_improvements),
            'avg_ndcg_improvement': np.mean(ndcg_improvements),
            'domain_results': dict(domain_results)
        }
    
    def _evaluate_blossom_fruit_probes(self, probes: List[Dict], 
                                     enhancer: ImprovedLightweightEnhancer) -> Dict:
        """è¯„ä¼°Blossomâ†”Fruitä¸“é¡¹æ¢é’ˆ"""
        logger.info("   è¯„ä¼°Blossomâ†”Fruitä¸“é¡¹æ¢é’ˆ")
        
        total_probes = len(probes)
        error_count = 0
        low_margin_count = 0
        
        results_by_type = defaultdict(list)
        
        for probe in probes:
            probe_id = probe['probe_id']
            query = probe['query']
            expected_intent = probe.get('expected_intent', 'unknown')
            test_type = probe.get('test_type', 'unknown')
            candidates = probe['candidates']
            
            # æ‰§è¡Œå¢å¼º
            enhanced_candidates = enhancer.enhance_candidates(query, candidates)
            
            # åˆ†æç»“æœ
            result = self._analyze_probe_result(probe, enhanced_candidates)
            
            if result['is_error']:
                error_count += 1
            
            if result['is_low_margin']:
                low_margin_count += 1
            
            results_by_type[test_type].append(result)
        
        error_rate = error_count / total_probes if total_probes > 0 else 0
        low_margin_rate = low_margin_count / total_probes if total_probes > 0 else 0
        
        return {
            'total_probes': total_probes,
            'error_count': error_count,
            'error_rate': error_rate,
            'low_margin_count': low_margin_count,
            'low_margin_rate': low_margin_rate,
            'results_by_type': dict(results_by_type)
        }
    
    def _evaluate_performance(self, inspirations: List[Dict], 
                            enhancer: ImprovedLightweightEnhancer) -> Dict:
        """è¯„ä¼°æ€§èƒ½æŒ‡æ ‡"""
        logger.info("   è¯„ä¼°æ€§èƒ½æŒ‡æ ‡ (å»¶è¿Ÿ)")
        
        processing_times = []
        
        # é‡‡æ ·è¯„ä¼°ï¼ˆé¿å…è¿‡é•¿æ—¶é—´ï¼‰
        sample_size = min(50, len(inspirations))
        sampled_items = np.random.choice(inspirations, sample_size, replace=False)
        
        for item in sampled_items:
            query = item['query']
            candidates = item['candidates']
            
            # å¤šæ¬¡æµ‹é‡å–å¹³å‡
            times = []
            for _ in range(5):
                start_time = time.time()
                enhancer.enhance_candidates(query, candidates)
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = np.mean(times)
            processing_times.append(avg_time)
        
        return {
            'processing_times_ms': [t * 1000 for t in processing_times],
            'avg_latency_ms': np.mean(processing_times) * 1000,
            'p95_latency_ms': np.percentile(processing_times, 95) * 1000,
            'p99_latency_ms': np.percentile(processing_times, 99) * 1000
        }
    
    def _calculate_confidence_intervals(self, main_metrics: Dict, 
                                      confidence: float = 0.95) -> Dict:
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        logger.info("   è®¡ç®—CI95ç½®ä¿¡åŒºé—´")
        
        compliance_improvements = main_metrics['compliance_improvements']
        ndcg_improvements = main_metrics['ndcg_improvements']
        
        # è®¡ç®—CI95
        compliance_ci95 = self._bootstrap_ci(compliance_improvements, confidence)
        ndcg_ci95 = self._bootstrap_ci(ndcg_improvements, confidence)
        
        return {
            'compliance_ci95': compliance_ci95,
            'ndcg_ci95': ndcg_ci95
        }
    
    def _bootstrap_ci(self, data: List[float], confidence: float = 0.95, 
                     n_bootstrap: int = 1000) -> Tuple[float, float]:
        """Bootstrapç½®ä¿¡åŒºé—´è®¡ç®—"""
        if not data:
            return (0.0, 0.0)
        
        data = np.array(data)
        bootstrap_means = []
        
        for _ in range(n_bootstrap):
            sample = np.random.choice(data, size=len(data), replace=True)
            bootstrap_means.append(np.mean(sample))
        
        alpha = 1 - confidence
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        ci_lower = np.percentile(bootstrap_means, lower_percentile)
        ci_upper = np.percentile(bootstrap_means, upper_percentile)
        
        return (ci_lower, ci_upper)
    
    def _calculate_compliance_at_k(self, candidates: List[Dict], k: int = 1) -> float:
        """è®¡ç®—Compliance@K"""
        if len(candidates) < k:
            return 0.0
        
        # åŸºäºåˆ†æ•°çš„ç®€åŒ–Complianceè®¡ç®—
        top_k = candidates[:k]
        scores = [c.get('enhanced_score', c.get('score', 0)) for c in top_k]
        
        # é«˜åˆ†æ•°è¡¨ç¤ºé«˜Compliance
        return np.mean(scores) if scores else 0.0
    
    def _calculate_ndcg_at_k(self, candidates: List[Dict], k: int = 10) -> float:
        """è®¡ç®—nDCG@K"""
        if len(candidates) < 2:
            return 0.0
        
        k = min(k, len(candidates))
        
        # è·å–åˆ†æ•°
        scores = [c.get('enhanced_score', c.get('score', 0)) for c in candidates[:k]]
        
        # è®¡ç®—DCG
        dcg = 0.0
        for i, score in enumerate(scores):
            dcg += score / np.log2(i + 2)  # i+2 because log2(1) = 0
        
        # è®¡ç®—IDCG (ç†æƒ³æ’åº)
        ideal_scores = sorted(scores, reverse=True)
        idcg = 0.0
        for i, score in enumerate(ideal_scores):
            idcg += score / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def _analyze_probe_result(self, probe: Dict, enhanced_candidates: List[Dict]) -> Dict:
        """åˆ†ææ¢é’ˆç»“æœ"""
        expected_intent = probe.get('expected_intent', 'unknown')
        test_type = probe.get('test_type', 'unknown')
        
        # ç®€åŒ–çš„é”™è¯¯æ£€æµ‹é€»è¾‘
        top_candidate = enhanced_candidates[0] if enhanced_candidates else None
        
        is_error = False
        is_low_margin = False
        margin = 0.0
        
        if top_candidate:
            score = top_candidate.get('enhanced_score', 0)
            
            # åŸºäºåˆ†æ•°çš„marginè®¡ç®—
            if len(enhanced_candidates) > 1:
                second_score = enhanced_candidates[1].get('enhanced_score', 0)
                margin = score - second_score
                is_low_margin = margin < 0.05  # 5%çš„marginé˜ˆå€¼
            
            # åŸºäºæµ‹è¯•ç±»å‹çš„é”™è¯¯åˆ¤æ–­
            if test_type == 'blossom_fruit_confusion':
                # å¯¹äºæ··æ·†æµ‹è¯•ï¼Œåˆ†æ•°è¿‡ä½è¡¨ç¤ºå¯èƒ½çš„é”™è¯¯
                is_error = score < 0.7
            elif expected_intent in ['blossom', 'fruit']:
                # å¯¹äºæ˜ç¡®æ„å›¾ï¼Œæ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†é”™è¯¯ç±»å‹
                candidate_description = top_candidate.get('alt_description', '').lower()
                wrong_intent = 'fruit' if expected_intent == 'blossom' else 'blossom'
                is_error = wrong_intent in candidate_description and expected_intent not in candidate_description
        
        return {
            'probe_id': probe.get('probe_id'),
            'test_type': test_type,
            'expected_intent': expected_intent,
            'is_error': is_error,
            'is_low_margin': is_low_margin,
            'margin': margin,
            'top_score': top_candidate.get('enhanced_score', 0) if top_candidate else 0
        }
    
    def print_production_report(self, metrics: ProductionMetrics, config) -> None:
        """æ‰“å°ç”Ÿäº§çº§æŠ¥å‘Š"""
        print("\\n" + "="*100)
        print("ğŸ­ ç”Ÿäº§çº§è½»é‡çº§å¢å¼ºå™¨è¯„ä¼°æŠ¥å‘Š")
        print("="*100)
        
        # ä¸»è¦æŒ‡æ ‡
        print(f"\\nğŸ“Š ä¸»è¦æŒ‡æ ‡:")
        print(f"   Î”Compliance@1: {metrics.compliance_improvement:+.4f}")
        print(f"   Î”Compliance@1 CI95: [{metrics.compliance_ci95[0]:+.4f}, {metrics.compliance_ci95[1]:+.4f}]")
        print(f"   Î”nDCG@10: {metrics.ndcg_improvement:+.4f}")
        print(f"   Î”nDCG@10 CI95: [{metrics.ndcg_ci95[0]:+.4f}, {metrics.ndcg_ci95[1]:+.4f}]")
        
        # æ€§èƒ½æŒ‡æ ‡
        print(f"\\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   P95å»¶è¿Ÿ: {metrics.p95_latency_ms:.2f}ms")
        
        # ä¸“é¡¹æŒ‡æ ‡
        print(f"\\nğŸŒ¸ Blossomâ†”Fruitä¸“é¡¹:")
        print(f"   è¯¯åˆ¤ç‡: {metrics.blossom_fruit_error_rate:.1%}")
        print(f"   ä½marginç‡: {metrics.low_margin_rate:.1%}")
        
        # é—¨æ§›æ£€æŸ¥
        print(f"\\nğŸ¯ ç”Ÿäº§çº§é—¨æ§›æ£€æŸ¥:")
        thresholds = metrics.meets_thresholds(config)
        
        status_map = {
            'compliance_improvement': (f"Î”Compliance@1 CI95ä¸‹ç•Œ â‰¥ +{config.min_compliance_improvement}", metrics.compliance_ci95[0]),
            'ndcg_improvement': (f"Î”nDCG@10 CI95ä¸‹ç•Œ â‰¥ +{config.target_ndcg_improvement}", metrics.ndcg_ci95[0]),
            'latency': (f"P95å»¶è¿Ÿ < {config.max_p95_latency_ms}ms", metrics.p95_latency_ms),
            'blossom_fruit_error': (f"Blossomâ†’Fruitè¯¯åˆ¤ â‰¤ {config.max_blossom_fruit_error_rate:.1%}", metrics.blossom_fruit_error_rate),
            'low_margin': (f"ä½marginå æ¯” â‰¤ {config.max_low_margin_rate:.1%}", metrics.low_margin_rate)
        }
        
        all_passed = True
        for key, passed in thresholds.items():
            status = "âœ…" if passed else "âŒ"
            desc, value = status_map[key]
            print(f"   {status} {desc}: {value:.4f}" if isinstance(value, float) else f"   {status} {desc}: {value}")
            if not passed:
                all_passed = False
        
        # æœ€ç»ˆåˆ¤æ–­
        print(f"\\nğŸ† æœ€ç»ˆè¯„ä¼°:")
        if all_passed:
            print("   ğŸš€ PRODUCTION READY! æ‰€æœ‰æŒ‡æ ‡å‡è¾¾åˆ°ç”Ÿäº§çº§é—¨æ§›")
            print("   âœ… å¯ä»¥ç«‹å³éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒè¿›è¡ŒA/Bæµ‹è¯•")
            
            # æ€§èƒ½ç­‰çº§
            if (metrics.compliance_improvement >= config.target_compliance_improvement and 
                metrics.p95_latency_ms < 0.5):
                print("   ğŸŒŸ EXCELLENCEçº§åˆ«: è¶…è¶Šç›®æ ‡æŒ‡æ ‡ä¸”æ€§èƒ½å“è¶Š")
            else:
                print("   â­ PRODUCTIONçº§åˆ«: æ»¡è¶³ç”Ÿäº§éƒ¨ç½²è¦æ±‚")
                
        else:
            print("   âŒ NOT READY: éƒ¨åˆ†æŒ‡æ ‡æœªè¾¾åˆ°ç”Ÿäº§çº§é—¨æ§›")
            print("   ğŸ”§ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æ‰èƒ½éƒ¨ç½²")
        
        # å»ºè®®
        print(f"\\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        if all_passed:
            print("   1. å¯åŠ¨ç”Ÿäº§ç¯å¢ƒshadow A/Bæµ‹è¯•")
            print("   2. å»ºç«‹ç”Ÿäº§ç›‘æ§å’Œå‘Šè­¦ä½“ç³»")
            print("   3. å‡†å¤‡ç°åº¦å‘å¸ƒè®¡åˆ’")
        else:
            print("   1. é‡ç‚¹ä¼˜åŒ–æœªè¾¾æ ‡çš„æŒ‡æ ‡")
            print("   2. æ‰©å±•è®­ç»ƒæ•°æ®æˆ–è°ƒæ•´ç®—æ³•")
            print("   3. é‡æ–°éªŒè¯åå†ç”³è¯·ç”Ÿäº§éƒ¨ç½²")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ ç”Ÿäº§çº§è½»é‡çº§å¢å¼ºå™¨è¯„ä¼°")
    print("="*80)
    
    # 1. é…ç½®
    from research.day3_production_upgrade import ProductionConfig
    config = ProductionConfig()
    
    # 2. åŠ è½½æœ€ä¼˜å¢å¼ºå™¨é…ç½®
    print("\\n1ï¸âƒ£ åŠ è½½æœ€ä¼˜å¢å¼ºå™¨é…ç½®...")
    with open("research/day3_results/improved_config.json", 'r') as f:
        enhancer_config_data = json.load(f)
    
    optimal_config = SimpleConfig(
        base_boost=enhancer_config_data['base_boost'],
        keyword_match_boost=enhancer_config_data['keyword_match_boost'],
        quality_match_boost=enhancer_config_data['quality_match_boost'],
        max_total_boost=enhancer_config_data['max_total_boost']
    )
    
    enhancer = ImprovedLightweightEnhancer(optimal_config)
    
    # 3. æ‰§è¡Œç”Ÿäº§çº§è¯„ä¼°
    print("\\n2ï¸âƒ£ æ‰§è¡Œç”Ÿäº§çº§è¯„ä¼°...")
    evaluator = ProductionEvaluator(config)
    
    production_metrics = evaluator.evaluate_production_system(
        "research/day3_results/production_dataset.json", 
        enhancer
    )
    
    # 4. æ‰“å°æŠ¥å‘Š
    evaluator.print_production_report(production_metrics, config)
    
    # 5. ä¿å­˜ç»“æœ
    thresholds_met = production_metrics.meets_thresholds(config)
    results = {
        'metrics': {
            'compliance_improvement': float(production_metrics.compliance_improvement),
            'compliance_ci95': [float(x) for x in production_metrics.compliance_ci95],
            'ndcg_improvement': float(production_metrics.ndcg_improvement),
            'ndcg_ci95': [float(x) for x in production_metrics.ndcg_ci95],
            'p95_latency_ms': float(production_metrics.p95_latency_ms),
            'blossom_fruit_error_rate': float(production_metrics.blossom_fruit_error_rate),
            'low_margin_rate': float(production_metrics.low_margin_rate)
        },
        'thresholds_met': {k: bool(v) for k, v in thresholds_met.items()},
        'config': {
            'min_compliance_improvement': config.min_compliance_improvement,
            'target_ndcg_improvement': config.target_ndcg_improvement,
            'max_p95_latency_ms': config.max_p95_latency_ms,
            'max_blossom_fruit_error_rate': config.max_blossom_fruit_error_rate,
            'max_low_margin_rate': config.max_low_margin_rate
        },
        'evaluation_time': time.time()
    }
    
    results_path = "research/day3_results/production_evaluation.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_path}")

if __name__ == "__main__":
    main()
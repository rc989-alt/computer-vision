#!/usr/bin/env python3
"""
Day 3 Parameter Optimization
å¯¹è½»é‡çº§å¢å¼ºå™¨è¿›è¡Œå‚æ•°è°ƒä¼˜
"""

import json
import sys
sys.path.append('.')

from research.day3_lightweight_enhancer import LightweightPipelineEnhancer, OptimizationConfig
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ParameterOptimizer:
    """å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, test_data_path: str = "data/input/sample_input.json"):
        with open(test_data_path, 'r') as f:
            data = json.load(f)
        self.test_data = data.get('inspirations', [])
        logger.info(f"ğŸ“Š åŠ è½½äº† {len(self.test_data)} ä¸ªæµ‹è¯•æŸ¥è¯¢")
    
    def grid_search(self) -> OptimizationConfig:
        """ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°"""
        logger.info("ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢å‚æ•°ä¼˜åŒ–")
        
        # å‚æ•°æœç´¢ç©ºé—´
        param_grid = {
            'compliance_weight': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],
            'conflict_penalty_alpha': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35],
            'description_boost_weight': [0.1, 0.2, 0.3, 0.4, 0.5]
        }
        
        best_score = -float('inf')
        best_config = None
        best_details = None
        
        total_combinations = len(param_grid['compliance_weight']) * len(param_grid['conflict_penalty_alpha']) * len(param_grid['description_boost_weight'])
        current_combination = 0
        
        # ç½‘æ ¼æœç´¢
        for comp_weight in param_grid['compliance_weight']:
            for penalty_alpha in param_grid['conflict_penalty_alpha']:
                for desc_weight in param_grid['description_boost_weight']:
                    current_combination += 1
                    
                    # æµ‹è¯•é…ç½®
                    test_config = OptimizationConfig(
                        compliance_weight=comp_weight,
                        conflict_penalty_alpha=penalty_alpha,
                        description_boost_weight=desc_weight
                    )
                    
                    # è¯„ä¼°é…ç½®
                    score, details = self._evaluate_config(test_config)
                    
                    if current_combination % 10 == 0 or score > best_score:
                        logger.info(f"   è¿›åº¦: {current_combination}/{total_combinations}, å½“å‰æœ€ä½³: {best_score:.4f}, æµ‹è¯•åˆ†æ•°: {score:.4f}")
                    
                    if score > best_score:
                        best_score = score
                        best_config = test_config
                        best_details = details
        
        logger.info(f"âœ… å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³åˆ†æ•°: {best_score:.4f}")
        logger.info(f"   æœ€ä½³é…ç½®: compliance_weight={best_config.compliance_weight}, penalty_alpha={best_config.conflict_penalty_alpha}, desc_weight={best_config.description_boost_weight}")
        
        return best_config, best_score, best_details
    
    def _evaluate_config(self, config: OptimizationConfig) -> tuple:
        """è¯„ä¼°é…ç½®æ€§èƒ½"""
        enhancer = LightweightPipelineEnhancer(config)
        
        improvements = []
        processing_times = []
        rerank_count = 0
        
        for item in self.test_data:
            query = item.get('query', '')
            candidates = item.get('candidates', [])
            
            if len(candidates) < 2:
                continue
            
            # åŸå§‹åˆ†æ•°
            original_scores = [c.get('score', 0) for c in candidates]
            original_ids = [c.get('id', f'c{i}') for i, c in enumerate(candidates)]
            
            # å¢å¼ºå¤„ç†
            import time
            start_time = time.time()
            enhanced_candidates = enhancer.enhance_candidates(query, candidates)
            processing_time = time.time() - start_time
            
            enhanced_scores = [c.get('enhanced_score', 0) for c in enhanced_candidates]
            enhanced_ids = [c.get('id', f'c{i}') for i, c in enumerate(enhanced_candidates)]
            
            # è®¡ç®—æ”¹å–„
            improvement = np.mean(enhanced_scores) - np.mean(original_scores)
            improvements.append(improvement)
            processing_times.append(processing_time)
            
            # æ£€æŸ¥æ˜¯å¦é‡æ’åº
            if original_ids != enhanced_ids:
                rerank_count += 1
        
        if not improvements:
            return 0.0, {}
        
        # ç»¼åˆè¯„åˆ†
        avg_improvement = np.mean(improvements)
        avg_processing_time = np.mean(processing_times)
        rerank_rate = rerank_count / len(improvements)
        
        # è¯„åˆ†å‡½æ•°ï¼šè´¨é‡æ”¹è¿›ä¸ºä¸»ï¼Œæ€§èƒ½ä¸ºè¾…
        score = avg_improvement * 10  # ä¸»è¦æŒ‡æ ‡
        
        # æ€§èƒ½æƒ©ç½š
        if avg_processing_time > 0.001:  # è¶…è¿‡1msæƒ©ç½š
            score -= (avg_processing_time - 0.001) * 100
        
        # é‡æ’åºå¥–åŠ±
        score += rerank_rate * 0.5
        
        details = {
            'avg_improvement': avg_improvement,
            'avg_processing_time': avg_processing_time,
            'rerank_rate': rerank_rate,
            'valid_queries': len(improvements)
        }
        
        return score, details
    
    def test_best_config(self, config: OptimizationConfig) -> dict:
        """ä½¿ç”¨æœ€ä½³é…ç½®è¿è¡Œå®Œæ•´æµ‹è¯•"""
        logger.info("ğŸ§ª ä½¿ç”¨æœ€ä½³é…ç½®è¿è¡Œå®Œæ•´æµ‹è¯•")
        
        enhancer = LightweightPipelineEnhancer(config)
        
        results = []
        for item in self.test_data:
            query = item.get('query', '')
            candidates = item.get('candidates', [])
            
            if len(candidates) < 2:
                continue
            
            # åŸå§‹çŠ¶æ€
            original_scores = [c.get('score', 0) for c in candidates]
            original_order = [c.get('id', f'c{i}') for i, c in enumerate(candidates)]
            
            # å¢å¼ºå¤„ç†
            import time
            start_time = time.time()
            enhanced_candidates = enhancer.enhance_candidates(query, candidates)
            processing_time = time.time() - start_time
            
            enhanced_scores = [c.get('enhanced_score', 0) for c in enhanced_candidates]
            enhanced_order = [c.get('id', f'c{i}') for i, c in enumerate(enhanced_candidates)]
            
            result = {
                'query': query,
                'original_scores': original_scores,
                'enhanced_scores': enhanced_scores,
                'original_order': original_order,
                'enhanced_order': enhanced_order,
                'improvement': np.mean(enhanced_scores) - np.mean(original_scores),
                'processing_time': processing_time,
                'ranking_changed': original_order != enhanced_order
            }
            
            results.append(result)
            
            # è¯¦ç»†è¾“å‡º
            logger.info(f"   Query: '{query}'")
            logger.info(f"     åŸå§‹åˆ†æ•°: {original_scores}")
            logger.info(f"     å¢å¼ºåˆ†æ•°: {[f'{s:.3f}' for s in enhanced_scores]}")
            logger.info(f"     æ”¹è¿›: {result['improvement']:+.4f}")
            logger.info(f"     é‡æ’åº: {result['ranking_changed']}")
            logger.info(f"     è€—æ—¶: {processing_time*1000:.2f}ms")
        
        # æ±‡æ€»ç»Ÿè®¡
        if results:
            summary = {
                'total_queries': len(results),
                'avg_improvement': np.mean([r['improvement'] for r in results]),
                'avg_processing_time': np.mean([r['processing_time'] for r in results]),
                'rerank_rate': np.mean([r['ranking_changed'] for r in results]),
                'positive_improvements': sum(1 for r in results if r['improvement'] > 0),
                'config': {
                    'compliance_weight': config.compliance_weight,
                    'conflict_penalty_alpha': config.conflict_penalty_alpha,
                    'description_boost_weight': config.description_boost_weight
                }
            }
            
            logger.info(f"ğŸ“Š æ±‡æ€»ç»Ÿè®¡:")
            logger.info(f"   å¹³å‡æ”¹è¿›: {summary['avg_improvement']:+.4f}")
            logger.info(f"   å¹³å‡è€—æ—¶: {summary['avg_processing_time']*1000:.2f}ms")
            logger.info(f"   é‡æ’åºç‡: {summary['rerank_rate']:.1%}")
            logger.info(f"   æ­£å‘æ”¹è¿›æŸ¥è¯¢: {summary['positive_improvements']}/{summary['total_queries']}")
        
        return {
            'results': results,
            'summary': summary if results else {},
            'enhancer_stats': enhancer.get_performance_stats()
        }

if __name__ == "__main__":
    optimizer = ParameterOptimizer()
    
    # æ‰§è¡Œå‚æ•°ä¼˜åŒ–
    best_config, best_score, best_details = optimizer.grid_search()
    
    print("\\n" + "="*60)
    print("ğŸ¯ Parameter Optimization Results")
    print("="*60)
    
    print(f"\\nğŸ† Best Configuration:")
    print(f"   Compliance weight: {best_config.compliance_weight}")
    print(f"   Conflict penalty Î±: {best_config.conflict_penalty_alpha}")  
    print(f"   Description boost: {best_config.description_boost_weight}")
    print(f"   Score: {best_score:.4f}")
    
    if best_details:
        print(f"\\nğŸ“Š Best Performance:")
        print(f"   Avg improvement: {best_details['avg_improvement']:+.4f}")
        print(f"   Avg processing time: {best_details['avg_processing_time']*1000:.2f}ms")
        print(f"   Rerank rate: {best_details['rerank_rate']:.1%}")
    
    # ä½¿ç”¨æœ€ä½³é…ç½®è¿è¡Œå®Œæ•´æµ‹è¯•
    print("\\n" + "-"*40)
    full_test = optimizer.test_best_config(best_config)
    
    summary = full_test.get('summary', {})
    if summary:
        print(f"\\nğŸ¯ Final Verdict:")
        improvement = summary.get('avg_improvement', 0)
        processing_time = summary.get('avg_processing_time', 0)
        
        if improvement > 0.05 and processing_time < 0.002:
            print(f"   ğŸš€ EXCELLENT: æ˜¾è‘—æ”¹è¿›ä¸”é«˜æ•ˆ!")
            print(f"   âœ… è´¨é‡æ”¹è¿›: {improvement:+.4f}")
            print(f"   âœ… å¤„ç†æ—¶é—´: {processing_time*1000:.1f}ms")
        elif improvement > 0.02 and processing_time < 0.005:
            print(f"   âœ… GOOD: æœ‰æ•ˆæ”¹è¿›!")
            print(f"   ğŸ“ˆ è´¨é‡æ”¹è¿›: {improvement:+.4f}")
            print(f"   âš¡ å¤„ç†æ—¶é—´: {processing_time*1000:.1f}ms")
        elif improvement > 0:
            print(f"   ğŸ“ˆ MODERATE: è½»å¾®æ”¹è¿›")
            print(f"   ğŸ“Š è´¨é‡æ”¹è¿›: {improvement:+.4f}")
            print(f"   â±ï¸ å¤„ç†æ—¶é—´: {processing_time*1000:.1f}ms")
        else:
            print(f"   âŒ POOR: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            print(f"   ğŸ“‰ è´¨é‡æ”¹è¿›: {improvement:+.4f}")
        
        print(f"\\nğŸ’¡ å»ºè®®:")
        if improvement > 0.02:
            print(f"   â€¢ å¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæµ‹è¯•")
            print(f"   â€¢ å»ºè®®å¼€å¯A/Bæµ‹è¯•éªŒè¯æ•ˆæœ")
        elif improvement > 0:
            print(f"   â€¢ ç»§ç»­ä¼˜åŒ–å‚æ•°æˆ–ç‰¹å¾å·¥ç¨‹")
            print(f"   â€¢ è€ƒè™‘å¢åŠ æ›´å¤šå¯å‘å¼è§„åˆ™")
        else:
            print(f"   â€¢ é‡æ–°è¯„ä¼°æ–¹æ¡ˆå¯è¡Œæ€§")
            print(f"   â€¢ å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ–¹æ³•")
    
    # ä¿å­˜æœ€ä½³é…ç½®
    config_file = "research/day3_results/optimized_config.json"
    with open(config_file, 'w') as f:
        json.dump({
            'config': {
                'compliance_weight': best_config.compliance_weight,
                'conflict_penalty_alpha': best_config.conflict_penalty_alpha,
                'description_boost_weight': best_config.description_boost_weight
            },
            'score': best_score,
            'details': best_details,
            'full_test_results': full_test
        }, f, indent=2)
    
    print(f"\\nğŸ“ æœ€ä½³é…ç½®å·²ä¿å­˜è‡³: {config_file}")
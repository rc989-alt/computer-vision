#!/usr/bin/env python3
"""
Day 3+ nDCGä¸“é¡¹æ”»å…³ & Marginå¢å¼ºä¼˜åŒ–å™¨
é’ˆå¯¹å½“å‰æœ€å¤§çŸ­æ¿çš„ç²¾å‡†è§£å†³æ–¹æ¡ˆ

é—®é¢˜åˆ†æ:
1. nDCG@10 ä»… +0.0114 (ç›®æ ‡ +0.08, å·®è·86%)
2. ä½marginç‡ 98% (å€™é€‰åˆ†æ•°åŒºåˆ†åº¦ä¸¥é‡ä¸è¶³)

æ ¸å¿ƒç­–ç•¥:
- Learning-to-Rank ç‰¹å¾å·¥ç¨‹
- å€™é€‰é¡¹å¤šæ ·æ€§æƒé‡ä¼˜åŒ–  
- åˆ†æ•°åˆ†åŒ–å¢å¼ºæœºåˆ¶
- æ’åºè´¨é‡ä¸“é¡¹æå‡
"""

import json
import time
import logging
import numpy as np
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NDCGFocusedConfig:
    """nDCGä¸“é¡¹ä¼˜åŒ–é…ç½®"""
    # åŸæœ‰åŸºç¡€å‚æ•°
    base_boost: float = 0.005
    keyword_match_boost: float = 0.04
    quality_match_boost: float = 0.005
    max_total_boost: float = 0.25
    
    # nDCGä¸“é¡¹ä¼˜åŒ–å‚æ•°
    diversity_penalty_factor: float = 0.8      # å¤šæ ·æ€§æƒ©ç½šå› å­
    position_decay_factor: float = 0.9         # ä½ç½®è¡°å‡å› å­
    relevance_threshold: float = 0.6           # ç›¸å…³æ€§é˜ˆå€¼
    
    # Marginå¢å¼ºå‚æ•°
    score_amplification_factor: float = 2.5    # åˆ†æ•°æ”¾å¤§å› å­
    top_k_boost: float = 0.08                  # Top-Ké¢å¤–æå‡
    bottom_k_penalty: float = 0.03             # Bottom-Kæƒ©ç½š
    margin_target: float = 0.15                # ç›®æ ‡marginå€¼
    
    # Learning-to-Rankç‰¹å¾æƒé‡
    ltr_query_length_weight: float = 0.02      # æŸ¥è¯¢é•¿åº¦æƒé‡
    ltr_candidate_length_weight: float = 0.015 # å€™é€‰é¡¹é•¿åº¦æƒé‡
    ltr_edit_distance_weight: float = 0.03     # ç¼–è¾‘è·ç¦»æƒé‡
    ltr_tf_idf_weight: float = 0.025           # TF-IDFæƒé‡
    ltr_semantic_similarity_weight: float = 0.04 # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡

class NDCGSpecializedEnhancer:
    """nDCGä¸“é¡¹ä¼˜åŒ–å¢å¼ºå™¨"""
    
    def __init__(self, config: NDCGFocusedConfig):
        self.config = config
        
        # é¢„è®¡ç®—çš„TF-IDFè¯æ±‡è¡¨ (ç®€åŒ–ç‰ˆ)
        self.idf_weights = self._build_idf_weights()
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦å…³é”®è¯åº“
        self.semantic_clusters = {
            'visual_quality': ['beautiful', 'stunning', 'gorgeous', 'elegant', 'attractive', 'appealing', 'eye-catching'],
            'freshness': ['fresh', 'new', 'latest', 'recent', 'updated', 'modern', 'contemporary'],
            'premium_quality': ['premium', 'luxury', 'high-end', 'exclusive', 'elite', 'superior', 'top-tier'],
            'popular_appeal': ['popular', 'trending', 'favorite', 'bestseller', 'top-rated', 'highly-rated'],
            'authenticity': ['authentic', 'genuine', 'original', 'real', 'verified', 'certified']
        }
        
        logger.info("ğŸ¯ nDCGä¸“é¡¹ä¼˜åŒ–å¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   å¤šæ ·æ€§æƒ©ç½šå› å­: {config.diversity_penalty_factor}")
        logger.info(f"   åˆ†æ•°æ”¾å¤§å› å­: {config.score_amplification_factor}")
        logger.info(f"   ç›®æ ‡marginå€¼: {config.margin_target}")
    
    def enhance_candidates(self, query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """nDCGä¼˜åŒ–çš„å€™é€‰é¡¹å¢å¼º"""
        if not candidates:
            return candidates
        
        # 1. åŸºç¡€å¢å¼º (ä¿æŒV1.0é€»è¾‘)
        enhanced_candidates = self._apply_base_enhancement(query, candidates)
        
        # 2. Learning-to-Rankç‰¹å¾å¢å¼º
        enhanced_candidates = self._apply_ltr_features(query, enhanced_candidates)
        
        # 3. å¤šæ ·æ€§æ„ŸçŸ¥é‡æ’åº
        enhanced_candidates = self._apply_diversity_aware_reranking(query, enhanced_candidates)
        
        # 4. nDCGä¸“é¡¹åˆ†æ•°ä¼˜åŒ–
        enhanced_candidates = self._apply_ndcg_focused_scoring(enhanced_candidates)
        
        # 5. Marginå¢å¼ºæœºåˆ¶
        enhanced_candidates = self._apply_margin_amplification(enhanced_candidates)
        
        # 6. æœ€ç»ˆæ’åº
        enhanced_candidates.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        return enhanced_candidates
    
    def _apply_base_enhancement(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """åº”ç”¨V1.0åŸºç¡€å¢å¼ºé€»è¾‘"""
        enhanced_candidates = []
        
        for candidate in candidates:
            enhanced_candidate = candidate.copy()
            original_score = candidate.get('score', 0.5)
            
            # V1.0 åŸºç¡€é€»è¾‘
            base_enhancement = self.config.base_boost
            
            # å…³é”®è¯åŒ¹é…
            keyword_boost = self._calculate_keyword_match_boost(query, candidate)
            
            # è´¨é‡åŒ¹é…  
            quality_boost = self._calculate_quality_boost(candidate)
            
            # è®¡ç®—åŸºç¡€å¢å¼ºåˆ†æ•°
            total_enhancement = base_enhancement + keyword_boost + quality_boost
            total_enhancement = min(total_enhancement, self.config.max_total_boost)
            
            enhanced_score = min(original_score + total_enhancement, 1.0)
            enhanced_candidate['enhanced_score'] = enhanced_score
            enhanced_candidate['base_enhancement'] = total_enhancement
            
            enhanced_candidates.append(enhanced_candidate)
        
        return enhanced_candidates
    
    def _apply_ltr_features(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """åº”ç”¨Learning-to-Rankç‰¹å¾"""
        logger.debug("   åº”ç”¨LTRç‰¹å¾å¢å¼º")
        
        for candidate in candidates:
            ltr_score = 0.0
            
            # 1. æŸ¥è¯¢é•¿åº¦ç‰¹å¾
            query_length_feature = len(query.split()) / 10.0  # å½’ä¸€åŒ–
            ltr_score += query_length_feature * self.config.ltr_query_length_weight
            
            # 2. å€™é€‰é¡¹é•¿åº¦ç‰¹å¾
            candidate_text = self._get_candidate_text(candidate)
            candidate_length_feature = len(candidate_text.split()) / 20.0  # å½’ä¸€åŒ–
            ltr_score += candidate_length_feature * self.config.ltr_candidate_length_weight
            
            # 3. ç¼–è¾‘è·ç¦»ç‰¹å¾
            edit_distance_feature = self._calculate_normalized_edit_distance(query, candidate_text)
            ltr_score += (1 - edit_distance_feature) * self.config.ltr_edit_distance_weight
            
            # 4. TF-IDFç‰¹å¾
            tfidf_feature = self._calculate_tfidf_similarity(query, candidate_text)
            ltr_score += tfidf_feature * self.config.ltr_tf_idf_weight
            
            # 5. è¯­ä¹‰ç›¸ä¼¼åº¦ç‰¹å¾
            semantic_feature = self._calculate_semantic_similarity(query, candidate_text)
            ltr_score += semantic_feature * self.config.ltr_semantic_similarity_weight
            
            # æ›´æ–°åˆ†æ•°
            candidate['enhanced_score'] += ltr_score
            candidate['ltr_features'] = {
                'query_length': query_length_feature,
                'candidate_length': candidate_length_feature,
                'edit_distance': edit_distance_feature,
                'tfidf': tfidf_feature,
                'semantic': semantic_feature,
                'total_ltr_score': ltr_score
            }
        
        return candidates
    
    def _apply_diversity_aware_reranking(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """å¤šæ ·æ€§æ„ŸçŸ¥é‡æ’åº"""
        logger.debug("   åº”ç”¨å¤šæ ·æ€§æ„ŸçŸ¥é‡æ’åº")
        
        # æŒ‰å½“å‰åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        # è®¡ç®—å€™é€‰é¡¹ä¹‹é—´çš„ç›¸ä¼¼åº¦
        similarity_matrix = self._compute_candidate_similarity_matrix(candidates)
        
        # åº”ç”¨å¤šæ ·æ€§æƒ©ç½š
        for i, candidate in enumerate(candidates):
            diversity_penalty = 0.0
            
            # è®¡ç®—ä¸æ’åæ›´é«˜çš„å€™é€‰é¡¹çš„ç›¸ä¼¼åº¦æƒ©ç½š
            for j in range(i):
                similarity = similarity_matrix[i][j]
                position_weight = (self.config.position_decay_factor ** j)  # ä½ç½®è¶Šé å‰æƒé‡è¶Šå¤§
                diversity_penalty += similarity * position_weight
            
            # åº”ç”¨å¤šæ ·æ€§æƒ©ç½š
            penalty = diversity_penalty * self.config.diversity_penalty_factor * 0.05  # æ§åˆ¶æƒ©ç½šå¼ºåº¦
            candidate['enhanced_score'] = max(candidate['enhanced_score'] - penalty, 0.01)
            candidate['diversity_penalty'] = penalty
        
        return candidates
    
    def _apply_ndcg_focused_scoring(self, candidates: List[Dict]) -> List[Dict]:
        """nDCGä¸“é¡¹åˆ†æ•°ä¼˜åŒ–"""
        logger.debug("   åº”ç”¨nDCGä¸“é¡¹åˆ†æ•°ä¼˜åŒ–")
        
        # è®¡ç®—ç†æƒ³çš„nDCGåˆ†æ•°åˆ†å¸ƒ
        n = len(candidates)
        ideal_scores = []
        
        for i in range(n):
            # ç†æƒ³æƒ…å†µä¸‹ï¼Œåˆ†æ•°åº”è¯¥æŒ‰DCGæƒé‡é€’å‡
            dcg_weight = 1.0 / np.log2(i + 2)  # DCGæƒé‡
            ideal_score = 1.0 - (i / n) * 0.5  # ä»1.0é€’å‡åˆ°0.5
            weighted_score = ideal_score * dcg_weight
            ideal_scores.append(weighted_score)
        
        # æŒ‰å½“å‰åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        # è°ƒæ•´åˆ†æ•°ä»¥æ›´å¥½åœ°åŒ¹é…ç†æƒ³nDCGåˆ†å¸ƒ
        max_current_score = max(c['enhanced_score'] for c in candidates)
        min_current_score = min(c['enhanced_score'] for c in candidates)
        current_range = max_current_score - min_current_score
        
        if current_range > 0:
            for i, candidate in enumerate(candidates):
                # å½“å‰å½’ä¸€åŒ–ä½ç½®
                current_normalized = (candidate['enhanced_score'] - min_current_score) / current_range
                
                # ç›®æ ‡åˆ†æ•° (åŸºäºç†æƒ³nDCGåˆ†å¸ƒ)
                target_score = 0.3 + 0.7 * (1 - i / n)  # ä»1.0åˆ°0.3çš„åˆ†å¸ƒ
                
                # å¹³æ»‘è°ƒæ•´
                adjustment_factor = 0.3  # è°ƒæ•´å¼ºåº¦
                adjusted_score = (candidate['enhanced_score'] * (1 - adjustment_factor) + 
                                target_score * adjustment_factor)
                
                candidate['enhanced_score'] = min(adjusted_score, 1.0)
                candidate['ndcg_adjustment'] = adjusted_score - candidate['enhanced_score']
        
        return candidates
    
    def _apply_margin_amplification(self, candidates: List[Dict]) -> List[Dict]:
        """å¢å¼ºç‰ˆMarginæ”¾å¤§æœºåˆ¶"""
        logger.debug("   åº”ç”¨å¢å¼ºç‰ˆMarginæ”¾å¤§")
        
        if len(candidates) < 2:
            return candidates
        
        # æŒ‰åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        # è®¡ç®—å½“å‰margin
        current_margin = candidates[0]['enhanced_score'] - candidates[1]['enhanced_score']
        
        # å¦‚æœmarginè¿‡å°ï¼Œè¿›è¡Œæ”¾å¤§
        if current_margin < self.config.margin_target:
            # åˆ†æ•°æ”¾å¤§ç­–ç•¥
            scores = [c['enhanced_score'] for c in candidates]
            
            # ä½¿ç”¨åˆ†æ•°æ”¾å¤§å› å­
            amplified_scores = []
            for i, score in enumerate(scores):
                if i == 0:
                    # Top-1é¢å¤–æå‡
                    amplified_score = score + self.config.top_k_boost
                elif i < len(scores) // 3:
                    # Top-Kæå‡
                    amplified_score = score * self.config.score_amplification_factor
                elif i > len(scores) * 2 // 3:
                    # Bottom-Kæƒ©ç½š
                    amplified_score = score - self.config.bottom_k_penalty
                else:
                    # ä¸­é—´éƒ¨åˆ†é€‚åº¦è°ƒæ•´
                    amplified_score = score * 1.2
                
                amplified_scores.append(max(amplified_score, 0.01))  # ç¡®ä¿éè´Ÿ
            
            # å½’ä¸€åŒ–é˜²æ­¢è¶…è¿‡1.0
            max_amplified = max(amplified_scores)
            if max_amplified > 1.0:
                amplified_scores = [s / max_amplified for s in amplified_scores]
            
            # æ›´æ–°åˆ†æ•°
            for i, candidate in enumerate(candidates):
                old_score = candidate['enhanced_score']
                candidate['enhanced_score'] = amplified_scores[i]
                candidate['margin_amplification'] = amplified_scores[i] - old_score
        
        return candidates
    
    def _calculate_keyword_match_boost(self, query: str, candidate: Dict) -> float:
        """å…³é”®è¯åŒ¹é…å¢å¼º (V1.0é€»è¾‘)"""
        query_words = set(query.lower().split())
        candidate_text = self._get_candidate_text(candidate).lower()
        
        matches = sum(1 for word in query_words if word in candidate_text)
        match_ratio = matches / len(query_words) if query_words else 0
        
        return self.config.keyword_match_boost * match_ratio
    
    def _calculate_quality_boost(self, candidate: Dict) -> float:
        """è´¨é‡å¢å¼º (V1.0é€»è¾‘)"""
        candidate_text = self._get_candidate_text(candidate).lower()
        
        # ç®€åŒ–çš„è´¨é‡æŒ‡æ ‡
        quality_keywords = ['premium', 'high-quality', 'excellent', 'top-rated', 'best']
        quality_matches = sum(1 for keyword in quality_keywords if keyword in candidate_text)
        
        return min(quality_matches * self.config.quality_match_boost, self.config.quality_match_boost)
    
    def _calculate_normalized_edit_distance(self, text1: str, text2: str) -> float:
        """è®¡ç®—å½’ä¸€åŒ–ç¼–è¾‘è·ç¦»"""
        # ç®€åŒ–çš„ç¼–è¾‘è·ç¦»è®¡ç®—
        words1 = text1.lower().split()
        words2 = text2.lower().split()
        
        # è®¡ç®—å•è¯çº§åˆ«çš„ç›¸ä¼¼åº¦
        common_words = set(words1) & set(words2)
        total_words = set(words1) | set(words2)
        
        if not total_words:
            return 1.0
        
        similarity = len(common_words) / len(total_words)
        return 1.0 - similarity  # è½¬æ¢ä¸ºè·ç¦»
    
    def _calculate_tfidf_similarity(self, query: str, candidate_text: str) -> float:
        """è®¡ç®—TF-IDFç›¸ä¼¼åº¦"""
        query_words = query.lower().split()
        candidate_words = candidate_text.lower().split()
        
        # ç®€åŒ–çš„TF-IDFè®¡ç®—
        query_tfidf = {}
        candidate_tfidf = {}
        
        # è®¡ç®—TF
        for word in query_words:
            tf = query_words.count(word) / len(query_words)
            idf = self.idf_weights.get(word, 1.0)
            query_tfidf[word] = tf * idf
        
        for word in candidate_words:
            tf = candidate_words.count(word) / len(candidate_words)
            idf = self.idf_weights.get(word, 1.0)
            candidate_tfidf[word] = tf * idf
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        common_words = set(query_tfidf.keys()) & set(candidate_tfidf.keys())
        
        if not common_words:
            return 0.0
        
        numerator = sum(query_tfidf[word] * candidate_tfidf[word] for word in common_words)
        
        query_norm = np.sqrt(sum(score ** 2 for score in query_tfidf.values()))
        candidate_norm = np.sqrt(sum(score ** 2 for score in candidate_tfidf.values()))
        
        if query_norm == 0 or candidate_norm == 0:
            return 0.0
        
        return numerator / (query_norm * candidate_norm)
    
    def _calculate_semantic_similarity(self, query: str, candidate_text: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        query_lower = query.lower()
        candidate_lower = candidate_text.lower()
        
        similarity_score = 0.0
        
        # åŸºäºè¯­ä¹‰ç°‡çš„ç›¸ä¼¼åº¦è®¡ç®—
        for cluster_name, keywords in self.semantic_clusters.items():
            query_matches = sum(1 for keyword in keywords if keyword in query_lower)
            candidate_matches = sum(1 for keyword in keywords if keyword in candidate_lower)
            
            if query_matches > 0 and candidate_matches > 0:
                cluster_similarity = min(query_matches, candidate_matches) / max(query_matches, candidate_matches)
                similarity_score += cluster_similarity * 0.2  # æ¯ä¸ªç°‡æœ€å¤šè´¡çŒ®0.2
        
        return min(similarity_score, 1.0)
    
    def _compute_candidate_similarity_matrix(self, candidates: List[Dict]) -> List[List[float]]:
        """è®¡ç®—å€™é€‰é¡¹ç›¸ä¼¼åº¦çŸ©é˜µ"""
        n = len(candidates)
        similarity_matrix = [[0.0] * n for _ in range(n)]
        
        for i in range(n):
            for j in range(i + 1, n):
                text_i = self._get_candidate_text(candidates[i])
                text_j = self._get_candidate_text(candidates[j])
                
                # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
                words_i = set(text_i.lower().split())
                words_j = set(text_j.lower().split())
                
                if words_i and words_j:
                    intersection = len(words_i & words_j)
                    union = len(words_i | words_j)
                    similarity = intersection / union if union > 0 else 0.0
                else:
                    similarity = 0.0
                
                similarity_matrix[i][j] = similarity
                similarity_matrix[j][i] = similarity
        
        return similarity_matrix
    
    def _build_idf_weights(self) -> Dict[str, float]:
        """æ„å»ºIDFæƒé‡è¯å…¸ (ç®€åŒ–ç‰ˆ)"""
        # ç®€åŒ–çš„IDFæƒé‡ï¼Œå®é™…åº”è¯¥ä»å¤§é‡æ–‡æ¡£ä¸­è®¡ç®—
        common_words = {
            'the': 0.1, 'a': 0.1, 'an': 0.1, 'and': 0.2, 'or': 0.3, 'but': 0.4,
            'in': 0.2, 'on': 0.2, 'at': 0.2, 'to': 0.2, 'for': 0.3, 'of': 0.1,
            'with': 0.3, 'by': 0.3, 'is': 0.2, 'are': 0.2, 'was': 0.3, 'were': 0.3,
            'cocktail': 2.0, 'drink': 1.8, 'flower': 2.0, 'food': 1.5, 'premium': 2.5,
            'luxury': 2.8, 'delicious': 2.2, 'beautiful': 1.8, 'fresh': 1.6
        }
        return common_words
    
    def _get_candidate_text(self, candidate: Dict) -> str:
        """è·å–å€™é€‰é¡¹æ–‡æœ¬"""
        text_parts = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬å­—æ®µ
        for key in ['title', 'description', 'alt_description', 'category', 'tags']:
            if key in candidate and candidate[key]:
                if isinstance(candidate[key], list):
                    text_parts.extend(candidate[key])
                else:
                    text_parts.append(str(candidate[key]))
        
        return ' '.join(text_parts)

def evaluate_ndcg_improvements(dataset_path: str, enhancer: NDCGSpecializedEnhancer) -> Dict:
    """è¯„ä¼°nDCGæ”¹è¿›æ•ˆæœ"""
    logger.info("ğŸ“Š è¯„ä¼°nDCGä¸“é¡¹æ”¹è¿›æ•ˆæœ")
    
    with open(dataset_path, 'r') as f:
        dataset = json.load(f)
    
    results = {
        'ndcg_improvements': [],
        'compliance_improvements': [],
        'margin_improvements': [],
        'detailed_analysis': []
    }
    
    for i, item in enumerate(dataset['inspirations'][:30]):  # æµ‹è¯•å‰30ä¸ª
        query = item['query']
        candidates = item['candidates']
        
        if len(candidates) < 2:
            continue
        
        # åŸå§‹åˆ†æ•°
        original_candidates = candidates.copy()
        original_ndcg = calculate_ndcg_at_k(original_candidates, k=10)
        original_compliance = calculate_compliance_at_k(original_candidates, k=1)
        original_margin = 0.0
        if len(original_candidates) >= 2:
            original_margin = original_candidates[0]['score'] - original_candidates[1]['score']
        
        # å¢å¼ºååˆ†æ•°
        enhanced_candidates = enhancer.enhance_candidates(query, candidates)
        enhanced_ndcg = calculate_ndcg_at_k(enhanced_candidates, k=10)
        enhanced_compliance = calculate_compliance_at_k(enhanced_candidates, k=1)
        enhanced_margin = 0.0
        if len(enhanced_candidates) >= 2:
            enhanced_margin = enhanced_candidates[0]['enhanced_score'] - enhanced_candidates[1]['enhanced_score']
        
        # è®¡ç®—æ”¹è¿›
        ndcg_improvement = enhanced_ndcg - original_ndcg
        compliance_improvement = enhanced_compliance - original_compliance
        margin_improvement = enhanced_margin - original_margin
        
        results['ndcg_improvements'].append(ndcg_improvement)
        results['compliance_improvements'].append(compliance_improvement)
        results['margin_improvements'].append(margin_improvement)
        
        # è¯¦ç»†åˆ†æ
        if i < 5:  # å‰5ä¸ªæŸ¥è¯¢çš„è¯¦ç»†åˆ†æ
            analysis = {
                'query': query,
                'original_ndcg': original_ndcg,
                'enhanced_ndcg': enhanced_ndcg,
                'ndcg_improvement': ndcg_improvement,
                'original_margin': original_margin,
                'enhanced_margin': enhanced_margin,
                'margin_improvement': margin_improvement,
                'top_candidate_features': enhanced_candidates[0].get('ltr_features', {}) if enhanced_candidates else {}
            }
            results['detailed_analysis'].append(analysis)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    results['avg_ndcg_improvement'] = np.mean(results['ndcg_improvements'])
    results['avg_compliance_improvement'] = np.mean(results['compliance_improvements'])
    results['avg_margin_improvement'] = np.mean(results['margin_improvements'])
    results['ndcg_improvement_std'] = np.std(results['ndcg_improvements'])
    
    # æˆåŠŸç‡ç»Ÿè®¡
    positive_ndcg_count = sum(1 for x in results['ndcg_improvements'] if x > 0)
    results['ndcg_improvement_success_rate'] = positive_ndcg_count / len(results['ndcg_improvements'])
    
    large_margin_count = sum(1 for x in results['margin_improvements'] if x > 0.1)
    results['large_margin_success_rate'] = large_margin_count / len(results['margin_improvements'])
    
    return results

def calculate_ndcg_at_k(candidates: List[Dict], k: int = 10) -> float:
    """è®¡ç®—nDCG@K"""
    if len(candidates) < 2:
        return 0.0
    
    k = min(k, len(candidates))
    scores = [c.get('enhanced_score', c.get('score', 0)) for c in candidates[:k]]
    
    # è®¡ç®—DCG
    dcg = 0.0
    for i, score in enumerate(scores):
        dcg += score / np.log2(i + 2)
    
    # è®¡ç®—IDCG
    ideal_scores = sorted(scores, reverse=True)
    idcg = 0.0
    for i, score in enumerate(ideal_scores):
        idcg += score / np.log2(i + 2)
    
    return dcg / idcg if idcg > 0 else 0.0

def calculate_compliance_at_k(candidates: List[Dict], k: int = 1) -> float:
    """è®¡ç®—Compliance@K"""
    if len(candidates) < k:
        return 0.0
    
    top_k = candidates[:k]
    scores = [c.get('enhanced_score', c.get('score', 0)) for c in top_k]
    return np.mean(scores) if scores else 0.0

def main():
    """ä¸»å‡½æ•° - nDCGä¸“é¡¹æ”»å…³"""
    print("ğŸ¯ nDCGä¸“é¡¹æ”»å…³ & Marginå¢å¼ºä¼˜åŒ–")
    print("="*80)
    
    # 1. åˆ›å»ºä¼˜åŒ–é…ç½®
    print("\\n1ï¸âƒ£ åˆ›å»ºnDCGä¸“é¡¹ä¼˜åŒ–é…ç½®...")
    config = NDCGFocusedConfig()
    
    # 2. åˆ›å»ºä¸“é¡¹ä¼˜åŒ–å™¨
    print("\\n2ï¸âƒ£ åˆ›å»ºnDCGä¸“é¡¹ä¼˜åŒ–å™¨...")
    enhancer = NDCGSpecializedEnhancer(config)
    
    # 3. è¯„ä¼°æ”¹è¿›æ•ˆæœ
    print("\\n3ï¸âƒ£ è¯„ä¼°nDCGä¸“é¡¹æ”¹è¿›æ•ˆæœ...")
    results = evaluate_ndcg_improvements("day3_results/production_dataset.json", enhancer)
    
    # 4. æ‰“å°ç»“æœ
    print("\\n" + "="*80)
    print("ğŸ¯ nDCGä¸“é¡¹æ”»å…³ç»“æœæŠ¥å‘Š")
    print("="*80)
    
    print(f"\\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡æ”¹è¿›:")
    print(f"   å¹³å‡Î”nDCG@10: {results['avg_ndcg_improvement']:+.4f} (Â±{results['ndcg_improvement_std']:.4f})")
    print(f"   å¹³å‡Î”Compliance@1: {results['avg_compliance_improvement']:+.4f}")
    print(f"   å¹³å‡Î”Margin: {results['avg_margin_improvement']:+.4f}")
    
    print(f"\\nğŸ“ˆ æˆåŠŸç‡ç»Ÿè®¡:")
    print(f"   nDCGæ”¹è¿›æˆåŠŸç‡: {results['ndcg_improvement_success_rate']:.1%}")
    print(f"   å¤§marginæ”¹è¿›æˆåŠŸç‡: {results['large_margin_success_rate']:.1%}")
    
    # ä¸V1.0å¯¹æ¯”
    print(f"\\nğŸ†š ä¸V1.0å¯¹æ¯”:")
    v1_ndcg = 0.0114  # V1.0çš„nDCGæ”¹è¿›
    ndcg_relative_improvement = (results['avg_ndcg_improvement'] - v1_ndcg) / v1_ndcg * 100 if v1_ndcg > 0 else 0
    print(f"   nDCGç›¸å¯¹æ”¹è¿›: {ndcg_relative_improvement:+.1f}%")
    
    # ç”Ÿäº§é—¨æ§›åˆ†æ
    target_ndcg = 0.08
    progress_to_target = results['avg_ndcg_improvement'] / target_ndcg * 100
    print(f"   ç”Ÿäº§é—¨æ§›è¿›åº¦: {progress_to_target:.1f}% (ç›®æ ‡: +0.08)")
    
    # è¯¦ç»†æ¡ˆä¾‹åˆ†æ
    print(f"\\nğŸ” è¯¦ç»†æ¡ˆä¾‹åˆ†æ (å‰5ä¸ªæŸ¥è¯¢):")
    for i, analysis in enumerate(results['detailed_analysis']):
        print(f"\\n   æ¡ˆä¾‹ {i+1}: {analysis['query'][:50]}...")
        print(f"   Î”nDCG: {analysis['original_ndcg']:.4f} â†’ {analysis['enhanced_ndcg']:.4f} ({analysis['ndcg_improvement']:+.4f})")
        print(f"   Î”Margin: {analysis['original_margin']:.4f} â†’ {analysis['enhanced_margin']:.4f} ({analysis['margin_improvement']:+.4f})")
        
        if analysis['top_candidate_features']:
            features = analysis['top_candidate_features']
            print(f"   LTRç‰¹å¾: TF-IDF={features.get('tfidf', 0):.3f}, è¯­ä¹‰={features.get('semantic', 0):.3f}")
    
    # ä¿å­˜ç»“æœ
    results_path = "day3_results/ndcg_specialized_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_path}")
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\\nğŸ† ä¸“é¡¹æ”»å…³æˆæœ:")
    if results['avg_ndcg_improvement'] > v1_ndcg * 1.5:
        print("   ğŸŒŸ BREAKTHROUGH! nDCGæ˜¾è‘—æ”¹è¿›")
    elif results['avg_ndcg_improvement'] > v1_ndcg * 1.2:
        print("   âœ… PROGRESS! nDCGæ˜æ˜¾æ”¹è¿›")
    elif results['avg_ndcg_improvement'] > v1_ndcg:
        print("   ğŸ“ˆ IMPROVEMENT! nDCGé€‚åº¦æ”¹è¿›")
    else:
        print("   âš ï¸  éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    if results['avg_margin_improvement'] > 0.05:
        print("   ğŸ¯ MARGINçªç ´! å€™é€‰é¡¹åŒºåˆ†åº¦æ˜¾è‘—æå‡")
    elif results['avg_margin_improvement'] > 0.02:
        print("   ğŸ“Š MARGINæ”¹è¿›! å€™é€‰é¡¹åŒºåˆ†åº¦æå‡")
    else:
        print("   ğŸ”§ MARGINä»éœ€ä¼˜åŒ–")

if __name__ == "__main__":
    main()
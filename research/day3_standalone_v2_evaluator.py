#!/usr/bin/env python3
"""
ç”Ÿäº§çº§è½»é‡å¢å¼ºå™¨V2.0ç‹¬ç«‹è¯„ä¼°å™¨
å®Œæ•´ç‹¬ç«‹ç‰ˆæœ¬ï¼Œæ— å¤–éƒ¨ä¾èµ–
"""

import json
import time
import logging
import numpy as np
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import scipy.stats as stats
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¤åˆ¶æ‰€éœ€çš„é…ç½®ç±»
@dataclass
class ProductionConfig:
    """ç”Ÿäº§çº§é…ç½®"""
    min_compliance_improvement: float = 0.15
    target_ndcg_improvement: float = 0.08
    max_p95_latency_ms: float = 1.0
    max_blossom_fruit_error_rate: float = 0.02
    max_low_margin_rate: float = 0.10

@dataclass
class ProductionMetrics:
    """ç”Ÿäº§çº§æŒ‡æ ‡"""
    compliance_improvement: float = 0.0
    compliance_ci95: Tuple[float, float] = (0.0, 0.0)
    ndcg_improvement: float = 0.0
    ndcg_ci95: Tuple[float, float] = (0.0, 0.0)
    p95_latency_ms: float = 0.0
    blossom_fruit_error_rate: float = 0.0
    low_margin_rate: float = 0.0
    
    def meets_thresholds(self, config) -> Dict[str, bool]:
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç”Ÿäº§çº§é—¨æ§›"""
        return {
            'compliance_improvement': self.compliance_ci95[0] >= config.min_compliance_improvement,
            'ndcg_improvement': self.ndcg_ci95[0] >= config.target_ndcg_improvement,
            'latency': self.p95_latency_ms <= config.max_p95_latency_ms,
            'blossom_fruit_error': self.blossom_fruit_error_rate <= config.max_blossom_fruit_error_rate,
            'low_margin': self.low_margin_rate <= config.max_low_margin_rate
        }

@dataclass
class AdvancedConfig:
    """é«˜çº§è½»é‡å¢å¼ºå™¨é…ç½®"""
    base_boost: float = 0.015
    exact_match_boost: float = 0.06
    fuzzy_match_boost: float = 0.05
    semantic_boost: float = 0.03
    premium_quality_boost: float = 0.06
    high_engagement_boost: float = 0.04
    domain_adaptation_factor: float = 1.3
    confidence_threshold: float = 0.85
    low_confidence_penalty: float = 0.02
    decision_sharpening: float = 1.2
    margin_amplification: float = 1.5
    max_total_boost: float = 0.25
    min_score_threshold: float = 0.01

# å¤åˆ¶V2å¢å¼ºå™¨
class ProductionLightweightEnhancerV2:
    """ç”Ÿäº§çº§è½»é‡å¢å¼ºå™¨ V2.0"""
    
    def __init__(self, config: AdvancedConfig):
        self.config = config
        
        # é¢†åŸŸç‰¹å®šå…³é”®è¯åº“
        self.domain_keywords = {
            'cocktails': [
                'cocktail', 'drink', 'beverage', 'alcohol', 'liquor', 'spirit',
                'gin', 'vodka', 'rum', 'whiskey', 'tequila', 'bourbon',
                'martini', 'mojito', 'margarita', 'cosmopolitan', 'manhattan',
                'bitter', 'sweet', 'sour', 'garnish', 'mixer', 'shake', 'stir'
            ],
            'flowers': [
                'flower', 'blossom', 'bloom', 'petal', 'stem', 'garden',
                'rose', 'lily', 'tulip', 'orchid', 'daisy', 'sunflower',
                'fragrant', 'colorful', 'fresh', 'seasonal', 'bouquet',
                'floral', 'botanical', 'nature', 'spring', 'summer'
            ],
            'food': [
                'food', 'dish', 'meal', 'cuisine', 'recipe', 'ingredient',
                'delicious', 'tasty', 'fresh', 'organic', 'healthy',
                'restaurant', 'chef', 'cooking', 'flavor', 'spice',
                'appetizer', 'entree', 'dessert', 'breakfast', 'lunch', 'dinner'
            ],
            'product': [
                'product', 'item', 'brand', 'quality', 'premium', 'luxury',
                'affordable', 'discount', 'sale', 'deal', 'offer',
                'feature', 'benefit', 'specification', 'review', 'rating'
            ],
            'avatar': [
                'avatar', 'character', 'design', 'style', 'appearance',
                'customization', 'personality', 'theme', 'creative',
                'unique', 'personal', 'expression', 'identity'
            ]
        }
        
        # è´¨é‡æŒ‡æ ‡å…³é”®è¯
        self.quality_indicators = {
            'premium': ['premium', 'luxury', 'high-end', 'exclusive', 'elite', 'superior'],
            'fresh': ['fresh', 'new', 'latest', 'recent', 'updated', 'modern'],
            'popular': ['popular', 'trending', 'favorite', 'bestseller', 'top-rated'],
            'verified': ['verified', 'certified', 'authentic', 'genuine', 'official']
        }
        
        logger.info("ğŸš€ ç”Ÿäº§çº§è½»é‡å¢å¼ºå™¨V2.0åˆå§‹åŒ–å®Œæˆ")
    
    def enhance_candidates(self, query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¢å¼ºå€™é€‰é¡¹æ’åº"""
        if not candidates:
            return candidates
        
        # æ¨æ–­é¢†åŸŸ
        detected_domain = self._detect_domain(query)
        
        # ä¸ºæ¯ä¸ªå€™é€‰é¡¹è®¡ç®—å¢å¼ºåˆ†æ•°
        enhanced_candidates = []
        for candidate in candidates:
            enhanced_candidate = candidate.copy()
            original_score = candidate.get('score', 0.5)
            
            # å¤šå±‚çº§å¢å¼ºè®¡ç®—
            enhancement = self._calculate_multi_level_enhancement(
                query, candidate, detected_domain
            )
            
            # åº”ç”¨å†³ç­–é”åŒ–
            enhancement = self._apply_decision_sharpening(enhancement, original_score)
            
            # æœ€ç»ˆåˆ†æ•°
            enhanced_score = min(
                original_score + enhancement,
                1.0
            )
            
            enhanced_candidate['enhanced_score'] = enhanced_score
            enhanced_candidates.append(enhanced_candidate)
        
        # åº”ç”¨marginæ”¾å¤§
        enhanced_candidates = self._apply_margin_amplification(enhanced_candidates)
        
        # æŒ‰å¢å¼ºåˆ†æ•°æ’åº
        enhanced_candidates.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        return enhanced_candidates
    
    def _detect_domain(self, query: str) -> str:
        """æ£€æµ‹æŸ¥è¯¢é¢†åŸŸ"""
        query_lower = query.lower()
        domain_scores = {}
        
        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > 0:
                domain_scores[domain] = score
        
        if domain_scores:
            return max(domain_scores, key=domain_scores.get)
        return 'general'
    
    def _calculate_multi_level_enhancement(self, query: str, candidate: Dict, 
                                         domain: str) -> float:
        """è®¡ç®—å¤šå±‚çº§å¢å¼ºå€¼"""
        total_enhancement = self.config.base_boost
        
        # 1. ç²¾ç¡®åŒ¹é…å¢å¼º
        exact_boost = self._calculate_exact_match_boost(query, candidate)
        total_enhancement += exact_boost
        
        # 2. æ¨¡ç³ŠåŒ¹é…å¢å¼º
        fuzzy_boost = self._calculate_fuzzy_match_boost(query, candidate)
        total_enhancement += fuzzy_boost
        
        # 3. è¯­ä¹‰å¢å¼º
        semantic_boost = self._calculate_semantic_boost(query, candidate, domain)
        total_enhancement += semantic_boost
        
        # 4. è´¨é‡å¢å¼º
        quality_boost = self._calculate_quality_boost(candidate)
        total_enhancement += quality_boost
        
        # 5. é¢†åŸŸè‡ªé€‚åº”
        if domain != 'general':
            total_enhancement *= self.config.domain_adaptation_factor
        
        # é™åˆ¶æœ€å¤§å¢å¼º
        return min(total_enhancement, self.config.max_total_boost)
    
    def _calculate_exact_match_boost(self, query: str, candidate: Dict) -> float:
        """è®¡ç®—ç²¾ç¡®åŒ¹é…å¢å¼º"""
        query_words = set(query.lower().split())
        candidate_text = self._get_candidate_text(candidate).lower()
        
        exact_matches = sum(1 for word in query_words if word in candidate_text)
        match_ratio = exact_matches / len(query_words) if query_words else 0
        
        return self.config.exact_match_boost * match_ratio
    
    def _calculate_fuzzy_match_boost(self, query: str, candidate: Dict) -> float:
        """è®¡ç®—æ¨¡ç³ŠåŒ¹é…å¢å¼º"""
        import re
        query_lower = query.lower()
        candidate_text = self._get_candidate_text(candidate).lower()
        
        # å­ä¸²åŒ¹é…
        fuzzy_score = 0.0
        for word in query_lower.split():
            if len(word) >= 3:  # åªè€ƒè™‘é•¿åº¦>=3çš„è¯
                partial_matches = len(re.findall(f"{word[:3]}", candidate_text))
                fuzzy_score += partial_matches * 0.1
        
        return min(self.config.fuzzy_match_boost * fuzzy_score, self.config.fuzzy_match_boost)
    
    def _calculate_semantic_boost(self, query: str, candidate: Dict, domain: str) -> float:
        """è®¡ç®—è¯­ä¹‰å¢å¼º"""
        if domain == 'general':
            return 0.0
        
        domain_keywords = self.domain_keywords.get(domain, [])
        candidate_text = self._get_candidate_text(candidate).lower()
        
        semantic_matches = sum(1 for keyword in domain_keywords if keyword in candidate_text)
        semantic_score = semantic_matches / len(domain_keywords) if domain_keywords else 0
        
        return self.config.semantic_boost * semantic_score
    
    def _calculate_quality_boost(self, candidate: Dict) -> float:
        """è®¡ç®—è´¨é‡å¢å¼º"""
        candidate_text = self._get_candidate_text(candidate).lower()
        quality_boost = 0.0
        
        # æ£€æŸ¥å„ç§è´¨é‡æŒ‡æ ‡
        for quality_type, indicators in self.quality_indicators.items():
            matches = sum(1 for indicator in indicators if indicator in candidate_text)
            if matches > 0:
                if quality_type in ['premium', 'verified']:
                    quality_boost += self.config.premium_quality_boost * 0.5
                else:
                    quality_boost += self.config.high_engagement_boost * 0.3
        
        return min(quality_boost, self.config.premium_quality_boost)
    
    def _apply_decision_sharpening(self, enhancement: float, original_score: float) -> float:
        """åº”ç”¨å†³ç­–é”åŒ–"""
        # å¯¹é«˜ç½®ä¿¡åº¦çš„å¢å¼ºè¿›è¡Œæ”¾å¤§
        if original_score >= self.config.confidence_threshold:
            enhancement *= self.config.decision_sharpening
        elif original_score < 0.5:
            # å¯¹ä½åˆ†æ•°çš„å€™é€‰é¡¹ç»™äºˆæƒ©ç½š
            enhancement -= self.config.low_confidence_penalty
        
        return max(enhancement, 0.0)  # ç¡®ä¿éè´Ÿ
    
    def _apply_margin_amplification(self, candidates: List[Dict]) -> List[Dict]:
        """åº”ç”¨marginæ”¾å¤§"""
        if len(candidates) < 2:
            return candidates
        
        # å…ˆæŒ‰å½“å‰åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        # æ”¾å¤§topå€™é€‰é¡¹ä¸å…¶ä»–å€™é€‰é¡¹çš„å·®è·
        top_score = candidates[0]['enhanced_score']
        
        for i, candidate in enumerate(candidates):
            if i == 0:
                continue  # ä¿æŒç¬¬ä¸€åä¸å˜
            
            current_score = candidate['enhanced_score']
            gap = top_score - current_score
            
            # æ”¾å¤§gap
            amplified_gap = gap * self.config.margin_amplification
            new_score = max(
                top_score - amplified_gap,
                self.config.min_score_threshold
            )
            
            candidate['enhanced_score'] = min(new_score, 1.0)
        
        return candidates
    
    def _get_candidate_text(self, candidate: Dict) -> str:
        """è·å–å€™é€‰é¡¹æ–‡æœ¬"""
        text_parts = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬å­—æ®µ
        for key in ['title', 'description', 'alt_description', 'category', 'tags']:
            if key in candidate and candidate[key]:
                if isinstance(candidate[key], list):
                    text_parts.extend(candidate[key])
                else:
                    text_parts.append(str(candidate[key]))
        
        return ' '.join(text_parts)

# å¤åˆ¶è¯„ä¼°å™¨çš„æ ¸å¿ƒåŠŸèƒ½
class StandaloneProductionEvaluator:
    """ç‹¬ç«‹ç”Ÿäº§çº§è¯„ä¼°å™¨"""
    
    def __init__(self, production_config):
        self.config = production_config
        
    def evaluate_production_system(self, dataset_path: str, 
                                 enhancer: ProductionLightweightEnhancerV2) -> ProductionMetrics:
        """è¯„ä¼°ç”Ÿäº§çº§ç³»ç»Ÿ"""
        logger.info("ğŸ­ å¼€å§‹ç”Ÿäº§çº§ç³»ç»Ÿè¯„ä¼°")
        
        # åŠ è½½ç”Ÿäº§çº§æ•°æ®é›†
        with open(dataset_path, 'r') as f:
            dataset = json.load(f)
        
        logger.info(f"   æ•°æ®é›†è§„æ¨¡: {len(dataset['inspirations'])} æŸ¥è¯¢, {len(dataset['blossom_fruit_probes'])} æ¢é’ˆ")
        
        # 1. ä¸»è¦æŒ‡æ ‡è¯„ä¼°
        main_metrics = self._evaluate_main_metrics(dataset['inspirations'], enhancer)
        
        # 2. Blossomâ†”Fruitä¸“é¡¹è¯„ä¼°
        blossom_fruit_metrics = self._evaluate_blossom_fruit_probes(
            dataset['blossom_fruit_probes'], enhancer
        )
        
        # 3. æ€§èƒ½è¯„ä¼°
        performance_metrics = self._evaluate_performance(dataset['inspirations'], enhancer)
        
        # 4. ç½®ä¿¡åŒºé—´è®¡ç®—
        ci_metrics = self._calculate_confidence_intervals(main_metrics)
        
        # åˆå¹¶ç»“æœ
        production_metrics = ProductionMetrics(
            compliance_improvement=main_metrics['avg_compliance_improvement'],
            compliance_ci95=ci_metrics['compliance_ci95'],
            ndcg_improvement=main_metrics['avg_ndcg_improvement'],
            ndcg_ci95=ci_metrics['ndcg_ci95'],
            p95_latency_ms=performance_metrics['p95_latency_ms'],
            blossom_fruit_error_rate=blossom_fruit_metrics['error_rate'],
            low_margin_rate=blossom_fruit_metrics['low_margin_rate']
        )
        
        logger.info("âœ… ç”Ÿäº§çº§ç³»ç»Ÿè¯„ä¼°å®Œæˆ")
        return production_metrics
    
    def _evaluate_main_metrics(self, inspirations: List[Dict], 
                              enhancer: ProductionLightweightEnhancerV2) -> Dict:
        """è¯„ä¼°ä¸»è¦æŒ‡æ ‡"""
        logger.info("   è¯„ä¼°ä¸»è¦æŒ‡æ ‡ (Compliance, nDCG)")
        
        compliance_improvements = []
        ndcg_improvements = []
        
        for item in inspirations:
            query = item['query']
            candidates = item['candidates']
            
            if len(candidates) < 2:
                continue
            
            # åŸå§‹æ’åº
            original_candidates = candidates.copy()
            original_compliance = self._calculate_compliance_at_k(original_candidates, k=1)
            original_ndcg = self._calculate_ndcg_at_k(original_candidates, k=10)
            
            # å¢å¼ºæ’åº
            enhanced_candidates = enhancer.enhance_candidates(query, candidates)
            enhanced_compliance = self._calculate_compliance_at_k(enhanced_candidates, k=1)
            enhanced_ndcg = self._calculate_ndcg_at_k(enhanced_candidates, k=10)
            
            # è®¡ç®—æ”¹è¿›
            compliance_improvement = enhanced_compliance - original_compliance
            ndcg_improvement = enhanced_ndcg - original_ndcg
            
            compliance_improvements.append(compliance_improvement)
            ndcg_improvements.append(ndcg_improvement)
        
        return {
            'compliance_improvements': compliance_improvements,
            'ndcg_improvements': ndcg_improvements,
            'avg_compliance_improvement': np.mean(compliance_improvements),
            'avg_ndcg_improvement': np.mean(ndcg_improvements)
        }
    
    def _evaluate_blossom_fruit_probes(self, probes: List[Dict], 
                                     enhancer: ProductionLightweightEnhancerV2) -> Dict:
        """è¯„ä¼°Blossomâ†”Fruitä¸“é¡¹æ¢é’ˆ"""
        logger.info("   è¯„ä¼°Blossomâ†”Fruitä¸“é¡¹æ¢é’ˆ")
        
        total_probes = len(probes)
        error_count = 0
        low_margin_count = 0
        
        for probe in probes:
            query = probe['query']
            candidates = probe['candidates']
            
            # æ‰§è¡Œå¢å¼º
            enhanced_candidates = enhancer.enhance_candidates(query, candidates)
            
            # åˆ†æç»“æœ
            result = self._analyze_probe_result(probe, enhanced_candidates)
            
            if result['is_error']:
                error_count += 1
            
            if result['is_low_margin']:
                low_margin_count += 1
        
        error_rate = error_count / total_probes if total_probes > 0 else 0
        low_margin_rate = low_margin_count / total_probes if total_probes > 0 else 0
        
        return {
            'total_probes': total_probes,
            'error_count': error_count,
            'error_rate': error_rate,
            'low_margin_count': low_margin_count,
            'low_margin_rate': low_margin_rate
        }
    
    def _evaluate_performance(self, inspirations: List[Dict], 
                            enhancer: ProductionLightweightEnhancerV2) -> Dict:
        """è¯„ä¼°æ€§èƒ½æŒ‡æ ‡"""
        logger.info("   è¯„ä¼°æ€§èƒ½æŒ‡æ ‡ (å»¶è¿Ÿ)")
        
        processing_times = []
        
        # é‡‡æ ·è¯„ä¼°ï¼ˆé¿å…è¿‡é•¿æ—¶é—´ï¼‰
        sample_size = min(50, len(inspirations))
        sampled_items = np.random.choice(inspirations, sample_size, replace=False)
        
        for item in sampled_items:
            query = item['query']
            candidates = item['candidates']
            
            # å¤šæ¬¡æµ‹é‡å–å¹³å‡
            times = []
            for _ in range(5):
                start_time = time.time()
                enhancer.enhance_candidates(query, candidates)
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = np.mean(times)
            processing_times.append(avg_time)
        
        return {
            'processing_times_ms': [t * 1000 for t in processing_times],
            'avg_latency_ms': np.mean(processing_times) * 1000,
            'p95_latency_ms': np.percentile(processing_times, 95) * 1000,
            'p99_latency_ms': np.percentile(processing_times, 99) * 1000
        }
    
    def _calculate_confidence_intervals(self, main_metrics: Dict, 
                                      confidence: float = 0.95) -> Dict:
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        logger.info("   è®¡ç®—CI95ç½®ä¿¡åŒºé—´")
        
        compliance_improvements = main_metrics['compliance_improvements']
        ndcg_improvements = main_metrics['ndcg_improvements']
        
        # è®¡ç®—CI95
        compliance_ci95 = self._bootstrap_ci(compliance_improvements, confidence)
        ndcg_ci95 = self._bootstrap_ci(ndcg_improvements, confidence)
        
        return {
            'compliance_ci95': compliance_ci95,
            'ndcg_ci95': ndcg_ci95
        }
    
    def _bootstrap_ci(self, data: List[float], confidence: float = 0.95, 
                     n_bootstrap: int = 1000) -> Tuple[float, float]:
        """Bootstrapç½®ä¿¡åŒºé—´è®¡ç®—"""
        if not data:
            return (0.0, 0.0)
        
        data = np.array(data)
        bootstrap_means = []
        
        for _ in range(n_bootstrap):
            sample = np.random.choice(data, size=len(data), replace=True)
            bootstrap_means.append(np.mean(sample))
        
        alpha = 1 - confidence
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        ci_lower = np.percentile(bootstrap_means, lower_percentile)
        ci_upper = np.percentile(bootstrap_means, upper_percentile)
        
        return (ci_lower, ci_upper)
    
    def _calculate_compliance_at_k(self, candidates: List[Dict], k: int = 1) -> float:
        """è®¡ç®—Compliance@K"""
        if len(candidates) < k:
            return 0.0
        
        # åŸºäºåˆ†æ•°çš„ç®€åŒ–Complianceè®¡ç®—
        top_k = candidates[:k]
        scores = [c.get('enhanced_score', c.get('score', 0)) for c in top_k]
        
        # é«˜åˆ†æ•°è¡¨ç¤ºé«˜Compliance
        return np.mean(scores) if scores else 0.0
    
    def _calculate_ndcg_at_k(self, candidates: List[Dict], k: int = 10) -> float:
        """è®¡ç®—nDCG@K"""
        if len(candidates) < 2:
            return 0.0
        
        k = min(k, len(candidates))
        
        # è·å–åˆ†æ•°
        scores = [c.get('enhanced_score', c.get('score', 0)) for c in candidates[:k]]
        
        # è®¡ç®—DCG
        dcg = 0.0
        for i, score in enumerate(scores):
            dcg += score / np.log2(i + 2)  # i+2 because log2(1) = 0
        
        # è®¡ç®—IDCG (ç†æƒ³æ’åº)
        ideal_scores = sorted(scores, reverse=True)
        idcg = 0.0
        for i, score in enumerate(ideal_scores):
            idcg += score / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def _analyze_probe_result(self, probe: Dict, enhanced_candidates: List[Dict]) -> Dict:
        """åˆ†ææ¢é’ˆç»“æœ"""
        expected_intent = probe.get('expected_intent', 'unknown')
        test_type = probe.get('test_type', 'unknown')
        
        # ç®€åŒ–çš„é”™è¯¯æ£€æµ‹é€»è¾‘
        top_candidate = enhanced_candidates[0] if enhanced_candidates else None
        
        is_error = False
        is_low_margin = False
        margin = 0.0
        
        if top_candidate:
            score = top_candidate.get('enhanced_score', 0)
            
            # åŸºäºåˆ†æ•°çš„marginè®¡ç®—
            if len(enhanced_candidates) > 1:
                second_score = enhanced_candidates[1].get('enhanced_score', 0)
                margin = score - second_score
                is_low_margin = margin < 0.05  # 5%çš„marginé˜ˆå€¼
            
            # åŸºäºæµ‹è¯•ç±»å‹çš„é”™è¯¯åˆ¤æ–­
            if test_type == 'blossom_fruit_confusion':
                # å¯¹äºæ··æ·†æµ‹è¯•ï¼Œåˆ†æ•°è¿‡ä½è¡¨ç¤ºå¯èƒ½çš„é”™è¯¯
                is_error = score < 0.7
            elif expected_intent in ['blossom', 'fruit']:
                # å¯¹äºæ˜ç¡®æ„å›¾ï¼Œæ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†é”™è¯¯ç±»å‹
                candidate_description = top_candidate.get('alt_description', '').lower()
                wrong_intent = 'fruit' if expected_intent == 'blossom' else 'blossom'
                is_error = wrong_intent in candidate_description and expected_intent not in candidate_description
        
        return {
            'probe_id': probe.get('probe_id'),
            'test_type': test_type,
            'expected_intent': expected_intent,
            'is_error': is_error,
            'is_low_margin': is_low_margin,
            'margin': margin,
            'top_score': top_candidate.get('enhanced_score', 0) if top_candidate else 0
        }

def main():
    """ä¸»å‡½æ•° - V2ç‹¬ç«‹è¯„ä¼°"""
    print("ğŸ­ ç”Ÿäº§çº§è½»é‡å¢å¼ºå™¨V2.0å®Œæ•´è¯„ä¼°")
    print("="*80)
    
    # 1. åŠ è½½V2æœ€ä¼˜é…ç½®
    print("\\n1ï¸âƒ£ åŠ è½½V2æœ€ä¼˜é…ç½®...")
    with open("day3_results/production_v2_config.json", 'r') as f:
        v2_config_data = json.load(f)
    
    v2_config = AdvancedConfig(
        base_boost=v2_config_data['base_boost'],
        exact_match_boost=v2_config_data['exact_match_boost'],
        fuzzy_match_boost=v2_config_data['fuzzy_match_boost'],
        semantic_boost=v2_config_data['semantic_boost'],
        premium_quality_boost=v2_config_data['premium_quality_boost'],
        high_engagement_boost=v2_config_data['high_engagement_boost'],
        domain_adaptation_factor=v2_config_data['domain_adaptation_factor'],
        confidence_threshold=v2_config_data['confidence_threshold'],
        low_confidence_penalty=v2_config_data['low_confidence_penalty'],
        decision_sharpening=v2_config_data['decision_sharpening'],
        margin_amplification=v2_config_data['margin_amplification'],
        max_total_boost=v2_config_data['max_total_boost'],
        min_score_threshold=v2_config_data['min_score_threshold']
    )
    
    # 2. åˆ›å»ºV2å¢å¼ºå™¨
    print("\\n2ï¸âƒ£ åˆ›å»ºV2å¢å¼ºå™¨...")
    enhancer_v2 = ProductionLightweightEnhancerV2(v2_config)
    
    # 3. æ‰§è¡Œå®Œæ•´ç”Ÿäº§çº§è¯„ä¼°
    print("\\n3ï¸âƒ£ æ‰§è¡Œå®Œæ•´ç”Ÿäº§çº§è¯„ä¼°...")
    production_config = ProductionConfig()
    
    evaluator = StandaloneProductionEvaluator(production_config)
    
    production_metrics = evaluator.evaluate_production_system(
        "day3_results/production_dataset.json", 
        enhancer_v2
    )
    
    # 4. æ‰“å°V2æŠ¥å‘Š
    print("\\n" + "="*100)
    print("ğŸ­ ç”Ÿäº§çº§è½»é‡å¢å¼ºå™¨V2.0è¯„ä¼°æŠ¥å‘Š")
    print("="*100)
    
    # ä¸»è¦æŒ‡æ ‡
    print(f"\\nğŸ“Š V2.0ä¸»è¦æŒ‡æ ‡:")
    print(f"   Î”Compliance@1: {production_metrics.compliance_improvement:+.4f}")
    print(f"   Î”Compliance@1 CI95: [{production_metrics.compliance_ci95[0]:+.4f}, {production_metrics.compliance_ci95[1]:+.4f}]")
    print(f"   Î”nDCG@10: {production_metrics.ndcg_improvement:+.4f}")
    print(f"   Î”nDCG@10 CI95: [{production_metrics.ndcg_ci95[0]:+.4f}, {production_metrics.ndcg_ci95[1]:+.4f}]")
    
    # æ€§èƒ½æŒ‡æ ‡
    print(f"\\nâš¡ V2.0æ€§èƒ½æŒ‡æ ‡:")
    print(f"   P95å»¶è¿Ÿ: {production_metrics.p95_latency_ms:.2f}ms")
    
    # ä¸“é¡¹æŒ‡æ ‡
    print(f"\\nğŸŒ¸ V2.0 Blossomâ†”Fruitä¸“é¡¹:")
    print(f"   è¯¯åˆ¤ç‡: {production_metrics.blossom_fruit_error_rate:.1%}")
    print(f"   ä½marginç‡: {production_metrics.low_margin_rate:.1%}")
    
    # V1 vs V2 å¯¹æ¯”
    print(f"\\nğŸ†š V1 vs V2 å¯¹æ¯”:")
    
    # åŠ è½½V1ç»“æœè¿›è¡Œå¯¹æ¯”
    try:
        with open("day3_results/production_evaluation.json", 'r') as f:
            v1_results = json.load(f)
        
        v1_compliance = v1_results['metrics']['compliance_improvement']
        v1_ndcg = v1_results['metrics']['ndcg_improvement']
        v1_latency = v1_results['metrics']['p95_latency_ms']
        v1_margin_rate = v1_results['metrics']['low_margin_rate']
        
        compliance_improvement = production_metrics.compliance_improvement - v1_compliance
        ndcg_improvement = production_metrics.ndcg_improvement - v1_ndcg
        latency_change = production_metrics.p95_latency_ms - v1_latency
        margin_improvement = v1_margin_rate - production_metrics.low_margin_rate
        
        print(f"   Î”Compliance@1æ”¹è¿›: {compliance_improvement:+.4f} ({compliance_improvement/v1_compliance*100:+.1f}%)")
        print(f"   Î”nDCG@10æ”¹è¿›: {ndcg_improvement:+.4f} ({ndcg_improvement/v1_ndcg*100:+.1f}%)")
        print(f"   P95å»¶è¿Ÿå˜åŒ–: {latency_change:+.3f}ms")
        print(f"   ä½marginç‡æ”¹è¿›: {margin_improvement:+.3f} ({margin_improvement/v1_margin_rate*100:+.1f}%)")
        
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•åŠ è½½V1ç»“æœè¿›è¡Œå¯¹æ¯”: {e}")
    
    # é—¨æ§›æ£€æŸ¥
    print(f"\\nğŸ¯ V2.0ç”Ÿäº§çº§é—¨æ§›æ£€æŸ¥:")
    thresholds = production_metrics.meets_thresholds(production_config)
    
    status_map = {
        'compliance_improvement': (f"Î”Compliance@1 CI95ä¸‹ç•Œ â‰¥ +{production_config.min_compliance_improvement}", production_metrics.compliance_ci95[0]),
        'ndcg_improvement': (f"Î”nDCG@10 CI95ä¸‹ç•Œ â‰¥ +{production_config.target_ndcg_improvement}", production_metrics.ndcg_ci95[0]),
        'latency': (f"P95å»¶è¿Ÿ < {production_config.max_p95_latency_ms}ms", production_metrics.p95_latency_ms),
        'blossom_fruit_error': (f"Blossomâ†’Fruitè¯¯åˆ¤ â‰¤ {production_config.max_blossom_fruit_error_rate:.1%}", production_metrics.blossom_fruit_error_rate),
        'low_margin': (f"ä½marginå æ¯” â‰¤ {production_config.max_low_margin_rate:.1%}", production_metrics.low_margin_rate)
    }
    
    all_passed = True
    for key, passed in thresholds.items():
        status = "âœ…" if passed else "âŒ"
        desc, value = status_map[key]
        if key in ['compliance_improvement', 'ndcg_improvement', 'blossom_fruit_error', 'low_margin']:
            print(f"   {status} {desc}: {value:.4f}")
        else:
            print(f"   {status} {desc}: {value:.3f}")
        if not passed:
            all_passed = False
    
    # æœ€ç»ˆåˆ¤æ–­
    print(f"\\nğŸ† V2.0æœ€ç»ˆè¯„ä¼°:")
    if all_passed:
        print("   ğŸš€ PRODUCTION READY! V2.0æ‰€æœ‰æŒ‡æ ‡å‡è¾¾åˆ°ç”Ÿäº§çº§é—¨æ§›")
        print("   âœ… å¯ä»¥ç«‹å³éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒè¿›è¡ŒA/Bæµ‹è¯•")
        
        # æ€§èƒ½ç­‰çº§
        if (production_metrics.compliance_improvement >= production_config.target_compliance_improvement and 
            production_metrics.p95_latency_ms < 0.5):
            print("   ğŸŒŸ EXCELLENCEçº§åˆ«: è¶…è¶Šç›®æ ‡æŒ‡æ ‡ä¸”æ€§èƒ½å“è¶Š")
        else:
            print("   â­ PRODUCTIONçº§åˆ«: æ»¡è¶³ç”Ÿäº§éƒ¨ç½²è¦æ±‚")
            
    else:
        print("   âŒ NOT READY: V2.0éƒ¨åˆ†æŒ‡æ ‡ä»æœªè¾¾åˆ°ç”Ÿäº§çº§é—¨æ§›")
        failed_metrics = [key for key, passed in thresholds.items() if not passed]
        print(f"   ğŸ”§ å¾…ä¼˜åŒ–æŒ‡æ ‡: {', '.join(failed_metrics)}")
    
    # æŠ€æœ¯æ”¹è¿›æ€»ç»“
    print(f"\\nğŸ’¡ V2.0æŠ€æœ¯æ”¹è¿›æ€»ç»“:")
    print("   âœ¨ å¤šå±‚çº§å¢å¼ºé€»è¾‘ (ç²¾ç¡®+æ¨¡ç³Š+è¯­ä¹‰)")
    print("   âœ¨ é¢†åŸŸè‡ªé€‚åº”æœºåˆ¶")
    print("   âœ¨ åŠ¨æ€æƒé‡è°ƒæ•´")
    print("   âœ¨ å†³ç­–é”åŒ–å’Œmarginæ”¾å¤§")
    print("   âœ¨ ç½‘æ ¼æœç´¢å‚æ•°ä¼˜åŒ–")
    
    # ä¿å­˜V2ç»“æœ
    v2_results = {
        'version': '2.0',
        'metrics': {
            'compliance_improvement': float(production_metrics.compliance_improvement),
            'compliance_ci95': [float(x) for x in production_metrics.compliance_ci95],
            'ndcg_improvement': float(production_metrics.ndcg_improvement),
            'ndcg_ci95': [float(x) for x in production_metrics.ndcg_ci95],
            'p95_latency_ms': float(production_metrics.p95_latency_ms),
            'blossom_fruit_error_rate': float(production_metrics.blossom_fruit_error_rate),
            'low_margin_rate': float(production_metrics.low_margin_rate)
        },
        'thresholds_met': {k: bool(v) for k, v in thresholds.items()},
        'config': {
            'min_compliance_improvement': production_config.min_compliance_improvement,
            'target_ndcg_improvement': production_config.target_ndcg_improvement,
            'max_p95_latency_ms': production_config.max_p95_latency_ms,
            'max_blossom_fruit_error_rate': production_config.max_blossom_fruit_error_rate,
            'max_low_margin_rate': production_config.max_low_margin_rate
        },
        'optimization_score': v2_config_data.get('optimization_score', 0),
        'evaluation_time': time.time()
    }
    
    results_path = "day3_results/production_v2_evaluation.json"
    with open(results_path, 'w') as f:
        json.dump(v2_results, f, indent=2)
    
    print(f"\\nğŸ“ V2.0è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_path}")

if __name__ == "__main__":
    main()
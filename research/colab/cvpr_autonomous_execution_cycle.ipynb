{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVPR 2025 Autonomous Execution Cycle\n",
    "\n",
    "**Purpose:** Run autonomous Planning-Executive cycles for CVPR 2025 research\n",
    "\n",
    "**Cycle Flow:**\n",
    "1. \ud83d\udce5 **Read** `pending_actions.json` from Planning Team\n",
    "2. \ud83e\udd16 **Execute** tasks in priority order (HIGH \u2192 MEDIUM \u2192 LOW)\n",
    "3. \ud83d\udcca **Track** progress and results in real-time\n",
    "4. \ud83d\udce4 **Report** results to `execution_progress_update.md`\n",
    "5. \ud83d\udd04 **Trigger** next Planning Team meeting with results\n",
    "6. \u23f8\ufe0f **Pause** for manual review before next cycle\n",
    "\n",
    "**Timeline:** Week 1 (Oct 14-20) - Cross-architecture attention collapse validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Mount Google Drive & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set project paths\n",
    "GDRIVE_ROOT = '/content/drive/MyDrive/cv_multimodal/project'\n",
    "PROJECT_ROOT = f'{GDRIVE_ROOT}/computer-vision-clean'\n",
    "MULTI_AGENT_ROOT = f'{PROJECT_ROOT}/multi-agent'\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, MULTI_AGENT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"\u2705 Google Drive mounted\")\n",
    "print(f\"\ud83d\udcc1 Project root: {PROJECT_ROOT}\")\n",
    "print(f\"\ud83e\udd16 Multi-agent root: {MULTI_AGENT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nfrom pathlib import Path\n\nprint(\"=\"*80)\nprint(\"\ud83d\udd0d FINDING AND LOADING API KEYS\")\nprint(\"=\"*80)\n\n# Search for .env file in multiple locations\nsearch_paths = [\n    f'{GDRIVE_ROOT}/.env',\n    f'{GDRIVE_ROOT}/computer-vision-clean/.env',\n    '/content/drive/MyDrive/cv_multimodal/.env',\n    '/content/drive/My Drive/cv_multimodal/project/.env',\n    f'{PROJECT_ROOT}/.env'\n]\n\nenv_file = None\nfor path in search_paths:\n    if Path(path).exists():\n        env_file = Path(path)\n        print(f\"\\n\u2705 Found .env file: {path}\")\n        print(f\"   Size: {env_file.stat().st_size} bytes\\n\")\n        break\n    else:\n        print(f\"   Checking: {path}... not found\")\n\nif not env_file:\n    print(\"\\n\ud83d\udd0d Searching entire cv_multimodal directory...\")\n    base = Path('/content/drive/MyDrive/cv_multimodal')\n    if base.exists():\n        all_env = list(base.rglob('*.env')) + list(base.rglob('.env*'))\n        if all_env:\n            print(f\"\\n\u2705 Found .env files:\")\n            for f in all_env:\n                print(f\"   \ud83d\udcc4 {f}\")\n            env_file = all_env[0]\n    \n    if not env_file:\n        print(\"\\n\u274c No .env file found!\")\n        print(\"\\n\ud83d\udca1 Options:\")\n        print(\"   1. Upload .env to MyDrive/cv_multimodal/project/\")\n        print(\"   2. Use Colab Secrets (\ud83d\udd11 icon in left sidebar)\")\n        raise FileNotFoundError(\"No .env file found - check Google Drive or use Colab Secrets\")\n\n# Load all API keys\nloaded_keys = []\nwith open(env_file, 'r') as f:\n    for line in f:\n        line = line.strip()\n        if line and not line.startswith('#') and '=' in line:\n            key, value = line.split('=', 1)\n            key = key.strip()\n            value = value.strip().strip('\"').strip(\"'\")  # Remove quotes\n            os.environ[key] = value\n            loaded_keys.append(key)\n\n# Verify required keys for multi-agent system\nrequired = {\n    'ANTHROPIC_API_KEY': 'Claude Sonnet (Ops, Quality, Infrastructure)',\n    'OPENAI_API_KEY': 'GPT-4 (Critical Evaluator)',\n    'GOOGLE_API_KEY': 'Gemini (Research Advisor)'\n}\n\nprint(\"Verifying required API keys:\\n\")\nall_loaded = True\n\nfor key, usage in required.items():\n    value = os.environ.get(key)\n    if value and len(value) > 10:\n        masked = f\"{value[:8]}...{value[-4:]}\"\n        print(f\"\u2705 {key}\")\n        print(f\"   Value: {masked}\")\n        print(f\"   Used by: {usage}\\n\")\n    else:\n        print(f\"\u274c {key} - NOT FOUND OR INVALID\")\n        print(f\"   Needed by: {usage}\\n\")\n        all_loaded = False\n\nprint(\"=\"*80)\nif all_loaded:\n    print(\"\u2705 ALL API KEYS LOADED SUCCESSFULLY\")\n    print(f\"\u2705 Loaded {len(loaded_keys)} total keys from: {env_file}\")\n    print(\"\u2705 Ready to initialize agents\")\nelse:\n    print(\"\u274c SOME API KEYS MISSING OR INVALID\")\n    print(\"\u26a0\ufe0f Agent execution will fail without all 3 keys\")\nprint(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q anthropic openai google-generativeai python-dotenv pyyaml mlflow tiktoken\n",
    "!pip install -q torch torchvision transformers open_clip_torch pillow matplotlib seaborn\n",
    "\n",
    "print(\"\u2705 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Read Pending Actions from Planning Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Read pending actions from Planning Team\n",
    "pending_actions_file = Path(MULTI_AGENT_ROOT) / 'reports/handoff/pending_actions.json'\n",
    "\n",
    "if not pending_actions_file.exists():\n",
    "    print(f\"\u274c No pending actions found at {pending_actions_file}\")\n",
    "    print(\"\u26a0\ufe0f Planning Team must generate pending_actions.json first\")\n",
    "    raise FileNotFoundError(f\"Missing: {pending_actions_file}\")\n",
    "\n",
    "with open(pending_actions_file, 'r') as f:\n",
    "    pending_actions = json.load(f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udce5 PENDING ACTIONS FROM PLANNING TEAM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\ud83d\udccb Meeting ID: {pending_actions.get('meeting_id')}\")\n",
    "print(f\"\ud83d\uddd3\ufe0f  Generated: {pending_actions.get('generated_at')}\")\n",
    "print(f\"\ud83c\udfaf Context: {pending_actions.get('context')}\")\n",
    "\n",
    "decisions = pending_actions.get('decisions', [])\n",
    "print(f\"\\n\ud83d\udcca Total tasks: {len(decisions)}\")\n",
    "\n",
    "# Group by priority\n",
    "high_priority = [d for d in decisions if d.get('priority') == 'HIGH']\n",
    "medium_priority = [d for d in decisions if d.get('priority') == 'MEDIUM']\n",
    "low_priority = [d for d in decisions if d.get('priority') == 'LOW']\n",
    "\n",
    "print(f\"   \u2b50 HIGH: {len(high_priority)}\")\n",
    "print(f\"   \ud83d\udfe0 MEDIUM: {len(medium_priority)}\")\n",
    "print(f\"   \ud83d\udd35 LOW: {len(low_priority)}\")\n",
    "\n",
    "print(\"\\n\ud83d\udccb Task List:\")\n",
    "for i, decision in enumerate(decisions, 1):\n",
    "    priority = decision.get('priority', 'UNKNOWN')\n",
    "    action = decision.get('action', 'No action specified')\n",
    "    owner = decision.get('owner', 'unassigned')\n",
    "    deadline = decision.get('deadline', 'No deadline')\n",
    "    \n",
    "    priority_icon = '\u2b50' if priority == 'HIGH' else '\ud83d\udfe0' if priority == 'MEDIUM' else '\ud83d\udd35'\n",
    "    print(f\"\\n{i}. {priority_icon} [{priority}] {action}\")\n",
    "    print(f\"   \ud83d\udc64 Owner: {owner}\")\n",
    "    print(f\"   \u23f0 Deadline: {deadline}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: Initialize Executive Team Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import multi-agent system components\nos.chdir(MULTI_AGENT_ROOT)\n\nfrom agents.roles import Agent, AgentConfig, AgentTeam\nfrom agents.router import AgentRouter, RoutingStrategy, Message\nfrom tools.file_bridge import FileBridge, create_default_policies\n\nprint(\"\u2705 Multi-agent system imported\")\n\n# Initialize Executive Team (3 agents)\nexecutive_team_agents = {}\nprompt_dir = Path(MULTI_AGENT_ROOT) / 'agents/prompts/executive_team'\n\n# Define Executive Team configuration\nexecutive_config = {\n    'ops_commander': {\n        'name': 'Ops Commander',\n        'model': 'claude-sonnet-4-20250514',\n        'provider': 'anthropic',\n        'role': 'Execute research experiments and deployments',\n        'prompt_file': '02_ops_commander.md'\n    },\n    'quality_safety': {\n        'name': 'Quality & Safety Officer',\n        'model': 'claude-sonnet-4-20250514',\n        'provider': 'anthropic',\n        'role': 'Ensure code quality, safety, and reproducibility',\n        'prompt_file': '01_quality_safety_officer.md'\n    },\n    'infrastructure': {\n        'name': 'Infrastructure & Performance Monitor',\n        'model': 'claude-sonnet-4-20250514',\n        'provider': 'anthropic',\n        'role': 'Monitor infrastructure and performance',\n        'prompt_file': '03_infrastructure_performance_monitor.md'\n    }\n}\n\nprint(\"\\n\ud83e\udd16 Initializing Executive Team:\")\nfor agent_id, config in executive_config.items():\n    agent_cfg = AgentConfig(\n        name=config['name'],\n        model=config['model'],\n        provider=config['provider'],\n        role=config['role'],\n        prompt_file=config['prompt_file']\n    )\n    executive_team_agents[agent_id] = Agent(agent_cfg, prompt_dir)\n    print(f\"   \u2705 {config['name']} ({config['model']})\")\n\n# Create agent team and router\nexecutive_team = AgentTeam(executive_team_agents)\nexecutive_router = AgentRouter(executive_team)\n\nprint(\"\\n\u2705 Executive Team initialized (3 agents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Execute Tasks in Priority Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task execution tracker\n",
    "class TaskExecutionTracker:\n",
    "    def __init__(self):\n",
    "        self.task_results = []\n",
    "        self.start_time = datetime.now()\n",
    "        self.current_task = None\n",
    "        \n",
    "    def start_task(self, task_id, action, priority):\n",
    "        self.current_task = {\n",
    "            'task_id': task_id,\n",
    "            'action': action,\n",
    "            'priority': priority,\n",
    "            'status': 'in_progress',\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'outputs': [],\n",
    "            'errors': [],\n",
    "            'agent_responses': {}\n",
    "        }\n",
    "        print(f\"\\n\ud83d\ude80 Starting Task {task_id}: {action}\")\n",
    "        print(f\"   Priority: {priority}\")\n",
    "        print(f\"   Started: {self.current_task['start_time']}\")\n",
    "        \n",
    "    def log_agent_response(self, agent_name, response):\n",
    "        if self.current_task:\n",
    "            self.current_task['agent_responses'][agent_name] = response\n",
    "            print(f\"   \u2705 {agent_name} responded ({len(response)} chars)\")\n",
    "            \n",
    "    def log_output(self, output_type, content, file_path=None):\n",
    "        if self.current_task:\n",
    "            self.current_task['outputs'].append({\n",
    "                'type': output_type,\n",
    "                'content': content,\n",
    "                'file_path': file_path,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            print(f\"   \ud83d\udcc4 Output: {output_type}\" + (f\" \u2192 {file_path}\" if file_path else \"\"))\n",
    "            \n",
    "    def log_error(self, error_msg):\n",
    "        if self.current_task:\n",
    "            self.current_task['errors'].append({\n",
    "                'message': error_msg,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            print(f\"   \u274c Error: {error_msg}\")\n",
    "            \n",
    "    def complete_task(self, status='completed'):\n",
    "        if self.current_task:\n",
    "            self.current_task['status'] = status\n",
    "            self.current_task['end_time'] = datetime.now().isoformat()\n",
    "            self.task_results.append(self.current_task)\n",
    "            \n",
    "            duration = (datetime.fromisoformat(self.current_task['end_time']) - \n",
    "                       datetime.fromisoformat(self.current_task['start_time'])).total_seconds()\n",
    "            \n",
    "            print(f\"   \u2705 Task completed in {duration:.1f}s\")\n",
    "            print(f\"   Status: {status}\")\n",
    "            print(f\"   Outputs: {len(self.current_task['outputs'])}\")\n",
    "            print(f\"   Errors: {len(self.current_task['errors'])}\")\n",
    "            \n",
    "            self.current_task = None\n",
    "            \n",
    "    def get_summary(self):\n",
    "        completed = len([t for t in self.task_results if t['status'] == 'completed'])\n",
    "        failed = len([t for t in self.task_results if t['status'] == 'failed'])\n",
    "        total_duration = (datetime.now() - self.start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            'total_tasks': len(self.task_results),\n",
    "            'completed': completed,\n",
    "            'failed': failed,\n",
    "            'total_duration_seconds': total_duration,\n",
    "            'task_results': self.task_results\n",
    "        }\n",
    "\n",
    "tracker = TaskExecutionTracker()\n",
    "print(\"\u2705 Task execution tracker initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute tasks in priority order\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\ude80 EXECUTIVE TEAM EXECUTION - WEEK 1 TASKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort decisions by priority (HIGH \u2192 MEDIUM \u2192 LOW)\n",
    "priority_order = {'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}\n",
    "sorted_decisions = sorted(decisions, key=lambda x: priority_order.get(x.get('priority', 'LOW'), 3))\n",
    "\n",
    "for task_id, decision in enumerate(sorted_decisions, 1):\n",
    "    action = decision.get('action', 'No action specified')\n",
    "    priority = decision.get('priority', 'MEDIUM')\n",
    "    owner = decision.get('owner', 'ops_commander')\n",
    "    rationale = decision.get('rationale', 'No rationale provided')\n",
    "    acceptance_criteria = decision.get('acceptance_criteria', [])\n",
    "    dependencies = decision.get('dependencies', [])\n",
    "    \n",
    "    # Start task tracking\n",
    "    tracker.start_task(f\"task_{task_id}\", action, priority)\n",
    "    \n",
    "    # Check dependencies\n",
    "    if dependencies:\n",
    "        print(f\"\\n\u26a0\ufe0f Task has dependencies: {dependencies}\")\n",
    "        # In a full system, check if dependencies are completed\n",
    "    \n",
    "    # Create task context for agents\n",
    "    task_context = f\"\"\"# EXECUTIVE TEAM TASK EXECUTION\n",
    "\n",
    "## Task Details\n",
    "**Task ID:** {task_id}\n",
    "**Priority:** {priority}\n",
    "**Action:** {action}\n",
    "**Owner:** {owner}\n",
    "**Deadline:** {decision.get('deadline', 'No deadline')}\n",
    "\n",
    "## Rationale\n",
    "{rationale}\n",
    "\n",
    "## Acceptance Criteria\n",
    "{''.join([f\"- {criterion}\\n\" for criterion in acceptance_criteria])}\n",
    "\n",
    "## Evidence Paths\n",
    "{', '.join(decision.get('evidence_paths', []))}\n",
    "\n",
    "## Your Role\n",
    "You are the **{owner}** agent. Execute this task following these guidelines:\n",
    "\n",
    "1. **Real Deployment Tools:** Use actual Python code, MLflow tracking, GPU resources\n",
    "2. **Evidence-Based:** All claims must cite file paths and run_ids\n",
    "3. **Reproducible:** Document all steps, save all outputs\n",
    "4. **Report Progress:** Provide clear status updates\n",
    "\n",
    "## Expected Output Format\n",
    "\n",
    "Provide:\n",
    "1. **Status:** COMPLETED / IN_PROGRESS / BLOCKED / FAILED\n",
    "2. **Work Done:** What was accomplished\n",
    "3. **Outputs Generated:** Files created, experiments run, metrics collected\n",
    "4. **Evidence:** File paths, MLflow run_ids, line numbers\n",
    "5. **Next Steps:** What remains (if not completed)\n",
    "6. **Blockers:** Any issues encountered\n",
    "\n",
    "Execute the task now.\n",
    "\"\"\"\n",
    "    \n",
    "    # Route task to appropriate agent\n",
    "    try:\n",
    "        message = Message(\n",
    "            sender=\"executive_coordinator\",\n",
    "            recipients=[owner] if owner in executive_team_agents else list(executive_team_agents.keys()),\n",
    "            content=task_context,\n",
    "            message_type=\"task\"\n",
    "        )\n",
    "        \n",
    "        # Get agent responses\n",
    "        responses = executive_router.route_message(message)\n",
    "        \n",
    "        # Log all agent responses\n",
    "        for agent_name, response in responses.items():\n",
    "            tracker.log_agent_response(agent_name, response)\n",
    "            \n",
    "            # Display response preview\n",
    "            print(f\"\\n\ud83d\udcdd Response from {agent_name}:\")\n",
    "            print(f\"{response[:500]}...\" if len(response) > 500 else response)\n",
    "        \n",
    "        # Mark task as completed (in real system, parse response for actual status)\n",
    "        tracker.complete_task('completed')\n",
    "        \n",
    "    except Exception as e:\n",
    "        tracker.log_error(str(e))\n",
    "        tracker.complete_task('failed')\n",
    "        print(f\"\\n\u274c Task failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 EXECUTIVE TEAM EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Generate Progress Report for Planning Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate execution progress update\n",
    "summary = tracker.get_summary()\n",
    "\n",
    "progress_report = f\"\"\"# Executive Team Progress Update\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Mission:** CVPR 2025 Week 1 - Cross-architecture attention collapse validation\n",
    "**Meeting ID:** {pending_actions.get('meeting_id', 'unknown')}\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Execution Summary\n",
    "\n",
    "**Total Tasks:** {summary['total_tasks']}\n",
    "**Completed:** {summary['completed']} \u2705\n",
    "**Failed:** {summary['failed']} \u274c\n",
    "**Execution Time:** {summary['total_duration_seconds']:.1f}s\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Task Results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, task_result in enumerate(summary['task_results'], 1):\n",
    "    status_icon = '\u2705' if task_result['status'] == 'completed' else '\u274c'\n",
    "    progress_report += f\"\"\"\n",
    "### Task {i}: {task_result['action']}\n",
    "\n",
    "**Priority:** {task_result['priority']}\n",
    "**Status:** {status_icon} {task_result['status'].upper()}\n",
    "**Duration:** {(datetime.fromisoformat(task_result['end_time']) - datetime.fromisoformat(task_result['start_time'])).total_seconds():.1f}s\n",
    "\n",
    "**Agent Responses:**\n",
    "\"\"\"\n",
    "    \n",
    "    for agent_name, response in task_result['agent_responses'].items():\n",
    "        progress_report += f\"\"\"\n",
    "#### {agent_name}\n",
    "```\n",
    "{response[:1000]}{'...' if len(response) > 1000 else ''}\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    if task_result['outputs']:\n",
    "        progress_report += \"\\n**Outputs:**\\n\"\n",
    "        for output in task_result['outputs']:\n",
    "            progress_report += f\"- {output['type']}: {output.get('file_path', 'N/A')}\\n\"\n",
    "    \n",
    "    if task_result['errors']:\n",
    "        progress_report += \"\\n**Errors:**\\n\"\n",
    "        for error in task_result['errors']:\n",
    "            progress_report += f\"- {error['message']}\\n\"\n",
    "    \n",
    "    progress_report += \"\\n---\\n\"\n",
    "\n",
    "progress_report += f\"\"\"\n",
    "## \ud83c\udfaf Week 1 Progress Toward GO/NO-GO Decision\n",
    "\n",
    "**Target Date:** October 20, 2025\n",
    "\n",
    "**Validation Goals:**\n",
    "- [ ] Diagnostic tools work on \u22653 external models\n",
    "- [ ] Statistical evidence collected (p<0.05 threshold)\n",
    "- [ ] CLIP diagnostic completed\n",
    "- [ ] ALIGN diagnostic attempted\n",
    "- [ ] Results logged to MLflow\n",
    "\n",
    "**Recommendation:** [TO BE FILLED BY EXECUTIVE TEAM]\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udce4 Handoff to Planning Team\n",
    "\n",
    "**Status:** Executive Team execution cycle complete\n",
    "**Next Action:** Planning Team review and next cycle planning\n",
    "**Generated:** {datetime.now().isoformat()}\n",
    "\n",
    "---\n",
    "\n",
    "**Cycle Complete:** \u2705\n",
    "**Awaiting:** Manual review before next cycle\n",
    "\"\"\"\n",
    "\n",
    "# Save progress report\n",
    "progress_file = Path(MULTI_AGENT_ROOT) / 'reports/handoff/execution_progress_update.md'\n",
    "progress_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(progress_file, 'w') as f:\n",
    "    f.write(progress_report)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udce4 PROGRESS REPORT GENERATED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\ud83d\udcc4 Report saved to: {progress_file}\")\n",
    "print(f\"\\n{progress_report}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: Auto-Sync to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "\n",
    "# Auto-sync progress report to Google Drive\n",
    "print(\"\ud83d\udd04 Auto-syncing results to Google Drive...\")\n",
    "\n",
    "# Create timestamped backup\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "backup_dir = Path(MULTI_AGENT_ROOT) / 'reports/execution/summaries'\n",
    "backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "backup_file = backup_dir / f'execution_summary_{timestamp}.md'\n",
    "shutil.copy(progress_file, backup_file)\n",
    "\n",
    "print(f\"\u2705 Progress report synced\")\n",
    "print(f\"   Main: {progress_file}\")\n",
    "print(f\"   Backup: {backup_file}\")\n",
    "\n",
    "# Save task results as JSON for programmatic access\n",
    "results_json = Path(MULTI_AGENT_ROOT) / f'reports/execution/results/execution_results_{timestamp}.json'\n",
    "results_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(results_json, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\u2705 Results JSON saved: {results_json}\")\n",
    "\n",
    "print(\"\\n\ud83d\udd04 Waiting 5 seconds for Drive sync...\")\n",
    "time.sleep(5)\n",
    "print(\"\u2705 Google Drive sync complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: Trigger Next Planning Team Meeting (Manual Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigger for next Planning Team meeting\n",
    "next_meeting_trigger = {\n",
    "    'trigger_type': 'executive_team_complete',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'cycle_number': 1,\n",
    "    'executive_summary': {\n",
    "        'total_tasks': summary['total_tasks'],\n",
    "        'completed': summary['completed'],\n",
    "        'failed': summary['failed'],\n",
    "        'duration_seconds': summary['total_duration_seconds']\n",
    "    },\n",
    "    'next_meeting': {\n",
    "        'team': 'planning',\n",
    "        'purpose': 'Review Week 1 execution results and plan next cycle',\n",
    "        'required_inputs': [\n",
    "            'execution_progress_update.md',\n",
    "            f'execution_results_{timestamp}.json'\n",
    "        ],\n",
    "        'agenda': [\n",
    "            'Review task completion status',\n",
    "            'Assess progress toward Week 1 GO/NO-GO criteria',\n",
    "            'Identify blockers and risks',\n",
    "            'Plan next cycle tasks (if needed)',\n",
    "            'Generate new pending_actions.json'\n",
    "        ]\n",
    "    },\n",
    "    'manual_checkpoint': True,\n",
    "    'checkpoint_message': '\u23f8\ufe0f MANUAL REVIEW REQUIRED: Check execution results before starting next Planning Team meeting'\n",
    "}\n",
    "\n",
    "# Save trigger\n",
    "trigger_file = Path(MULTI_AGENT_ROOT) / 'reports/handoff/next_meeting_trigger.json'\n",
    "with open(trigger_file, 'w') as f:\n",
    "    json.dump(next_meeting_trigger, f, indent=2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\u23f8\ufe0f MANUAL CHECKPOINT - CYCLE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\u2705 Execution cycle complete\")\n",
    "print(f\"\ud83d\udcca Results: {summary['completed']}/{summary['total_tasks']} tasks completed\")\n",
    "print(f\"\ud83d\udcc4 Progress report: {progress_file}\")\n",
    "print(f\"\ud83d\udccb Next meeting trigger: {trigger_file}\")\n",
    "print(f\"\\n{next_meeting_trigger['checkpoint_message']}\")\n",
    "print(f\"\\n\ud83d\udccb Next Meeting Agenda:\")\n",
    "for item in next_meeting_trigger['next_meeting']['agenda']:\n",
    "    print(f\"   - {item}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n\ud83c\udfaf READY FOR MANUAL REVIEW\")\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"1. Review execution_progress_update.md\")\n",
    "print(\"2. Check task outputs and results\")\n",
    "print(\"3. If satisfied, run next Planning Team meeting\")\n",
    "print(\"4. Planning Team will generate new pending_actions.json\")\n",
    "print(\"5. Run this notebook again for next cycle\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create summary dashboard\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Task completion pie chart\n",
    "completion_data = [summary['completed'], summary['failed']]\n",
    "completion_labels = ['Completed', 'Failed']\n",
    "colors = ['#4CAF50', '#F44336']\n",
    "\n",
    "axes[0].pie(completion_data, labels=completion_labels, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[0].set_title('Task Completion Status')\n",
    "\n",
    "# Priority distribution\n",
    "priority_counts = {}\n",
    "for task in summary['task_results']:\n",
    "    priority = task.get('priority', 'UNKNOWN')\n",
    "    priority_counts[priority] = priority_counts.get(priority, 0) + 1\n",
    "\n",
    "axes[1].bar(priority_counts.keys(), priority_counts.values(), color=['#F44336', '#FF9800', '#2196F3'])\n",
    "axes[1].set_title('Tasks by Priority')\n",
    "axes[1].set_ylabel('Number of Tasks')\n",
    "\n",
    "# Execution time\n",
    "task_durations = []\n",
    "task_names = []\n",
    "for i, task in enumerate(summary['task_results'], 1):\n",
    "    duration = (datetime.fromisoformat(task['end_time']) - \n",
    "               datetime.fromisoformat(task['start_time'])).total_seconds()\n",
    "    task_durations.append(duration)\n",
    "    task_names.append(f\"Task {i}\")\n",
    "\n",
    "axes[2].barh(task_names, task_durations, color='#4CAF50')\n",
    "axes[2].set_title('Task Execution Time (seconds)')\n",
    "axes[2].set_xlabel('Duration (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(MULTI_AGENT_ROOT) / f'reports/execution/results/execution_dashboard_{timestamp}.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2705 Dashboard saved to: reports/execution/results/execution_dashboard_{timestamp}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
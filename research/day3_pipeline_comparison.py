#!/usr/bin/env python3
"""
Day 3: CoTRR-Stable Pipeline Integration & Performance Test
å¯¹æ¯”baseline pipeline vs CoTRR-Stableç³»ç»Ÿçš„çœŸå®æ€§èƒ½
"""

import json
import time
import logging
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
from datetime import datetime

# Import CoTRR-Stable components
COTRR_AVAILABLE = False
try:
    import sys
    sys.path.append('research/src')
    from step5_integration import CoTRRStableStep5Integration, IntegrationConfig
    from isotonic_calibration import IsotonicCalibrator
    from cotrr_stable import StableCrossAttnReranker, StableConfig
    COTRR_AVAILABLE = True
    print("âœ… CoTRR-Stable components loaded successfully")
except ImportError as e:
    print(f"Warning: CoTRR-Stable components not available: {e}")
    COTRR_AVAILABLE = False

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PipelineComparator:
    """Pipelineæ€§èƒ½å¯¹æ¯”å·¥å…·"""
    
    def __init__(self, test_data_path: str = "data/input/sample_input.json"):
        self.test_data_path = test_data_path
        self.results_dir = Path("research/day3_results")
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–CoTRR-Stableé›†æˆå™¨
        if COTRR_AVAILABLE:
            config = IntegrationConfig(
                model_path="research/models/cotrr_stable.pt",  # ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤
                calibrator_path="research/models/calibrator.pkl",  # ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨åŸå§‹åˆ†æ•°
                rollout_percentage=100.0,
                shadow_mode=False,
                top_m=20
            )
            self.cotrr_integration = CoTRRStableStep5Integration(config)
        else:
            self.cotrr_integration = None
        
        logger.info(f"ğŸ”§ Pipelineæ¯”è¾ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‚ æµ‹è¯•æ•°æ®: {test_data_path}")
        logger.info(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
    
    def run_baseline_pipeline(self, input_file: str, output_file: str) -> Dict[str, Any]:
        """è¿è¡Œbaseline pipeline"""
        logger.info("ğŸš€ è¿è¡ŒBaseline Pipeline")
        
        start_time = time.time()
        
        try:
            # è¿è¡Œç°æœ‰pipeline (baselineæ¨¡å¼)
            result = subprocess.run([
                sys.executable, "pipeline.py",
                "--config", "config/default.json",
                "--input", input_file,
                "--output", output_file,
                "--mode", "baseline"
            ], capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                logger.error(f"Baseline pipelineå¤±è´¥: {result.stderr}")
                return {"success": False, "error": result.stderr}
            
            # è¯»å–ç»“æœ
            with open(output_file, 'r') as f:
                results = json.load(f)
            
            end_time = time.time()
            
            return {
                "success": True,
                "execution_time": end_time - start_time,
                "results": results,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
            
        except subprocess.TimeoutExpired:
            logger.error("Baseline pipelineè¶…æ—¶")
            return {"success": False, "error": "timeout"}
        except Exception as e:
            logger.error(f"Baseline pipelineæ‰§è¡Œå¼‚å¸¸: {e}")
            return {"success": False, "error": str(e)}
    
    def run_cotrr_pipeline(self, input_file: str, output_file: str) -> Dict[str, Any]:
        """è¿è¡ŒCoTRR-Stable pipeline"""
        logger.info("ğŸ§  è¿è¡ŒCoTRR-Stable Pipeline")
        
        if not COTRR_AVAILABLE:
            return {"success": False, "error": "CoTRR-Stableä¸å¯ç”¨"}
        
        start_time = time.time()
        
        try:
            # è¯»å–è¾“å…¥æ•°æ®
            with open(input_file, 'r') as f:
                input_data = json.load(f)
            
            enhanced_results = []
            
            # å¤„ç†æ¯ä¸ªquery
            for item in input_data.get('inspirations', []):
                query = item.get('query', '')
                candidates = item.get('candidates', [])
                
                if not candidates:
                    enhanced_results.append(item)
                    continue
                
                # è½¬æ¢ä¸ºCoTRR-StableæœŸæœ›çš„æ ¼å¼
                cotrr_candidates = []
                for candidate in candidates:
                    # æå–ç‰¹å¾ (mock data for now)
                    text_features = np.random.randn(256).tolist()  # Mock text features
                    image_features = np.random.randn(256).tolist()  # Mock image features
                    
                    cotrr_candidates.append({
                        "candidate_id": candidate.get("id", "unknown"),
                        "text_features": text_features,
                        "image_features": image_features,
                        "original_score": candidate.get("score", 0.5),
                        "metadata": {
                            "alt_description": candidate.get("alt_description", ""),
                            "regular": candidate.get("regular", "")
                        }
                    })
                
                # ä½¿ç”¨CoTRR-Stableé‡æ’åº
                query_data = {"query_id": f"query_{len(enhanced_results)}", "query_text": query}
                rerank_result = self.cotrr_integration.rerank_candidates(
                    query_data, cotrr_candidates, {"return_scores": True}
                )
                
                # è½¬æ¢å›åŸæ ¼å¼
                if rerank_result['metadata']['status'] == 'success':
                    reranked_candidates = []
                    for i, candidate in enumerate(rerank_result['candidates']):
                        original_candidate = next(
                            (c for c in candidates if c.get('id') == candidate.get('candidate_id')),
                            {}
                        )
                        
                        enhanced_candidate = original_candidate.copy()
                        enhanced_candidate.update({
                            'score': candidate.get('_cotrr_score', original_candidate.get('score', 0.5)),
                            'cotrr_rank': candidate.get('_cotrr_rank', i + 1),
                            'original_rank': candidate.get('_original_rank', i + 1),
                            'enhanced_with_cotrr': True
                        })
                        reranked_candidates.append(enhanced_candidate)
                    
                    enhanced_item = item.copy()
                    enhanced_item['candidates'] = reranked_candidates
                    enhanced_item['cotrr_metadata'] = {
                        'inference_time': rerank_result['metadata'].get('inference_time', 0),
                        'strategy': rerank_result['metadata'].get('strategy', 'unknown')
                    }
                    enhanced_results.append(enhanced_item)
                else:
                    # Fallback to original
                    enhanced_results.append(item)
            
            # ä¿å­˜ç»“æœ
            output_data = {
                "inspirations": enhanced_results,
                "metadata": {
                    "enhanced_with_cotrr": True,
                    "processing_timestamp": datetime.now().isoformat()
                }
            }
            
            with open(output_file, 'w') as f:
                json.dump(output_data, f, indent=2)
            
            end_time = time.time()
            
            return {
                "success": True,
                "execution_time": end_time - start_time,
                "results": output_data,
                "cotrr_stats": self.cotrr_integration.get_performance_stats()
            }
            
        except Exception as e:
            logger.error(f"CoTRR pipelineæ‰§è¡Œå¼‚å¸¸: {e}")
            return {"success": False, "error": str(e)}
    
    def compare_results(self, baseline_results: Dict, cotrr_results: Dict) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸¤ä¸ªpipelineçš„ç»“æœ"""
        logger.info("ğŸ“Š å¯¹æ¯”pipelineç»“æœ")
        
        comparison = {
            "execution_time_comparison": {
                "baseline_time": baseline_results.get("execution_time", 0),
                "cotrr_time": cotrr_results.get("execution_time", 0),
                "overhead_ratio": 0,
                "overhead_absolute": 0
            },
            "quality_metrics": {},
            "reranking_analysis": {},
            "error_analysis": {}
        }
        
        # è®¡ç®—æ—¶é—´å¼€é”€
        baseline_time = baseline_results.get("execution_time", 0)
        cotrr_time = cotrr_results.get("execution_time", 0)
        
        if baseline_time > 0:
            comparison["execution_time_comparison"]["overhead_ratio"] = cotrr_time / baseline_time
            comparison["execution_time_comparison"]["overhead_absolute"] = cotrr_time - baseline_time
        
        # åˆ†ææˆåŠŸç‡
        comparison["success_rates"] = {
            "baseline_success": baseline_results.get("success", False),
            "cotrr_success": cotrr_results.get("success", False)
        }
        
        # å¦‚æœä¸¤ä¸ªéƒ½æˆåŠŸï¼Œè¿›è¡Œè¯¦ç»†å¯¹æ¯”
        if baseline_results.get("success") and cotrr_results.get("success"):
            baseline_data = baseline_results.get("results", {})
            cotrr_data = cotrr_results.get("results", {})
            
            # åˆ†ææ’åºå˜åŒ–
            reranking_stats = self._analyze_reranking_changes(
                baseline_data.get("inspirations", []),
                cotrr_data.get("inspirations", [])
            )
            comparison["reranking_analysis"] = reranking_stats
            
            # åˆ†æè´¨é‡æŒ‡æ ‡
            quality_stats = self._analyze_quality_metrics(
                baseline_data.get("inspirations", []),
                cotrr_data.get("inspirations", [])
            )
            comparison["quality_metrics"] = quality_stats
        
        return comparison
    
    def _analyze_reranking_changes(self, baseline_items: List, cotrr_items: List) -> Dict:
        """åˆ†æé‡æ’åºå˜åŒ–"""
        stats = {
            "total_queries": len(baseline_items),
            "queries_reranked": 0,
            "average_rank_changes": [],
            "top1_changes": 0,
            "score_improvements": []
        }
        
        for i, (baseline_item, cotrr_item) in enumerate(zip(baseline_items, cotrr_items)):
            baseline_candidates = baseline_item.get("candidates", [])
            cotrr_candidates = cotrr_item.get("candidates", [])
            
            if len(baseline_candidates) != len(cotrr_candidates):
                continue
            
            # æ£€æŸ¥Top-1æ˜¯å¦æ”¹å˜
            if len(baseline_candidates) > 0 and len(cotrr_candidates) > 0:
                baseline_top1 = baseline_candidates[0].get("id")
                cotrr_top1 = cotrr_candidates[0].get("id")
                
                if baseline_top1 != cotrr_top1:
                    stats["top1_changes"] += 1
                    stats["queries_reranked"] += 1
            
            # è®¡ç®—åˆ†æ•°æ”¹è¿›
            for baseline_cand, cotrr_cand in zip(baseline_candidates, cotrr_candidates):
                if baseline_cand.get("id") == cotrr_cand.get("id"):
                    baseline_score = baseline_cand.get("score", 0)
                    cotrr_score = cotrr_cand.get("score", 0)
                    improvement = cotrr_score - baseline_score
                    stats["score_improvements"].append(improvement)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        if stats["score_improvements"]:
            stats["average_score_improvement"] = np.mean(stats["score_improvements"])
            stats["score_improvement_std"] = np.std(stats["score_improvements"])
        
        return stats
    
    def _analyze_quality_metrics(self, baseline_items: List, cotrr_items: List) -> Dict:
        """åˆ†æè´¨é‡æŒ‡æ ‡"""
        metrics = {
            "average_scores": {},
            "score_distributions": {},
            "quality_indicators": {}
        }
        
        baseline_scores = []
        cotrr_scores = []
        
        for baseline_item, cotrr_item in zip(baseline_items, cotrr_items):
            for baseline_cand in baseline_item.get("candidates", []):
                baseline_scores.append(baseline_cand.get("score", 0))
            
            for cotrr_cand in cotrr_item.get("candidates", []):
                cotrr_scores.append(cotrr_cand.get("score", 0))
        
        if baseline_scores and cotrr_scores:
            metrics["average_scores"] = {
                "baseline_avg": np.mean(baseline_scores),
                "cotrr_avg": np.mean(cotrr_scores),
                "improvement": np.mean(cotrr_scores) - np.mean(baseline_scores)
            }
            
            metrics["score_distributions"] = {
                "baseline_std": np.std(baseline_scores),
                "cotrr_std": np.std(cotrr_scores),
                "baseline_range": [np.min(baseline_scores), np.max(baseline_scores)],
                "cotrr_range": [np.min(cotrr_scores), np.max(cotrr_scores)]
            }
        
        return metrics
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•"""
        logger.info("ğŸ§ª å¼€å§‹Day 3ç»¼åˆå¯¹æ¯”æµ‹è¯•")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ–‡ä»¶è·¯å¾„
        baseline_output = self.results_dir / f"baseline_results_{timestamp}.json"
        cotrr_output = self.results_dir / f"cotrr_results_{timestamp}.json"
        comparison_output = self.results_dir / f"comparison_report_{timestamp}.json"
        
        # è¿è¡Œbaseline
        logger.info("1ï¸âƒ£ è¿è¡ŒBaseline Pipeline")
        baseline_results = self.run_baseline_pipeline(self.test_data_path, str(baseline_output))
        
        # è¿è¡ŒCoTRR-Stable
        logger.info("2ï¸âƒ£ è¿è¡ŒCoTRR-Stable Pipeline")
        cotrr_results = self.run_cotrr_pipeline(self.test_data_path, str(cotrr_output))
        
        # å¯¹æ¯”ç»“æœ
        logger.info("3ï¸âƒ£ åˆ†æå’Œå¯¹æ¯”ç»“æœ")
        comparison = self.compare_results(baseline_results, cotrr_results)
        
        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        full_report = {
            "test_metadata": {
                "timestamp": timestamp,
                "test_data": self.test_data_path,
                "cotrr_available": COTRR_AVAILABLE
            },
            "baseline_results": baseline_results,
            "cotrr_results": cotrr_results,
            "comparison": comparison,
            "summary": self._generate_summary(baseline_results, cotrr_results, comparison)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(comparison_output, 'w') as f:
            json.dump(full_report, f, indent=2)
        
        logger.info(f"âœ… ç»¼åˆæµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {comparison_output}")
        
        return full_report
    
    def _generate_summary(self, baseline_results: Dict, cotrr_results: Dict, comparison: Dict) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            "overall_status": "UNKNOWN",
            "key_findings": [],
            "performance_verdict": "UNKNOWN",
            "recommendations": []
        }
        
        # æ£€æŸ¥åŸºæœ¬æˆåŠŸç‡
        baseline_success = baseline_results.get("success", False)
        cotrr_success = cotrr_results.get("success", False)
        
        if not baseline_success and not cotrr_success:
            summary["overall_status"] = "BOTH_FAILED"
            summary["key_findings"].append("Both pipelines failed to execute")
            summary["recommendations"].append("Debug pipeline setup and dependencies")
        elif not baseline_success:
            summary["overall_status"] = "BASELINE_FAILED"
            summary["key_findings"].append("Baseline pipeline failed, CoTRR succeeded")
            summary["recommendations"].append("Check baseline pipeline configuration")
        elif not cotrr_success:
            summary["overall_status"] = "COTRR_FAILED"
            summary["key_findings"].append("CoTRR pipeline failed, baseline succeeded")
            summary["recommendations"].append("Debug CoTRR integration issues")
        else:
            summary["overall_status"] = "BOTH_SUCCESS"
            
            # åˆ†ææ€§èƒ½
            overhead_ratio = comparison.get("execution_time_comparison", {}).get("overhead_ratio", 0)
            if overhead_ratio <= 2.0:
                summary["performance_verdict"] = "ACCEPTABLE_OVERHEAD"
                summary["key_findings"].append(f"CoTRR overhead: {overhead_ratio:.2f}x baseline time")
            else:
                summary["performance_verdict"] = "HIGH_OVERHEAD"
                summary["key_findings"].append(f"High CoTRR overhead: {overhead_ratio:.2f}x baseline time")
                summary["recommendations"].append("Optimize CoTRR inference performance")
            
            # åˆ†æè´¨é‡æ”¹è¿›
            score_improvement = comparison.get("quality_metrics", {}).get("average_scores", {}).get("improvement", 0)
            if score_improvement > 0.05:
                summary["key_findings"].append(f"Significant quality improvement: +{score_improvement:.3f}")
                summary["recommendations"].append("CoTRR shows promise, consider production deployment")
            elif score_improvement > 0.01:
                summary["key_findings"].append(f"Modest quality improvement: +{score_improvement:.3f}")
                summary["recommendations"].append("Further optimization may be beneficial")
            else:
                summary["key_findings"].append(f"Limited quality improvement: +{score_improvement:.3f}")
                summary["recommendations"].append("Reassess CoTRR value proposition")
        
        return summary

if __name__ == "__main__":
    comparator = PipelineComparator()
    report = comparator.run_comprehensive_test()
    
    # æ‰“å°å…³é”®ç»“æœ
    print("\n" + "="*50)
    print("ğŸ¯ Day 3 Pipeline Comparison Results")
    print("="*50)
    
    summary = report.get("summary", {})
    print(f"Overall Status: {summary.get('overall_status', 'UNKNOWN')}")
    print(f"Performance Verdict: {summary.get('performance_verdict', 'UNKNOWN')}")
    
    print("\nğŸ” Key Findings:")
    for finding in summary.get("key_findings", []):
        print(f"â€¢ {finding}")
    
    print("\nğŸ’¡ Recommendations:")
    for rec in summary.get("recommendations", []):
        print(f"â€¢ {rec}")
    
    print("\nğŸ“Š Detailed report saved to research/day3_results/")
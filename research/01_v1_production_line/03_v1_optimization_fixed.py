# ===================================================================
# V1.0å¤œé—´ä¼˜åŒ–ç ”ç©¶ - ä¿®å¤ç‰ˆï¼ˆç®€åŒ–ç‰¹å¾å¤„ç†ï¼‰
# ç›®æ ‡ï¼šä½¿ç”¨å¢å¼ºæ•°æ®é›†ä¼˜åŒ–V1.0ç®—æ³•
# ===================================================================

print("ğŸŒ™ V1.0å¤œé—´ä¼˜åŒ–ç ”ç©¶å¯åŠ¨ (ä¿®å¤ç‰ˆ)")
print("="*80)
print("ğŸ¯ ç›®æ ‡: åŸºäºå¢å¼ºæ•°æ®é›†ä¼˜åŒ–V1.0ç®—æ³•")
print("â° è®¡åˆ’: 6å°æ—¶è‡ªåŠ¨åŒ–æ‰§è¡Œ")
print("ğŸ”§ æ–¹æ³•: ç‰¹å¾å·¥ç¨‹ + ç®—æ³•ä¼˜åŒ– + å‚æ•°è°ƒä¼˜")
print("="*80)

import torch
import numpy as np
import json
import time
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import ndcg_score
import warnings
warnings.filterwarnings('ignore')

# æ£€æŸ¥æ‰§è¡Œç¯å¢ƒ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ğŸ”§ æ‰§è¡Œç¯å¢ƒ: {device}')
if torch.cuda.is_available():
    print(f'ğŸš€ GPU: {torch.cuda.get_device_name()}')

# ===================================================================
# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
# ===================================================================

print("\n" + "="*60)
print("ğŸ“Š æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
print("="*60)

# åŠ è½½å¢å¼ºæ•°æ®é›†
try:
    with open('/Users/guyan/computer_vision/computer-vision/data/input/enhanced_dataset.json', 'r') as f:
        production_data = json.load(f)
    inspirations = production_data.get('inspirations', [])
    print(f'âœ… åŠ è½½å¢å¼ºæ•°æ®é›†: {len(inspirations)} ä¸ªæŸ¥è¯¢')
    
    # æ•°æ®ç»Ÿè®¡
    domains = {}
    total_candidates = 0
    for inspiration in inspirations:
        domain = inspiration.get('domain', 'unknown')
        domains[domain] = domains.get(domain, 0) + 1
        total_candidates += len(inspiration.get('candidates', []))
    
    print(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    for domain, count in domains.items():
        print(f"   {domain}: {count} æŸ¥è¯¢")
    print(f"   æ€»å€™é€‰é¡¹: {total_candidates}")
    print(f"   å¹³å‡å€™é€‰é¡¹/æŸ¥è¯¢: {total_candidates/len(inspirations):.1f}")
    
except Exception as e:
    print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    exit(1)

# ===================================================================
# å°æ—¶1-2: ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®å‡†å¤‡
# ===================================================================

print("\n" + "="*60)
print("ğŸ•ğŸ•‘ å°æ—¶1-2: é«˜çº§ç‰¹å¾å·¥ç¨‹")
print("="*60)

class OptimizedFeatureEngineer:
    """ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹å¸ˆ"""
    
    def __init__(self):
        self.feature_dim = 50  # å›ºå®šç‰¹å¾ç»´åº¦
        
    def extract_features(self, inspirations):
        """æå–ç»Ÿä¸€çš„ç‰¹å¾"""
        print("ğŸ”§ æå–ç»Ÿä¸€ç‰¹å¾...")
        
        enhanced_samples = []
        
        for inspiration in inspirations:
            query = inspiration.get('query', '')
            domain = inspiration.get('domain', 'unknown')
            candidates = inspiration.get('candidates', [])
            
            for candidate in candidates:
                # åŸºç¡€ç‰¹å¾
                base_score = candidate.get('score', 0)
                compliance_score = candidate.get('compliance_score', 0)
                title = candidate.get('title', '')
                
                # æ–‡æœ¬ç‰¹å¾
                title_length = len(title)
                query_length = len(query)
                title_words = len(title.split())
                query_words = len(query.split())
                
                # åŒ¹é…ç‰¹å¾
                query_terms = set(query.lower().split())
                title_terms = set(title.lower().split())
                common_terms = len(query_terms & title_terms)
                jaccard_similarity = common_terms / max(len(query_terms | title_terms), 1)
                
                # é¢†åŸŸç‰¹å¾
                domain_features = {
                    'food': [1, 0, 0, 0, 0],
                    'cocktails': [0, 1, 0, 0, 0], 
                    'alcohol': [0, 0, 1, 0, 0],
                    'dining': [0, 0, 0, 1, 0],
                    'beverages': [0, 0, 0, 0, 1]
                }.get(domain, [0, 0, 0, 0, 0])
                
                # å…³é”®è¯ç‰¹å¾
                premium_keywords = ['premium', 'craft', 'artisan', 'gourmet', 'delicious', 'fresh', 'elegant', 'signature']
                keyword_score = sum(1 for kw in premium_keywords if kw in title.lower())
                
                # ç»„åˆæˆå›ºå®šé•¿åº¦ç‰¹å¾å‘é‡
                features = [
                    base_score,
                    compliance_score,
                    title_length / 100.0,  # å½’ä¸€åŒ–
                    query_length / 100.0,
                    title_words / 10.0,
                    query_words / 10.0,
                    common_terms,
                    jaccard_similarity,
                    keyword_score / len(premium_keywords)
                ] + domain_features  # 14ç»´åŸºç¡€ç‰¹å¾
                
                # å¡«å……åˆ°å›ºå®šç»´åº¦
                while len(features) < self.feature_dim:
                    features.append(0.0)
                
                enhanced_samples.append({
                    'query': query,
                    'domain': domain,
                    'candidate': candidate,
                    'features': np.array(features[:self.feature_dim]),
                    'score': base_score,
                    'compliance': compliance_score
                })
        
        print(f"âœ… ç‰¹å¾æå–å®Œæˆ: {len(enhanced_samples)} æ ·æœ¬, {self.feature_dim}ç»´ç‰¹å¾")
        return enhanced_samples

feature_engineer = OptimizedFeatureEngineer()
enhanced_dataset = feature_engineer.extract_features(inspirations)

# ===================================================================
# å°æ—¶3: æœºå™¨å­¦ä¹ ä¼˜åŒ–
# ===================================================================

print("\n" + "="*60)
print("ğŸ•’ å°æ—¶3: æœºå™¨å­¦ä¹ ä¼˜åŒ–")
print("="*60)

class MLOptimizer:
    """æœºå™¨å­¦ä¹ ä¼˜åŒ–å™¨"""
    
    def __init__(self, feature_dim):
        self.feature_dim = feature_dim
        self.model = None
        
    def train_ranking_model(self, dataset):
        """è®­ç»ƒæ’åºæ¨¡å‹"""
        print("ğŸ§  è®­ç»ƒæ’åºä¼˜åŒ–æ¨¡å‹...")
        
        if len(dataset) < 10:
            print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹è®­ç»ƒ")
            return None
            
        # ç®€åŒ–çš„ç¥ç»ç½‘ç»œ
        class RankingNet(torch.nn.Module):
            def __init__(self, input_dim):
                super().__init__()
                self.net = torch.nn.Sequential(
                    torch.nn.Linear(input_dim, 32),
                    torch.nn.ReLU(),
                    torch.nn.Dropout(0.2),
                    torch.nn.Linear(32, 16),
                    torch.nn.ReLU(),
                    torch.nn.Linear(16, 1),
                    torch.nn.Sigmoid()
                )
                
            def forward(self, x):
                return self.net(x)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        features = torch.FloatTensor([d['features'] for d in dataset]).to(device)
        targets = torch.FloatTensor([[d['compliance']] for d in dataset]).to(device)
        
        model = RankingNet(self.feature_dim).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
        criterion = torch.nn.MSELoss()
        
        # è®­ç»ƒ
        model.train()
        for epoch in range(30):
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch+1}/30, Loss: {loss.item():.6f}")
        
        self.model = model
        print("âœ… æ’åºæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return model

ml_optimizer = MLOptimizer(feature_engineer.feature_dim)
ranking_model = ml_optimizer.train_ranking_model(enhanced_dataset)

# ===================================================================
# å°æ—¶4: æƒé‡ä¼˜åŒ–
# ===================================================================

print("\n" + "="*60)
print("ğŸ•“ å°æ—¶4: æƒé‡ä¼˜åŒ–")
print("="*60)

class WeightOptimizer:
    """æƒé‡ä¼˜åŒ–å™¨"""
    
    def optimize_weights(self, dataset):
        """ä¼˜åŒ–æƒé‡å‚æ•°"""
        print("âš–ï¸ ä¼˜åŒ–ç®—æ³•æƒé‡...")
        
        param_grid = {
            'score_weight': [0.3, 0.4, 0.5, 0.6, 0.7],
            'compliance_weight': [0.2, 0.3, 0.4, 0.5],
            'text_weight': [0.1, 0.15, 0.2, 0.25],
            'ml_weight': [0.1, 0.2, 0.3]
        }
        
        best_score = -1
        best_weights = None
        
        sample_size = min(len(dataset), 200)  # é™åˆ¶è®¡ç®—é‡
        sample_data = dataset[:sample_size]
        
        for i, params in enumerate(ParameterGrid(param_grid)):
            if i % 50 == 0:
                print(f"   æµ‹è¯•å‚æ•°ç»„åˆ {i+1}...")
                
            # è®¡ç®—åŠ æƒå¾—åˆ†
            weighted_scores = []
            true_scores = []
            
            for data in sample_data:
                # åŸºç¡€å¾—åˆ†
                base_score = data['score']
                compliance = data['compliance']
                
                # æ–‡æœ¬å¾—åˆ†
                text_score = len(data['candidate'].get('title', '')) / 100.0
                
                # MLå¾—åˆ† (å¦‚æœæœ‰æ¨¡å‹)
                if ranking_model:
                    with torch.no_grad():
                        features = torch.FloatTensor(data['features']).unsqueeze(0).to(device)
                        ml_score = ranking_model(features).item()
                else:
                    ml_score = compliance
                
                # åŠ æƒç»„åˆ
                weighted_score = (
                    params['score_weight'] * base_score +
                    params['compliance_weight'] * compliance +
                    params['text_weight'] * text_score +
                    params['ml_weight'] * ml_score
                )
                
                weighted_scores.append(weighted_score)
                true_scores.append(compliance)
            
            # è¯„ä¼°
            if len(true_scores) > 1:
                correlation = np.corrcoef(weighted_scores, true_scores)[0, 1]
                if not np.isnan(correlation) and correlation > best_score:
                    best_score = correlation
                    best_weights = params
        
        if best_weights is None:
            best_weights = {'score_weight': 0.5, 'compliance_weight': 0.3, 'text_weight': 0.1, 'ml_weight': 0.1}
            best_score = 0
            
        print(f"âœ… æƒé‡ä¼˜åŒ–å®Œæˆ")
        print(f"   æœ€ä¼˜æƒé‡: {best_weights}")
        print(f"   ç›¸å…³æ€§å¾—åˆ†: {best_score:.4f}")
        
        return best_weights, best_score

weight_optimizer = WeightOptimizer()
optimal_weights, correlation_score = weight_optimizer.optimize_weights(enhanced_dataset)

# ===================================================================
# å°æ—¶5: ç®—æ³•éªŒè¯
# ===================================================================

print("\n" + "="*60)
print("ğŸ•” å°æ—¶5: å¢å¼ºç®—æ³•éªŒè¯")
print("="*60)

class EnhancedV1Ranker:
    """å¢å¼ºç‰ˆV1æ’åºå™¨"""
    
    def __init__(self, weights, ml_model=None):
        self.weights = weights
        self.ml_model = ml_model
        
    def rank_candidates(self, query, candidates, domain):
        """æ’åºå€™é€‰é¡¹"""
        enhanced_scores = []
        
        for candidate in candidates:
            # åŸºç¡€åˆ†æ•°
            base_score = candidate.get('score', 0)
            compliance_score = candidate.get('compliance_score', 0)
            title = candidate.get('title', '')
            
            # æ–‡æœ¬ç‰¹å¾
            text_score = min(len(title) / 50.0, 1.0)
            
            # MLé¢„æµ‹ (å¦‚æœæœ‰æ¨¡å‹)
            if self.ml_model:
                # å¿«é€Ÿç‰¹å¾æå–
                features = [
                    base_score, compliance_score, len(title)/100.0, len(query)/100.0,
                    len(title.split())/10.0, len(query.split())/10.0
                ]
                while len(features) < 50:
                    features.append(0.0)
                
                try:
                    with torch.no_grad():
                        feat_tensor = torch.FloatTensor(features).unsqueeze(0).to(device)
                        ml_score = self.ml_model(feat_tensor).item()
                except:
                    ml_score = compliance_score
            else:
                ml_score = compliance_score
                
            # åŠ æƒè¯„åˆ†
            enhanced_score = (
                self.weights.get('score_weight', 0.5) * base_score +
                self.weights.get('compliance_weight', 0.3) * compliance_score +
                self.weights.get('text_weight', 0.1) * text_score +
                self.weights.get('ml_weight', 0.1) * ml_score
            )
            
            enhanced_scores.append({
                'candidate': candidate,
                'enhanced_score': enhanced_score,
                'original_score': base_score
            })
        
        # æ’åº
        enhanced_scores.sort(key=lambda x: x['enhanced_score'], reverse=True)
        return enhanced_scores

# åˆ›å»ºå¢å¼ºæ’åºå™¨
enhanced_ranker = EnhancedV1Ranker(optimal_weights, ranking_model)

# éªŒè¯ç®—æ³•æ”¹è¿›
print("ğŸ” éªŒè¯ç®—æ³•æ”¹è¿›æ•ˆæœ...")
improvements = []
validation_sample = inspirations[:30]  # éªŒè¯æ ·æœ¬

for inspiration in validation_sample:
    query = inspiration.get('query', '')
    domain = inspiration.get('domain', 'unknown')
    candidates = inspiration.get('candidates', [])
    
    if len(candidates) >= 2:
        # åŸå§‹æ’åº
        original_scores = [c.get('score', 0) for c in candidates]
        true_labels = [c.get('compliance_score', 0) for c in candidates]
        
        # å¢å¼ºæ’åº
        enhanced_ranking = enhanced_ranker.rank_candidates(query, candidates, domain)
        enhanced_scores = [r['enhanced_score'] for r in enhanced_ranking]
        
        # è®¡ç®—nDCGæ”¹è¿›
        if len(true_labels) >= 2:
            try:
                original_ndcg = ndcg_score([true_labels], [original_scores], k=5)
                enhanced_ndcg = ndcg_score([true_labels], [enhanced_scores], k=5)
                improvement = enhanced_ndcg - original_ndcg
                improvements.append(improvement)
            except:
                continue

if improvements:
    avg_improvement = np.mean(improvements)
    improvement_std = np.std(improvements)
    positive_improvements = sum(1 for imp in improvements if imp > 0)
    
    print(f"âœ… ç®—æ³•éªŒè¯å®Œæˆ")
    print(f"   éªŒè¯æ ·æœ¬æ•°: {len(improvements)}")
    print(f"   å¹³å‡nDCGæ”¹è¿›: {avg_improvement:+.6f}")
    print(f"   æ”¹è¿›æ ‡å‡†å·®: {improvement_std:.6f}")
    print(f"   æ­£æ”¹è¿›æ¯”ä¾‹: {positive_improvements}/{len(improvements)} ({positive_improvements/len(improvements)*100:.1f}%)")
else:
    avg_improvement = 0
    print("âš ï¸ éªŒè¯æ•°æ®ä¸è¶³")

# ===================================================================
# å°æ—¶6: æœ€ç»ˆé›†æˆå’Œæ€»ç»“
# ===================================================================

print("\n" + "="*60)
print("ğŸ•• å°æ—¶6: æœ€ç»ˆé›†æˆå’Œæ€»ç»“")
print("="*60)

# åˆ›å»ºæœ€ç»ˆå¢å¼ºåŒ…
enhancement_package = {
    'version': 'V1.1-Enhanced-100Dataset',
    'timestamp': datetime.now().isoformat(),
    'dataset_info': {
        'total_queries': len(inspirations),
        'total_candidates': sum(len(q.get('candidates', [])) for q in inspirations),
        'domains': list(set(q.get('domain') for q in inspirations)),
        'enhanced_samples': len(enhanced_dataset)
    },
    'optimizations': {
        'feature_engineering': f'{feature_engineer.feature_dim}ç»´ç»Ÿä¸€ç‰¹å¾',
        'ml_model': 'PyTorchæ·±åº¦æ’åºç½‘ç»œ' if ranking_model else 'æœªå¯ç”¨',
        'weight_optimization': optimal_weights,
        'correlation_score': correlation_score
    },
    'performance_metrics': {
        'avg_ndcg_improvement': avg_improvement,
        'validation_samples': len(improvements) if improvements else 0,
        'positive_improvement_rate': f"{positive_improvements/len(improvements)*100:.1f}%" if improvements else "N/A"
    }
}

print("ğŸ­ åˆ›å»ºç”Ÿäº§å°±ç»ªå¢å¼ºåŒ…...")
print(f"âœ… å¢å¼ºåŒ…ç”Ÿæˆå®Œæˆ:")
print(f"   ç‰ˆæœ¬: {enhancement_package['version']}")
print(f"   æ•°æ®è§„æ¨¡: {enhancement_package['dataset_info']['total_queries']} æŸ¥è¯¢")
print(f"   ç‰¹å¾ç»´åº¦: {feature_engineer.feature_dim}")
print(f"   æ¨¡å‹çŠ¶æ€: {'å·²è®­ç»ƒ' if ranking_model else 'æœªå¯ç”¨'}")

# ===================================================================
# ç ”ç©¶æ€»ç»“
# ===================================================================

print("\n" + "="*80)
print("ğŸŒ… V1.0å¤œé—´ä¼˜åŒ–ç ”ç©¶å®Œæˆ (å¢å¼ºç‰ˆ)")
print("="*80)

print("ğŸ¯ ç ”ç©¶æˆæœæ€»ç»“:")
print(f"âœ… æ•°æ®å¤„ç†: {len(inspirations)} æŸ¥è¯¢, {len(enhanced_dataset)} å¢å¼ºæ ·æœ¬")
print(f"âœ… ç‰¹å¾å·¥ç¨‹: {feature_engineer.feature_dim}ç»´ç»Ÿä¸€ç‰¹å¾")
print(f"âœ… æœºå™¨å­¦ä¹ : {'æ·±åº¦æ’åºæ¨¡å‹è®­ç»ƒå®Œæˆ' if ranking_model else 'æ¨¡å‹è®­ç»ƒè·³è¿‡'}")
print(f"âœ… æƒé‡ä¼˜åŒ–: ç›¸å…³æ€§å¾—åˆ† {correlation_score:.4f}")
print(f"âœ… ç®—æ³•éªŒè¯: nDCGæ”¹è¿› {avg_improvement:+.6f}")

print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
print(f"   ğŸ¯ éªŒè¯æ ·æœ¬: {len(improvements) if improvements else 0}")
if improvements:
    print(f"   ğŸ“ˆ æ­£å‘æ”¹è¿›ç‡: {positive_improvements/len(improvements)*100:.1f}%")
    print(f"   ğŸ“Š å¹³å‡æ”¹è¿›: {avg_improvement:+.6f}")
    print(f"   ğŸ“ æ”¹è¿›æ ‡å‡†å·®: {improvement_std:.6f}")

recommendation = "PROCEED_WITH_TESTING" if avg_improvement > 0 else "CONTINUE_V1_MONITORING"
print(f"\nğŸš€ æœ€ç»ˆå»ºè®®: {recommendation}")

if avg_improvement > 0:
    print("âœ… å¤œé—´ç ”ç©¶å–å¾—æ­£å‘æ”¹è¿›ï¼Œå»ºè®®è¿›è¡ŒA/Bæµ‹è¯•éªŒè¯")
else:
    print("âš ï¸ æ”¹è¿›æ•ˆæœæœ‰é™ï¼Œå»ºè®®ç»§ç»­V1.0ç¨³å®šè¿è¡Œå¹¶æ”¶é›†æ›´å¤šæ•°æ®")

print("\n" + "="*80)
print("ğŸŠ å¤œé—´ç ”ç©¶å®Œæˆï¼")
print("ğŸ’¡ å»ºè®®: æ™¨é—´è¯„ä¼°ç»“æœå¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨")
print("="*80)
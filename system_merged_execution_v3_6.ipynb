{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rc989-alt/computer-vision/blob/main/system_merged_execution_v3_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "yR9PDzPujGF8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR9PDzPujGF8",
        "outputId": "6338b207-f4d6-403d-c281-4e232eb4b676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive mounted\n",
            "ðŸ“ Project root: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean\n",
            "ðŸ¤– Multi-agent root: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent\n",
            "ðŸ§  Memory system: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/memory\n",
            "ðŸ“Š Ledgers: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers\n",
            "ðŸ“„ Reports: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/reports\n"
          ]
        }
      ],
      "source": [
        "#cell_1\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Set project paths\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/cv_multimodal/project'\n",
        "PROJECT_ROOT = f'{GDRIVE_ROOT}/computer-vision-clean'\n",
        "MULTI_AGENT_ROOT = f'{PROJECT_ROOT}/multi-agent'\n",
        "\n",
        "# V3.5: Reflection & Memory System paths\n",
        "MEMORY_ROOT = f'{MULTI_AGENT_ROOT}/memory'\n",
        "LEDGERS_ROOT = f'{MULTI_AGENT_ROOT}/ledgers'\n",
        "REPORTS_ROOT = f'{MULTI_AGENT_ROOT}/reports'\n",
        "LOGS_ROOT = f'{MULTI_AGENT_ROOT}/logs'\n",
        "\n",
        "# Add to Python path\n",
        "sys.path.insert(0, MULTI_AGENT_ROOT)\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Create V3.5 directories if they don't exist\n",
        "os.makedirs(f'{MEMORY_ROOT}/procedural_cache', exist_ok=True)\n",
        "os.makedirs(LEDGERS_ROOT, exist_ok=True)\n",
        "os.makedirs(f'{REPORTS_ROOT}/execution_summary', exist_ok=True)\n",
        "os.makedirs(f'{LOGS_ROOT}/execution_cycles', exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Google Drive mounted\")\n",
        "print(f\"ðŸ“ Project root: {PROJECT_ROOT}\")\n",
        "print(f\"ðŸ¤– Multi-agent root: {MULTI_AGENT_ROOT}\")\n",
        "print(f\"ðŸ§  Memory system: {MEMORY_ROOT}\")\n",
        "print(f\"ðŸ“Š Ledgers: {LEDGERS_ROOT}\")\n",
        "print(f\"ðŸ“„ Reports: {REPORTS_ROOT}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "z_Uh9QdIjGF_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Uh9QdIjGF_",
        "outputId": "e7bd0617-fa11-4e43-b975-fe25df929749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ” FINDING AND LOADING API KEYS\n",
            "================================================================================\n",
            "   Checking: /content/drive/MyDrive/cv_multimodal/project/.env... not found\n",
            "\n",
            "âœ… Found .env file: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/.env\n",
            "   Size: 362 bytes\n",
            "\n",
            "Verifying required API keys:\n",
            "\n",
            "âœ… ANTHROPIC_API_KEY\n",
            "   Value: sk-ant-a...7gAA\n",
            "   Used by: Claude Sonnet (Ops, Quality, Infrastructure)\n",
            "\n",
            "âœ… OPENAI_API_KEY\n",
            "   Value: sk-proj-...-6oA\n",
            "   Used by: GPT-4 (Critical Evaluator)\n",
            "\n",
            "âœ… GOOGLE_API_KEY\n",
            "   Value: AIzaSyBL...FVCI\n",
            "   Used by: Gemini (Research Advisor)\n",
            "\n",
            "================================================================================\n",
            "âœ… ALL API KEYS LOADED SUCCESSFULLY\n",
            "âœ… Loaded 3 total keys from: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/.env\n",
            "âœ… Ready to initialize agents\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "#cell_2\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸ” FINDING AND LOADING API KEYS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Search for .env file in multiple locations\n",
        "search_paths = [\n",
        "    f'{GDRIVE_ROOT}/.env',\n",
        "    f'{GDRIVE_ROOT}/computer-vision-clean/.env',\n",
        "    '/content/drive/MyDrive/cv_multimodal/.env',\n",
        "    '/content/drive/My Drive/cv_multimodal/project/.env',\n",
        "    f'{PROJECT_ROOT}/.env'\n",
        "]\n",
        "\n",
        "env_file = None\n",
        "for path in search_paths:\n",
        "    if Path(path).exists():\n",
        "        env_file = Path(path)\n",
        "        print(f\"\\nâœ… Found .env file: {path}\")\n",
        "        print(f\"   Size: {env_file.stat().st_size} bytes\\n\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"   Checking: {path}... not found\")\n",
        "\n",
        "if not env_file:\n",
        "    print(\"\\nðŸ” Searching entire cv_multimodal directory...\")\n",
        "    base = Path('/content/drive/MyDrive/cv_multimodal')\n",
        "    if base.exists():\n",
        "        all_env = list(base.rglob('*.env')) + list(base.rglob('.env*'))\n",
        "        if all_env:\n",
        "            print(f\"\\nâœ… Found .env files:\")\n",
        "            for f in all_env:\n",
        "                print(f\"   ðŸ“„ {f}\")\n",
        "            env_file = all_env[0]\n",
        "\n",
        "    if not env_file:\n",
        "        print(\"\\nâŒ No .env file found!\")\n",
        "        print(\"\\nðŸ’¡ Options:\")\n",
        "        print(\"   1. Upload .env to MyDrive/cv_multimodal/project/\")\n",
        "        print(\"   2. Use Colab Secrets (ðŸ”‘ icon in left sidebar)\")\n",
        "        raise FileNotFoundError(\"No .env file found - check Google Drive or use Colab Secrets\")\n",
        "\n",
        "# Load all API keys\n",
        "loaded_keys = []\n",
        "with open(env_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line and not line.startswith('#') and '=' in line:\n",
        "            key, value = line.split('=', 1)\n",
        "            key = key.strip()\n",
        "            value = value.strip().strip('\"').strip(\"'\")  # Remove quotes\n",
        "            os.environ[key] = value\n",
        "            loaded_keys.append(key)\n",
        "\n",
        "# Verify required keys for multi-agent system\n",
        "required = {\n",
        "    'ANTHROPIC_API_KEY': 'Claude Sonnet (Ops, Quality, Infrastructure)',\n",
        "    'OPENAI_API_KEY': 'GPT-4 (Critical Evaluator)',\n",
        "    'GOOGLE_API_KEY': 'Gemini (Research Advisor)'\n",
        "}\n",
        "\n",
        "print(\"Verifying required API keys:\\n\")\n",
        "all_loaded = True\n",
        "\n",
        "for key, usage in required.items():\n",
        "    value = os.environ.get(key)\n",
        "    if value and len(value) > 10:\n",
        "        masked = f\"{value[:8]}...{value[-4:]}\"\n",
        "        print(f\"âœ… {key}\")\n",
        "        print(f\"   Value: {masked}\")\n",
        "        print(f\"   Used by: {usage}\\n\")\n",
        "    else:\n",
        "        print(f\"âŒ {key} - NOT FOUND OR INVALID\")\n",
        "        print(f\"   Needed by: {usage}\\n\")\n",
        "        all_loaded = False\n",
        "\n",
        "print(\"=\"*80)\n",
        "if all_loaded:\n",
        "    print(\"âœ… ALL API KEYS LOADED SUCCESSFULLY\")\n",
        "    print(f\"âœ… Loaded {len(loaded_keys)} total keys from: {env_file}\")\n",
        "    print(\"âœ… Ready to initialize agents\")\n",
        "else:\n",
        "    print(\"âŒ SOME API KEYS MISSING OR INVALID\")\n",
        "    print(\"âš ï¸ Agent execution will fail without all 3 keys\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "IoO9SWFUjGF_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoO9SWFUjGF_",
        "outputId": "84f9eec1-f08c-40f0-b5fc-476cd07ec4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dependencies installed\n",
            "âœ… V3.5 Reflection Service dependencies included (pyyaml for memory system)\n"
          ]
        }
      ],
      "source": [
        "  #cell 3\n",
        "  # Install dependencies\n",
        "  !pip install -q anthropic openai google-generativeai python-dotenv pyyaml mlflow tiktoken\n",
        "  !pip install -q torch torchvision transformers open_clip_torch pillow matplotlib seaborn\n",
        "\n",
        "  print(\"âœ… Dependencies installed\")\n",
        "  print(\"âœ… V3.5 Reflection Service dependencies included (pyyaml for memory system)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ob37NqAfjGF_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob37NqAfjGF_",
        "outputId": "61dab646-55ab-4bba-94d8-18e336afcc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ“¥ PENDING ACTIONS FROM PLANNING TEAM\n",
            "================================================================================\n",
            "\n",
            "ðŸ“‹ Meeting ID: None\n",
            "ðŸ—“ï¸  Generated: None\n",
            "ðŸŽ¯ Context: None\n",
            "\n",
            "ðŸ“Š Total tasks: 0\n",
            "   â­ HIGH: 0\n",
            "   ðŸŸ  MEDIUM: 0\n",
            "   ðŸ”µ LOW: 0\n",
            "\n",
            "ðŸ“‹ Task List:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "ðŸ§  V3.5 MEMORY SYSTEM CHECK\n",
            "================================================================================\n",
            "âœ… Found reflection summary from previous cycle\n",
            "   ðŸ“„ /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/reports/reflection_summary.md\n",
            "   Preview: # Reflection Summary\n",
            "\n",
            "**Total Reflections**: 8\n",
            "**Average Confidence**: 0.75\n",
            "**Retry Success Rate**: 100.0%\n",
            "**Eligible for Retry**: 8/8\n",
            "\n",
            "## Top Root Causes\n",
            "- code_error: 8 occurrences\n",
            "...\n",
            "\n",
            "âœ… Loaded 1 learned rules from semantic memory:\n",
            "   â€¢ code_error â†’ Add error handling and validation\n",
            "     Confidence: 0.75\n",
            "â„¹ï¸  No proven templates yet (will be built during execution)\n",
            "\n",
            "================================================================================\n",
            "âœ… Ready to execute tasks with memory-enhanced decision making\n",
            "================================================================================\n",
            "âœ… Multi-agent system imported\n",
            "âœ… V3.5 Reflection Service initialized\n",
            "   ðŸ§  Memory root: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/memory\n",
            "   ðŸ“Š Confidence threshold (Ï„): 0.7\n",
            "   ðŸ”„ Max retries: Approval=2, Exec=2, Verify=2\n",
            "\n",
            "ðŸ¤– Initializing Executive Team:\n",
            "   âœ… Ops Commander (claude-sonnet-4-20250514)\n",
            "   âœ… Quality & Safety Officer (claude-sonnet-4-20250514)\n",
            "   âœ… Infrastructure & Performance Monitor (claude-sonnet-4-20250514)\n",
            "\n",
            "âœ… Executive Team initialized (3 agents)\n",
            "âœ… V3.5 enhanced execution with:\n",
            "   â€¢ Phase 4.5: Approval Retry (confidence-gated)\n",
            "   â€¢ Phase 5.5: Execution Retry (sandboxed)\n",
            "   â€¢ Phase 6.5: Verification Retry (artifact regeneration)\n",
            "   â€¢ Parallel Reflection Service (non-blocking monitoring)\n",
            "\n",
            "ðŸ“ Initialization log: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/logs/system_init/exec_team_init_20251021_204606.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3199308654.py:157: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": datetime.utcnow().isoformat(),\n",
            "/tmp/ipython-input-3199308654.py:188: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  init_log_file = Path(LOGS_ROOT) / 'system_init' / f'exec_team_init_{datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")}.json'\n"
          ]
        }
      ],
      "source": [
        "#cell 4\n",
        "import json\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Read pending actions from Planning Team\n",
        "pending_actions_file = Path(MULTI_AGENT_ROOT) / 'reports/handoff/pending_actions.json'\n",
        "\n",
        "if not pending_actions_file.exists():\n",
        "    print(f\"âŒ No pending actions found at {pending_actions_file}\")\n",
        "    print(\"âš ï¸ Planning Team must generate pending_actions.json first\")\n",
        "    raise FileNotFoundError(f\"Missing: {pending_actions_file}\")\n",
        "\n",
        "with open(pending_actions_file, 'r') as f:\n",
        "    pending_actions = json.load(f)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸ“¥ PENDING ACTIONS FROM PLANNING TEAM\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nðŸ“‹ Meeting ID: {pending_actions.get('meeting_id')}\")\n",
        "print(f\"ðŸ—“ï¸  Generated: {pending_actions.get('generated_at')}\")\n",
        "print(f\"ðŸŽ¯ Context: {pending_actions.get('context')}\")\n",
        "\n",
        "decisions = pending_actions.get('decisions', [])\n",
        "print(f\"\\nðŸ“Š Total tasks: {len(decisions)}\")\n",
        "\n",
        "# Group by priority\n",
        "high_priority = [d for d in decisions if d.get('priority') == 'HIGH']\n",
        "medium_priority = [d for d in decisions if d.get('priority') == 'MEDIUM']\n",
        "low_priority = [d for d in decisions if d.get('priority') == 'LOW']\n",
        "\n",
        "print(f\"   â­ HIGH: {len(high_priority)}\")\n",
        "print(f\"   ðŸŸ  MEDIUM: {len(medium_priority)}\")\n",
        "print(f\"   ðŸ”µ LOW: {len(low_priority)}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Task List:\")\n",
        "for i, decision in enumerate(decisions, 1):\n",
        "    priority = decision.get('priority', 'UNKNOWN')\n",
        "    action = decision.get('action', 'No action specified')\n",
        "    owner = decision.get('owner', 'unassigned')\n",
        "    deadline = decision.get('deadline', 'No deadline')\n",
        "\n",
        "    priority_icon = 'â­' if priority == 'HIGH' else 'ðŸŸ ' if priority == 'MEDIUM' else 'ðŸ”µ'\n",
        "    print(f\"\\n{i}. {priority_icon} [{priority}] {action}\")\n",
        "    print(f\"   ðŸ‘¤ Owner: {owner}\")\n",
        "    print(f\"   â° Deadline: {deadline}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# V3.5: Load Memory System (Learned Rules from Previous Cycles)\n",
        "print(\"\\nðŸ§  V3.5 MEMORY SYSTEM CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check for previous cycle's reflection summary\n",
        "reflection_summary_file = Path(REPORTS_ROOT) / 'reflection_summary.md'\n",
        "if reflection_summary_file.exists():\n",
        "    print(f\"âœ… Found reflection summary from previous cycle\")\n",
        "    print(f\"   ðŸ“„ {reflection_summary_file}\")\n",
        "    with open(reflection_summary_file, 'r') as f:\n",
        "        summary_preview = f.read()[:200]\n",
        "        print(f\"   Preview: {summary_preview}...\")\n",
        "else:\n",
        "    print(f\"â„¹ï¸  No reflection summary from previous cycle (first run)\")\n",
        "\n",
        "# Load semantic memory (long-term learned rules)\n",
        "semantic_file = Path(MEMORY_ROOT) / 'semantic.yml'\n",
        "if semantic_file.exists():\n",
        "    with open(semantic_file, 'r') as f:\n",
        "        semantic_memory = yaml.safe_load(f)\n",
        "\n",
        "    learned_patterns = semantic_memory.get('learned_patterns', [])\n",
        "    if learned_patterns:\n",
        "        print(f\"\\nâœ… Loaded {len(learned_patterns)} learned rules from semantic memory:\")\n",
        "        for rule in learned_patterns[:3]:  # Show first 3\n",
        "            print(f\"   â€¢ {rule.get('pattern')} â†’ {rule.get('fix')}\")\n",
        "            print(f\"     Confidence: {rule.get('confidence', 0):.2f}\")\n",
        "        if len(learned_patterns) > 3:\n",
        "            print(f\"   ... and {len(learned_patterns) - 3} more rules\")\n",
        "    else:\n",
        "        print(f\"â„¹ï¸  No learned patterns yet (will be built during execution)\")\n",
        "else:\n",
        "    print(f\"â„¹ï¸  No semantic memory file (first run)\")\n",
        "\n",
        "# Check procedural cache (proven templates)\n",
        "procedural_cache_dir = Path(MEMORY_ROOT) / 'procedural_cache'\n",
        "if procedural_cache_dir.exists():\n",
        "    templates = list(procedural_cache_dir.glob('*.json'))\n",
        "    if templates:\n",
        "        print(f\"\\nâœ… Found {len(templates)} proven job templates in procedural cache\")\n",
        "        for template in templates[:2]:  # Show first 2\n",
        "            print(f\"   ðŸ“„ {template.name}\")\n",
        "    else:\n",
        "        print(f\"â„¹ï¸  No proven templates yet (will be built during execution)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… Ready to execute tasks with memory-enhanced decision making\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# cell 5\n",
        "# Import multi-agent system components\n",
        "os.chdir(MULTI_AGENT_ROOT)\n",
        "\n",
        "from agents.roles import Agent, AgentConfig, AgentTeam\n",
        "from agents.router import AgentRouter, RoutingStrategy, Message\n",
        "from tools.file_bridge import FileBridge, create_default_policies\n",
        "\n",
        "print(\"âœ… Multi-agent system imported\")\n",
        "\n",
        "# V3.5: Initialize Reflection Service and Retry Mechanisms\n",
        "from reflection_service import ReflectionService, process_execution_reflection\n",
        "from retry_mechanisms import RetryMechanisms\n",
        "\n",
        "reflection_service = ReflectionService(MULTI_AGENT_ROOT)\n",
        "retry_mechanisms = RetryMechanisms(MULTI_AGENT_ROOT)\n",
        "\n",
        "print(\"âœ… V3.5 Reflection Service initialized\")\n",
        "print(f\"   ðŸ§  Memory root: {MEMORY_ROOT}\")\n",
        "print(f\"   ðŸ“Š Confidence threshold (Ï„): {reflection_service.tau}\")\n",
        "print(f\"   ðŸ”„ Max retries: Approval={retry_mechanisms.MAX_APPROVAL_RETRIES}, \"\n",
        "      f\"Exec={retry_mechanisms.MAX_EXEC_RETRIES}, Verify={retry_mechanisms.MAX_VERIF_RETRIES}\")\n",
        "\n",
        "# Initialize Executive Team (3 agents)\n",
        "executive_team_agents = {}\n",
        "prompt_dir = Path(MULTI_AGENT_ROOT) / 'agents/prompts/executive_team'\n",
        "\n",
        "# Define Executive Team configuration\n",
        "executive_config = {\n",
        "    'ops_commander': {\n",
        "        'name': 'Ops Commander',\n",
        "        'model': 'claude-sonnet-4-20250514',\n",
        "        'provider': 'anthropic',\n",
        "        'role': 'Execute research experiments and deployments',\n",
        "        'prompt_file': '02_ops_commander.md',\n",
        "        'max_tokens': 8192  # Increased to allow longer responses\n",
        "    },\n",
        "    'quality_safety': {\n",
        "        'name': 'Quality & Safety Officer',\n",
        "        'model': 'claude-sonnet-4-20250514',\n",
        "        'provider': 'anthropic',\n",
        "        'role': 'Ensure code quality, safety, and reproducibility',\n",
        "        'prompt_file': '01_quality_safety_officer.md',\n",
        "        'max_tokens': 8192\n",
        "    },\n",
        "    'infrastructure': {\n",
        "        'name': 'Infrastructure & Performance Monitor',\n",
        "        'model': 'claude-sonnet-4-20250514',\n",
        "        'provider': 'anthropic',\n",
        "        'role': 'Monitor infrastructure and performance',\n",
        "        'prompt_file': '03_infrastructure_performance_monitor.md',\n",
        "        'max_tokens': 8192\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nðŸ¤– Initializing Executive Team:\")\n",
        "init_log = {\n",
        "    \"timestamp\": datetime.utcnow().isoformat(),\n",
        "    \"team\": \"executive\",\n",
        "    \"v3_5_features\": True,\n",
        "    \"agents\": []\n",
        "}\n",
        "\n",
        "for agent_id, config in executive_config.items():\n",
        "    agent_cfg = AgentConfig(\n",
        "        name=config['name'],\n",
        "        model=config['model'],\n",
        "        provider=config['provider'],\n",
        "        role=config['role'],\n",
        "        max_tokens=config.get('max_tokens', 8192),  # Increased from default 2000\n",
        "        prompt_file=config['prompt_file']\n",
        "    )\n",
        "    executive_team_agents[agent_id] = Agent(agent_cfg, prompt_dir)\n",
        "    print(f\"   âœ… {config['name']} ({config['model']})\")\n",
        "\n",
        "    # V3.5: Log agent initialization\n",
        "    init_log[\"agents\"].append({\n",
        "        \"id\": agent_id,\n",
        "        \"name\": config['name'],\n",
        "        \"model\": config['model'],\n",
        "        \"status\": \"READY\"\n",
        "    })\n",
        "\n",
        "# Create agent team and router\n",
        "executive_team = AgentTeam(executive_team_agents)\n",
        "executive_router = AgentRouter(executive_team)\n",
        "\n",
        "# V3.5: Save initialization log\n",
        "init_log_file = Path(LOGS_ROOT) / 'system_init' / f'exec_team_init_{datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "with open(init_log_file, 'w') as f:\n",
        "    json.dump(init_log, f, indent=2)\n",
        "\n",
        "print(\"\\nâœ… Executive Team initialized (3 agents)\")\n",
        "print(f\"âœ… V3.5 enhanced execution with:\")\n",
        "print(f\"   â€¢ Phase 4.5: Approval Retry (confidence-gated)\")\n",
        "print(f\"   â€¢ Phase 5.5: Execution Retry (sandboxed)\")\n",
        "print(f\"   â€¢ Phase 6.5: Verification Retry (artifact regeneration)\")\n",
        "print(f\"   â€¢ Parallel Reflection Service (non-blocking monitoring)\")\n",
        "print(f\"\\nðŸ“ Initialization log: {init_log_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2xMdbcbZjGGA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xMdbcbZjGGA",
        "outputId": "c9bf9633-68e6-414b-b6ff-c8534576346c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Patched CLIP models to use eager attention implementation by default\n"
          ]
        }
      ],
      "source": [
        "#cell_clip_patch\n",
        "def configure_clip_attn_defaults() -> None:\n",
        "    \"\"\"Force CLIP models to use eager attention so output_attentions is supported.\"\"\"\n",
        "    try:\n",
        "        from transformers import CLIPModel, CLIPTextModel, CLIPVisionModel\n",
        "    except Exception as err:\n",
        "        print(f\"âš ï¸ Unable to patch CLIP attention defaults: {err}\")\n",
        "        return\n",
        "\n",
        "    def _set_config(config):\n",
        "        if hasattr(config, \"attn_implementation\"):\n",
        "            config.attn_implementation = \"eager\"\n",
        "        if hasattr(config, \"output_attentions\"):\n",
        "            config.output_attentions = True\n",
        "\n",
        "    def _patch_from_pretrained(cls, original_fn):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            model = original_fn(*args, **kwargs)\n",
        "            if hasattr(model, \"text_model\") and hasattr(model.text_model, \"config\"):\n",
        "                _set_config(model.text_model.config)\n",
        "            if hasattr(model, \"vision_model\") and hasattr(model.vision_model, \"config\"):\n",
        "                _set_config(model.vision_model.config)\n",
        "            return model\n",
        "        return wrapper\n",
        "\n",
        "    CLIPModel.from_pretrained = _patch_from_pretrained(CLIPModel, CLIPModel.from_pretrained)\n",
        "    CLIPTextModel.from_pretrained = _patch_from_pretrained(CLIPTextModel, CLIPTextModel.from_pretrained)\n",
        "    CLIPVisionModel.from_pretrained = _patch_from_pretrained(CLIPVisionModel, CLIPVisionModel.from_pretrained)\n",
        "\n",
        "    print(\"âœ… Patched CLIP models to use eager attention implementation by default\")\n",
        "\n",
        "\n",
        "configure_clip_attn_defaults()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "VH-F1hV77v9t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH-F1hV77v9t",
        "outputId": "ec801d5d-08e9-4c62-f69b-b51902dbebc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… V3.6 Task execution tracker initialized\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#cell_6\n",
        "# Tracker & shared execution state\n",
        "from typing import Any, Dict, List, Optional\n",
        "class TaskExecutionTracker:\n",
        "    def __init__(self):\n",
        "        self.task_results = []\n",
        "        self.start_time = datetime.now()\n",
        "        self.current_task = None\n",
        "        self.current_task_id: Optional[str] = None\n",
        "        self.task_records: Dict[str, Dict[str, Any]] = {}\n",
        "        self.task_timers: Dict[str, Dict[str, Any]] = {}\n",
        "        self.retry_stats = {\n",
        "            'approval_retries': 0,\n",
        "            'execution_retries': 0,\n",
        "            'verification_retries': 0,\n",
        "            'total_retries': 0,\n",
        "        }\n",
        "\n",
        "    def start_task(self, task_id, action, priority):\n",
        "        start_ts = datetime.now()\n",
        "        task_record = {\n",
        "            'task_id': task_id,\n",
        "            'action': action,\n",
        "            'priority': priority,\n",
        "            'status': 'in_progress',\n",
        "            'start_time': start_ts.isoformat(),\n",
        "            'outputs': [],\n",
        "            'errors': [],\n",
        "            'agent_responses': {},\n",
        "            'retry_attempts': {'approval': 0, 'execution': 0, 'verification': 0},\n",
        "            'patches_applied': [],\n",
        "            'reflection_notes': [],\n",
        "            'confidence_scores': [],\n",
        "            'phases': {\n",
        "                'implementation': None,\n",
        "                'approval': None,\n",
        "                'execution': None,\n",
        "                'verification': None,\n",
        "            },\n",
        "        }\n",
        "        self.current_task = task_record\n",
        "        self.current_task_id = task_id\n",
        "        self.task_results = [t for t in self.task_results if t['task_id'] != task_id]\n",
        "        self.task_records[task_id] = task_record\n",
        "        self.task_timers[task_id] = {\n",
        "            'start': start_ts,\n",
        "            'end': None,\n",
        "            'duration': 0.0,\n",
        "        }\n",
        "        print(f\"\\nðŸš€ Starting Task {task_id}: {action}\")\n",
        "        print(f\"   Priority: {priority}\")\n",
        "\n",
        "    def log_agent_response(self, agent_name, response):\n",
        "        if self.current_task:\n",
        "            self.current_task['agent_responses'][agent_name] = response\n",
        "            print(f\"   âœ… {agent_name} responded ({len(response)} chars)\")\n",
        "\n",
        "    def log_retry_attempt(self, phase, patch_id=None, confidence=None):\n",
        "        if self.current_task:\n",
        "            self.current_task['retry_attempts'][phase] += 1\n",
        "            self.retry_stats[f'{phase}_retries'] += 1\n",
        "            self.retry_stats['total_retries'] += 1\n",
        "            if patch_id:\n",
        "                self.current_task['patches_applied'].append(\n",
        "                    {\n",
        "                        'phase': phase,\n",
        "                        'patch_id': patch_id,\n",
        "                        'confidence': confidence,\n",
        "                        'timestamp': datetime.now().isoformat(),\n",
        "                    }\n",
        "                )\n",
        "            print(\n",
        "                f\"   ðŸ”„ Retry #{self.current_task['retry_attempts'][phase]} ({phase})\"\n",
        "                + (f\" - Confidence: {confidence:.2f}\" if confidence else \"\")\n",
        "            )\n",
        "\n",
        "    def log_reflection_note(self, reflection_note):\n",
        "        if self.current_task:\n",
        "            self.current_task['reflection_notes'].append(reflection_note)\n",
        "            print(f\"   ðŸ§  Reflection: {reflection_note.get('why_failed', 'unknown')}\")\n",
        "\n",
        "    def log_phase_completion(self, phase, status, details=None):\n",
        "        if self.current_task:\n",
        "            self.current_task['phases'][phase] = {\n",
        "                'status': status,\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'details': details,\n",
        "            }\n",
        "            status_icon = 'âœ…' if status == 'pass' else 'âŒ' if status == 'fail' else 'â­ï¸'\n",
        "            print(f\"   {status_icon} Phase {phase}: {status}\")\n",
        "\n",
        "    def activate_task(self, task_id: str) -> None:\n",
        "        task_record = self.task_records.get(task_id)\n",
        "        if task_record:\n",
        "            self.current_task = task_record\n",
        "            self.current_task_id = task_id\n",
        "\n",
        "    def complete_task(self, status='completed', final_status_reason=None, task_id: Optional[str] = None):\n",
        "        valid_statuses = {'completed', 'failed', 'rejected', 'cancelled', 'skipped'}\n",
        "\n",
        "        if task_id is None and status not in valid_statuses and final_status_reason in valid_statuses:\n",
        "            # Support legacy call pattern: complete_task(task_id, 'completed')\n",
        "            task_id = status\n",
        "            status = final_status_reason\n",
        "            final_status_reason = None\n",
        "\n",
        "        if task_id:\n",
        "            task_record = self.task_records.get(task_id)\n",
        "            if task_record:\n",
        "                self.current_task = task_record\n",
        "                self.current_task_id = task_id\n",
        "        else:\n",
        "            task_record = self.current_task\n",
        "            task_id = getattr(self, \"current_task_id\", None)\n",
        "\n",
        "        if not task_record:\n",
        "            print(\"âš ï¸ Unable to complete task â€“ no active task context.\")\n",
        "            return\n",
        "\n",
        "        task_record['status'] = status\n",
        "        task_record['status_reason'] = final_status_reason\n",
        "        task_record['end_time'] = datetime.now().isoformat()\n",
        "        total_retries = sum(task_record['retry_attempts'].values())\n",
        "        task_record['total_retries'] = total_retries\n",
        "\n",
        "        # Update task timers\n",
        "        timer_entry = self.task_timers.setdefault(task_id, {})\n",
        "        end_time = datetime.fromisoformat(task_record['end_time'])\n",
        "        timer_entry['end'] = end_time\n",
        "        start_value = timer_entry.get('start')\n",
        "        if isinstance(start_value, datetime):\n",
        "            start_dt = start_value\n",
        "        elif isinstance(start_value, str):\n",
        "            start_dt = datetime.fromisoformat(start_value)\n",
        "        else:\n",
        "            start_dt = datetime.fromisoformat(task_record['start_time'])\n",
        "            timer_entry['start'] = start_dt\n",
        "        timer_entry['duration'] = max((end_time - start_dt).total_seconds(), 0.0)\n",
        "\n",
        "        # Replace existing record for deterministic ordering\n",
        "        self.task_results = [t for t in self.task_results if t['task_id'] != task_id]\n",
        "        self.task_results.append(task_record)\n",
        "\n",
        "        print(f\"   âœ… Task completed in {timer_entry['duration']:.1f}s - Status: {status}\")\n",
        "        if total_retries > 0:\n",
        "            print(f\"   ðŸ”„ Total retries: {total_retries}\")\n",
        "\n",
        "        if self.current_task_id == task_id:\n",
        "            self.current_task = None\n",
        "            self.current_task_id = None\n",
        "\n",
        "    def get_summary(self):\n",
        "        completed = len([t for t in self.task_results if t['status'] == 'completed'])\n",
        "        failed = len([t for t in self.task_results if t['status'] == 'failed'])\n",
        "        total_duration = (datetime.now() - self.start_time).total_seconds()\n",
        "\n",
        "        return {\n",
        "            'total_tasks': len(self.task_results),\n",
        "            'completed': completed,\n",
        "            'failed': failed,\n",
        "            'total_duration_seconds': total_duration,\n",
        "            'retry_stats': self.retry_stats,\n",
        "            'task_results': self.task_results,\n",
        "        }\n",
        "\n",
        "\n",
        "# Initialize tracker\n",
        "tracker = TaskExecutionTracker()\n",
        "print(\"âœ… V3.6 Task execution tracker initialized\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "6jHMnZ58jGGB",
      "metadata": {
        "id": "6jHMnZ58jGGB"
      },
      "outputs": [],
      "source": [
        "#cell_7\n",
        "\n",
        "\n",
        "def _fresh_execution_stats() -> Dict[str, Any]:\n",
        "    return {\n",
        "        'tasks_processed': 0,\n",
        "        'tasks_approved': 0,\n",
        "        'tasks_executed': 0,\n",
        "        'tasks_completed': 0,\n",
        "        'retry_attempts': {'approval': 0, 'execution': 0, 'verification': 0},\n",
        "    }\n",
        "\n",
        "\n",
        "execution_stats: Dict[str, Any] = _fresh_execution_stats()\n",
        "task_contexts: Dict[str, Dict[str, Any]] = {}\n",
        "current_task_order: List[str] = []\n",
        "\n",
        "\n",
        "def reset_execution_state() -> None:\n",
        "    \"\"\"Reset tracker, stats, and cached task contexts.\"\"\"\n",
        "    global tracker, execution_stats, task_contexts, current_task_order\n",
        "    tracker = TaskExecutionTracker()\n",
        "    execution_stats = _fresh_execution_stats()\n",
        "    task_contexts = {}\n",
        "    current_task_order = []\n",
        "    print(\"ðŸ”„ Execution state reset\")\n",
        "\n",
        "\n",
        "def parse_agent_verdict(response: str) -> Dict[str, Any]:\n",
        "    \"\"\"Extract structured verdict from agent response.\"\"\"\n",
        "    verdict_patterns = [\n",
        "        r'\\*\\*.*?Final Verdict:\\*\\*\\s*(âœ…\\s*APPROVED|âš ï¸\\s*CAUTION|âŒ\\s*BLOCKED)',\n",
        "        r'Final Verdict:\\s*(APPROVED|CAUTION|BLOCKED)',\n",
        "        r'Status:\\s*(APPROVED|REJECTED)',\n",
        "    ]\n",
        "\n",
        "    for pattern in verdict_patterns:\n",
        "        match = re.search(pattern, response, re.IGNORECASE)\n",
        "        if match:\n",
        "            verdict_text = match.group(1).upper()\n",
        "            if 'APPROVED' in verdict_text or 'âœ…' in verdict_text:\n",
        "                return {'structured': True, 'status': 'APPROVED', 'confidence': 0.9}\n",
        "            if 'BLOCKED' in verdict_text or 'âŒ' in verdict_text:\n",
        "                return {'structured': True, 'status': 'REJECTED', 'confidence': 0.9}\n",
        "            return {'structured': True, 'status': 'CAUTION', 'confidence': 0.5}\n",
        "\n",
        "    approved_keywords = ['approved', 'looks good', 'ready', 'pass', 'âœ…']\n",
        "    rejected_keywords = ['blocked', 'rejected', 'unsafe', 'fail', 'âŒ']\n",
        "\n",
        "    approved_count = sum(1 for kw in approved_keywords if kw in response.lower())\n",
        "    rejected_count = sum(1 for kw in rejected_keywords if kw in response.lower())\n",
        "\n",
        "    if approved_count > rejected_count:\n",
        "        return {'structured': False, 'status': 'APPROVED', 'confidence': 0.6}\n",
        "    if rejected_count > approved_count:\n",
        "        return {'structured': False, 'status': 'REJECTED', 'confidence': 0.6}\n",
        "    return {'structured': False, 'status': 'REJECTED', 'confidence': 0.3}\n",
        "\n",
        "\n",
        "def determine_task_status_v2(\n",
        "    ops_response: str,\n",
        "    quality_response: str,\n",
        "    infra_response: str,\n",
        ") -> str:\n",
        "    \"\"\"Enhanced approval gate with structured verdict support.\"\"\"\n",
        "    ops_verdict = parse_agent_verdict(ops_response)\n",
        "    quality_verdict = parse_agent_verdict(quality_response)\n",
        "    infra_verdict = parse_agent_verdict(infra_response)\n",
        "\n",
        "    ops_approved = ops_verdict['status'] == 'APPROVED'\n",
        "    quality_approved = quality_verdict['status'] == 'APPROVED'\n",
        "    infra_approved = infra_verdict['status'] == 'APPROVED'\n",
        "\n",
        "    print(f\"\\n   ðŸ” Approval Gate Analysis:\")\n",
        "    print(f\"      Ops: {ops_verdict['status']} (conf: {ops_verdict['confidence']:.2f})\")\n",
        "    print(f\"      Quality: {quality_verdict['status']} (conf: {quality_verdict['confidence']:.2f})\")\n",
        "    print(f\"      Infrastructure: {infra_verdict['status']} (conf: {infra_verdict['confidence']:.2f})\")\n",
        "\n",
        "    if ops_approved and quality_approved and infra_approved:\n",
        "        avg_conf = (\n",
        "            ops_verdict['confidence']\n",
        "            + quality_verdict['confidence']\n",
        "            + infra_verdict['confidence']\n",
        "        ) / 3\n",
        "        print(f\"      âœ… ALL GATES APPROVED (avg confidence: {avg_conf:.2f})\")\n",
        "        return \"approved\"\n",
        "    print(\"      âŒ SOME GATES REJECTED\")\n",
        "    return \"rejected\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "Nu_4g1JCWgpy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu_4g1JCWgpy",
        "outputId": "342c86bf-b4cb-45c8-e8a8-c9d4ce57980d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Multi-agent root resolved to: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent\n",
            "   âœ… reflections: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers/reflections.jsonl\n",
            "   âœ… episodic_memory: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/memory/episodic.jsonl\n",
            "   âœ… semantic_memory: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/memory/semantic.yml\n",
            "   âœ… patches_dir: /content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/patches\n",
            "   ðŸ§  Reflection service Ï„ threshold: 0.7\n",
            "âœ… Patched CLIP models to use eager attention implementation by default\n",
            "âœ… V3.6 Task execution tracker initialized\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#cell 7\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# FINAL COMPLETE PHASES 3-7 LOOP (V3.6 WITH TRACKER INITIALIZATION)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Copy this ENTIRE file into ONE Colab cell after Cell 8 (agent initialization)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from reflection_service import ReflectionService, process_execution_reflection\n",
        "from retry_mechanisms import RetryMechanisms\n",
        "\n",
        "\n",
        "# Locate multi-agent root (supports Google Drive trajectories)\n",
        "_candidate_roots = []\n",
        "\n",
        "env_root = os.environ.get(\"MULTI_AGENT_ROOT\")\n",
        "if env_root:\n",
        "    _candidate_roots.append(Path(env_root))\n",
        "\n",
        "_candidate_roots.extend([\n",
        "    Path(\"/Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent\"),\n",
        "    Path(\"/content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent\"),\n",
        "    Path.cwd() / \"multi-agent\",\n",
        "    Path.cwd(),\n",
        "])\n",
        "\n",
        "MULTI_AGENT_ROOT = next((p for p in _candidate_roots if p and p.exists()), Path.cwd())\n",
        "\n",
        "print(f\"ðŸ“‚ Multi-agent root resolved to: {MULTI_AGENT_ROOT}\")\n",
        "\n",
        "_ledger_paths = {\n",
        "    \"reflections\": MULTI_AGENT_ROOT / \"ledgers/reflections.jsonl\",\n",
        "    \"episodic_memory\": MULTI_AGENT_ROOT / \"memory/episodic.jsonl\",\n",
        "    \"semantic_memory\": MULTI_AGENT_ROOT / \"memory/semantic.yml\",\n",
        "    \"patches_dir\": MULTI_AGENT_ROOT / \"patches\",\n",
        "}\n",
        "\n",
        "for name, path in _ledger_paths.items():\n",
        "    status = \"âœ…\" if path.exists() else \"âš ï¸\"\n",
        "    print(f\"   {status} {name}: {path}\")\n",
        "\n",
        "reflection_service = ReflectionService(str(MULTI_AGENT_ROOT))\n",
        "retry_mechanisms = RetryMechanisms(str(MULTI_AGENT_ROOT))\n",
        "print(f\"   ðŸ§  Reflection service Ï„ threshold: {reflection_service.tau}\")\n",
        "\n",
        "#cell_clip_patch\n",
        "_CLIP_ATTENTION_PATCHED = False\n",
        "\n",
        "\n",
        "def _apply_clip_attention_patch() -> None:\n",
        "    \"\"\"Force CLIP models to use eager attention so output_attentions is supported.\"\"\"\n",
        "    global _CLIP_ATTENTION_PATCHED\n",
        "    if _CLIP_ATTENTION_PATCHED:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        from transformers import CLIPModel, CLIPTextModel, CLIPVisionModel\n",
        "    except Exception as err:\n",
        "        print(f\"âš ï¸ Unable to patch CLIP attention defaults: {err}\")\n",
        "        return\n",
        "\n",
        "    def _force_eager(module) -> None:\n",
        "        if module is None:\n",
        "            return\n",
        "        if hasattr(module, \"set_attn_implementation\"):\n",
        "            try:\n",
        "                module.set_attn_implementation(\"eager\")\n",
        "            except TypeError:\n",
        "                pass\n",
        "        config = getattr(module, \"config\", None)\n",
        "        if config is not None:\n",
        "            if hasattr(config, \"attn_implementation\"):\n",
        "                config.attn_implementation = \"eager\"\n",
        "            if hasattr(config, \"output_attentions\"):\n",
        "                config.output_attentions = True\n",
        "\n",
        "    def _configure_model(model):\n",
        "        _force_eager(model)\n",
        "        for attr in (\"vision_model\", \"text_model\"):\n",
        "            _force_eager(getattr(model, attr, None))\n",
        "        for attr in (\"vision_model\", \"text_model\"):\n",
        "            sub = getattr(model, attr, None)\n",
        "            if sub is None:\n",
        "                continue\n",
        "            for child in getattr(sub, \"modules\", lambda: [])():\n",
        "                _force_eager(child)\n",
        "        return model\n",
        "\n",
        "    def _wrap_from_pretrained(cls, original_fn):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            kwargs.setdefault(\"attn_implementation\", \"eager\")\n",
        "            model = original_fn(*args, **kwargs)\n",
        "            return _configure_model(model)\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    CLIPModel.from_pretrained = _wrap_from_pretrained(CLIPModel, CLIPModel.from_pretrained)\n",
        "    CLIPTextModel.from_pretrained = _wrap_from_pretrained(CLIPTextModel, CLIPTextModel.from_pretrained)\n",
        "    CLIPVisionModel.from_pretrained = _wrap_from_pretrained(CLIPVisionModel, CLIPVisionModel.from_pretrained)\n",
        "\n",
        "    _CLIP_ATTENTION_PATCHED = True\n",
        "    print(\"âœ… Patched CLIP models to use eager attention implementation by default\")\n",
        "\n",
        "\n",
        "def configure_clip_attn_defaults() -> None:\n",
        "    _apply_clip_attention_patch()\n",
        "\n",
        "\n",
        "def ensure_clip_attention_patch() -> None:\n",
        "    _apply_clip_attention_patch()\n",
        "\n",
        "\n",
        "configure_clip_attn_defaults()\n",
        "\n",
        "#cell_6\n",
        "# Tracker & shared execution state\n",
        "class TaskExecutionTracker:\n",
        "    def __init__(self):\n",
        "        self.task_results = []\n",
        "        self.start_time = datetime.now()\n",
        "        self.current_task = None\n",
        "        self.retry_stats = {\n",
        "            'approval_retries': 0,\n",
        "            'execution_retries': 0,\n",
        "            'verification_retries': 0,\n",
        "            'total_retries': 0,\n",
        "        }\n",
        "\n",
        "    def start_task(self, task_id, action, priority):\n",
        "        self.current_task = {\n",
        "            'task_id': task_id,\n",
        "            'action': action,\n",
        "            'priority': priority,\n",
        "            'status': 'in_progress',\n",
        "            'start_time': datetime.now().isoformat(),\n",
        "            'outputs': [],\n",
        "            'errors': [],\n",
        "            'agent_responses': {},\n",
        "            'retry_attempts': {'approval': 0, 'execution': 0, 'verification': 0},\n",
        "            'patches_applied': [],\n",
        "            'reflection_notes': [],\n",
        "            'confidence_scores': [],\n",
        "            'phases': {\n",
        "                'implementation': None,\n",
        "                'approval': None,\n",
        "                'execution': None,\n",
        "                'verification': None,\n",
        "            },\n",
        "        }\n",
        "        print(f\"\\nðŸš€ Starting Task {task_id}: {action}\")\n",
        "        print(f\"   Priority: {priority}\")\n",
        "\n",
        "    def log_agent_response(self, agent_name, response):\n",
        "        if self.current_task:\n",
        "            self.current_task['agent_responses'][agent_name] = response\n",
        "            print(f\"   âœ… {agent_name} responded ({len(response)} chars)\")\n",
        "\n",
        "    def log_retry_attempt(self, phase, patch_id=None, confidence=None):\n",
        "        if self.current_task:\n",
        "            self.current_task['retry_attempts'][phase] += 1\n",
        "            self.retry_stats[f'{phase}_retries'] += 1\n",
        "            self.retry_stats['total_retries'] += 1\n",
        "            if patch_id:\n",
        "                self.current_task['patches_applied'].append(\n",
        "                    {\n",
        "                        'phase': phase,\n",
        "                        'patch_id': patch_id,\n",
        "                        'confidence': confidence,\n",
        "                        'timestamp': datetime.now().isoformat(),\n",
        "                    }\n",
        "                )\n",
        "            print(\n",
        "                f\"   ðŸ”„ Retry #{self.current_task['retry_attempts'][phase]} ({phase})\"\n",
        "                + (f\" - Confidence: {confidence:.2f}\" if confidence else \"\")\n",
        "            )\n",
        "\n",
        "    def log_reflection_note(self, reflection_note):\n",
        "        if self.current_task:\n",
        "            self.current_task['reflection_notes'].append(reflection_note)\n",
        "            print(f\"   ðŸ§  Reflection: {reflection_note.get('why_failed', 'unknown')}\")\n",
        "\n",
        "    def log_phase_completion(self, phase, status, details=None):\n",
        "        if self.current_task:\n",
        "            self.current_task['phases'][phase] = {\n",
        "                'status': status,\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'details': details,\n",
        "            }\n",
        "            status_icon = 'âœ…' if status == 'pass' else 'âŒ' if status == 'fail' else 'â­ï¸'\n",
        "            print(f\"   {status_icon} Phase {phase}: {status}\")\n",
        "\n",
        "    def complete_task(self, status='completed', final_status_reason=None):\n",
        "        if self.current_task:\n",
        "            self.current_task['status'] = status\n",
        "            self.current_task['status_reason'] = final_status_reason\n",
        "            self.current_task['end_time'] = datetime.now().isoformat()\n",
        "            total_retries = sum(self.current_task['retry_attempts'].values())\n",
        "            self.current_task['total_retries'] = total_retries\n",
        "            self.task_results.append(self.current_task)\n",
        "\n",
        "            duration = (\n",
        "                datetime.fromisoformat(self.current_task['end_time'])\n",
        "                - datetime.fromisoformat(self.current_task['start_time'])\n",
        "            ).total_seconds()\n",
        "\n",
        "            print(f\"   âœ… Task completed in {duration:.1f}s - Status: {status}\")\n",
        "            if total_retries > 0:\n",
        "                print(f\"   ðŸ”„ Total retries: {total_retries}\")\n",
        "\n",
        "            self.current_task = None\n",
        "\n",
        "    def get_summary(self):\n",
        "        completed = len([t for t in self.task_results if t['status'] == 'completed'])\n",
        "        failed = len([t for t in self.task_results if t['status'] == 'failed'])\n",
        "        total_duration = (datetime.now() - self.start_time).total_seconds()\n",
        "\n",
        "        return {\n",
        "            'total_tasks': len(self.task_results),\n",
        "            'completed': completed,\n",
        "            'failed': failed,\n",
        "            'total_duration_seconds': total_duration,\n",
        "            'retry_stats': self.retry_stats,\n",
        "            'task_results': self.task_results,\n",
        "        }\n",
        "\n",
        "\n",
        "# Initialize tracker\n",
        "tracker = TaskExecutionTracker()\n",
        "print(\"âœ… V3.6 Task execution tracker initialized\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "istqXjC07c3k",
      "metadata": {
        "id": "istqXjC07c3k"
      },
      "outputs": [],
      "source": [
        "#cell_8\n",
        "from copy import deepcopy\n",
        "import re\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "ops_commander = executive_team_agents['ops_commander']\n",
        "quality_safety = executive_team_agents['quality_safety']\n",
        "infrastructure = executive_team_agents['infrastructure']\n",
        "\n",
        "_phase0_state = {\"completed\": False, \"data_dir\": None}\n",
        "_json_patch_applied = False\n",
        "\n",
        "\n",
        "def ensure_phase_0() -> None:\n",
        "    \"\"\"Prepare synthetic assets used across execution phases.\"\"\"\n",
        "    if _phase0_state[\"completed\"]:\n",
        "        return\n",
        "\n",
        "    print(\"\\nðŸ› ï¸ PHASE 0: Pre-Execution Infrastructure Setup\")\n",
        "    try:\n",
        "        from PIL import Image\n",
        "        import numpy as np\n",
        "\n",
        "        test_dir = Path(\"/content/test_data\")\n",
        "        test_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(\"   ðŸ–¼ï¸ Creating test images for model validation...\")\n",
        "        for i in range(5):\n",
        "            test_img = Image.fromarray(\n",
        "                np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
        "            )\n",
        "            test_img.save(test_dir / f\"test_image_{i}.png\")\n",
        "\n",
        "        test_images = list(test_dir.glob('*.png'))\n",
        "        print(f\"   âœ… Created {len(test_images)} test images at {test_dir}\")\n",
        "        print(f\"   ðŸ“ Primary test image: {test_dir / 'test_image_0.png'}\")\n",
        "\n",
        "        os.environ['TEST_IMAGE_PATH'] = str(test_dir / \"test_image_0.png\")\n",
        "        os.environ['TEST_DATA_DIR'] = str(test_dir)\n",
        "\n",
        "        _phase0_state[\"completed\"] = True\n",
        "        _phase0_state[\"data_dir\"] = str(test_dir)\n",
        "        print(\"   âœ… Phase 0 infrastructure setup complete\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ Phase 0 setup warning: {e}\")\n",
        "        print(\"   â„¹ï¸ Continuing execution - agents will handle infrastructure setup\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def _ensure_numpy_json_patch() -> None:\n",
        "    \"\"\"Monkey-patch json.dump/json.dumps to gracefully handle numpy scalars.\"\"\"\n",
        "    global _json_patch_applied\n",
        "    if _json_patch_applied:\n",
        "        return\n",
        "\n",
        "    import json\n",
        "\n",
        "    try:\n",
        "        import numpy as np  # type: ignore\n",
        "    except Exception:  # pragma: no cover - numpy may not be available\n",
        "        np = None\n",
        "\n",
        "    original_dump = json.dump\n",
        "    original_dumps = json.dumps\n",
        "\n",
        "    def _default(o):  # type: ignore[override]\n",
        "        if np is not None and isinstance(o, np.generic):\n",
        "            return o.item()\n",
        "        if hasattr(o, \"tolist\"):\n",
        "            try:\n",
        "                return o.tolist()\n",
        "            except Exception:\n",
        "                pass\n",
        "        if isinstance(o, set):\n",
        "            return list(o)\n",
        "        if isinstance(o, bytes):\n",
        "            return o.decode(\"utf-8\", errors=\"replace\")\n",
        "        return str(o)\n",
        "\n",
        "    def _patched_dump(obj, fp, *args, **kwargs):\n",
        "        kwargs.setdefault(\"default\", _default)\n",
        "        return original_dump(obj, fp, *args, **kwargs)\n",
        "\n",
        "    def _patched_dumps(obj, *args, **kwargs):\n",
        "        kwargs.setdefault(\"default\", _default)\n",
        "        return original_dumps(obj, *args, **kwargs)\n",
        "\n",
        "    json.dump = _patched_dump  # type: ignore[assignment]\n",
        "    json.dumps = _patched_dumps  # type: ignore[assignment]\n",
        "    _json_patch_applied = True\n",
        "\n",
        "\n",
        "def _verify_research_outputs(task_id: str) -> None:\n",
        "    \"\"\"Enforce research-grade artifacts for critical Week 1 tasks.\"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    root = Path(MULTI_AGENT_ROOT)\n",
        "    week1_dir = root / 'results' / 'week1'\n",
        "\n",
        "    if task_id == \"W1-004\":\n",
        "        expected = week1_dir / 'clip_baseline_metrics.json'\n",
        "        if not expected.exists():\n",
        "            raise FileNotFoundError(\n",
        "                \"Expected aggregated CLIP baseline metrics at \"\n",
        "                f\"{expected}. Export per-layer entropy/dispersion statistics \"\n",
        "                \"and log them to MLflow before marking the task complete.\"\n",
        "            )\n",
        "    elif task_id == \"W1-006\":\n",
        "        required = [\n",
        "            week1_dir / 'statistical_analysis.md',\n",
        "            week1_dir / 'hypothesis_tests.json',\n",
        "            week1_dir / 'effect_sizes.csv',\n",
        "        ]\n",
        "        missing = [str(path) for path in required if not path.exists()]\n",
        "        if missing:\n",
        "            raise FileNotFoundError(\n",
        "                \"Statistical analysis artifacts missing. Generate the \"\n",
        "                f\"following files: {', '.join(missing)}.\"\n",
        "            )\n",
        "\n",
        "\n",
        "def initialize_task_context(task: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Create the mutable context object tracked across all phases.\"\"\"\n",
        "    ctx = {\n",
        "        'task': task,\n",
        "        'task_id': task['task_id'],\n",
        "        'priority': task.get('priority', 'MEDIUM'),\n",
        "        'action': task['action'],\n",
        "        'agent_responses': {},\n",
        "        'task_result': None,\n",
        "        'execution_telemetry': {'task_id': task['task_id'], 'exit_code': 0},\n",
        "        'preliminary_status': None,\n",
        "        'execution_success': False,\n",
        "        'verification_passed': False,\n",
        "        '_execution_runs': 0,\n",
        "        '_verification_runs': 0,\n",
        "    }\n",
        "    tracker.start_task(ctx['task_id'], ctx['action'], ctx['priority'])\n",
        "    execution_stats['tasks_processed'] += 1\n",
        "    task_contexts[ctx['task_id']] = ctx\n",
        "    return ctx\n",
        "\n",
        "\n",
        "def _fallback_ops_response(task_id: str) -> str:\n",
        "    \"\"\"Return a deterministic Ops Commander response with a full CLIP validation script.\"\"\"\n",
        "    import textwrap\n",
        "\n",
        "    code_body = f\"\"\"\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import mlflow\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "\n",
        "EXPERIMENT_NAME = \"CVPR_Week1_CLIP_Attention_Checks\"\n",
        "RUN_TAG = \"{task_id}_verification\"\n",
        "\n",
        "\n",
        "def _ensure_mlflow_experiment(name: str) -> str:\n",
        "    mlflow.set_tracking_uri(\"file:///content/mlruns\")\n",
        "    existing = mlflow.get_experiment_by_name(name)\n",
        "    return existing.experiment_id if existing else mlflow.create_experiment(name)\n",
        "\n",
        "\n",
        "def _load_test_image() -> Image.Image:\n",
        "    path = os.environ.get(\"TEST_IMAGE_PATH\")\n",
        "    if path and os.path.exists(path):\n",
        "        return Image.open(path).convert(\"RGB\")\n",
        "    data_dir = os.environ.get(\"TEST_DATA_DIR\")\n",
        "    if data_dir and os.path.isdir(data_dir):\n",
        "        for candidate in sorted(Path(data_dir).glob(\"*.png\")):\n",
        "            return Image.open(candidate).convert(\"RGB\")\n",
        "    return Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
        "\n",
        "\n",
        "def _patch_model_for_attn(model: CLIPModel) -> CLIPModel:\n",
        "    if hasattr(model.vision_model, \"set_attn_implementation\"):\n",
        "        model.vision_model.set_attn_implementation(\"eager\")\n",
        "    else:\n",
        "        model.vision_model.config.attn_implementation = \"eager\"\n",
        "    if hasattr(model.text_model, \"set_attn_implementation\"):\n",
        "        model.text_model.set_attn_implementation(\"eager\")\n",
        "    else:\n",
        "        model.text_model.config.attn_implementation = \"eager\"\n",
        "    model.vision_model.config.output_attentions = True\n",
        "    model.text_model.config.output_attentions = True\n",
        "    return model\n",
        "\n",
        "\n",
        "def _load_clip_model():\n",
        "    model_name = \"openai/clip-vit-base-patch32\"\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model = CLIPModel.from_pretrained(model_name)\n",
        "    model = _patch_model_for_attn(model)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model, processor, device\n",
        "\n",
        "\n",
        "def _run_attention_pass(model, processor, device, image):\n",
        "    inputs = processor(\n",
        "        text=[\"Synthetic CLIP validation image\"],\n",
        "        images=image,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    vision_attn = outputs.vision_model_output.attentions\n",
        "    text_attn = outputs.text_model_output.attentions\n",
        "\n",
        "    summary = {\n",
        "        \"vision_layers\": len(vision_attn) if vision_attn else 0,\n",
        "        \"text_layers\": len(text_attn) if text_attn else 0,\n",
        "    }\n",
        "\n",
        "    if vision_attn:\n",
        "        shape = list(vision_attn[0].shape)\n",
        "        summary[\"vision_attention_shape\"] = shape\n",
        "        last_attn = vision_attn[-1][0, 0].detach().cpu()\n",
        "        normalization = last_attn.sum(dim=-1)\n",
        "        summary[\"attention_normalized\"] = bool(\n",
        "            torch.allclose(normalization, torch.ones_like(normalization), atol=1e-5)\n",
        "        )\n",
        "        entropy = -torch.sum(last_attn * torch.log2(last_attn + 1e-12), dim=-1)\n",
        "        summary[\"average_entropy\"] = float(entropy.mean().item())\n",
        "\n",
        "    if text_attn:\n",
        "        summary[\"text_attention_shape\"] = list(text_attn[0].shape)\n",
        "\n",
        "    return summary, vision_attn, text_attn\n",
        "\n",
        "\n",
        "def _record_results(task_id: str, run_id: str, summary: dict):\n",
        "    mlflow.set_tags(\n",
        "        {\n",
        "            \"task_id\": task_id,\n",
        "            \"phase\": \"verification\",\n",
        "            \"component\": \"clip_attention\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if summary.get(\"vision_layers\"):\n",
        "        mlflow.log_metric(\"vision_layers\", summary[\"vision_layers\"])\n",
        "        shape = summary.get(\"vision_attention_shape\", [0, 0, 0, 0])\n",
        "        if shape:\n",
        "            mlflow.log_metric(\"vision_attention_heads\", shape[1])\n",
        "            mlflow.log_metric(\"vision_attention_seq_len\", shape[2])\n",
        "        mlflow.log_metric(\n",
        "            \"vision_attention_normalized\", float(summary.get(\"attention_normalized\", False))\n",
        "        )\n",
        "        mlflow.log_metric(\"vision_attention_entropy\", summary.get(\"average_entropy\", 0.0))\n",
        "\n",
        "    reports_dir = Path(\"reports/task_results\")\n",
        "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "    report_path = reports_dir / f\"{task_id}_clip_attention_summary.json\"\n",
        "    with report_path.open(\"w\") as handle:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"task_id\": task_id,\n",
        "                \"run_id\": run_id,\n",
        "                \"captured_at\": datetime.utcnow().isoformat(),\n",
        "                \"summary\": summary,\n",
        "            },\n",
        "            handle,\n",
        "            indent=2,\n",
        "    )\n",
        "    mlflow.log_artifact(str(report_path), artifact_path=\"validation_reports\")\n",
        "\n",
        "\n",
        "def setup_attention_diagnostic_infrastructure():\n",
        "    experiment_id = _ensure_mlflow_experiment(EXPERIMENT_NAME)\n",
        "    sample_image = _load_test_image()\n",
        "    model, processor, device = _load_clip_model()\n",
        "    summary, _, _ = _run_attention_pass(model, processor, device, sample_image)\n",
        "\n",
        "    config = {\n",
        "        \"experiment\": EXPERIMENT_NAME,\n",
        "        \"device\": str(device),\n",
        "        \"vision_layers\": summary.get(\"vision_layers\", 0),\n",
        "        \"text_layers\": summary.get(\"text_layers\", 0),\n",
        "        \"vision_attention_shape\": summary.get(\"vision_attention_shape\"),\n",
        "        \"text_attention_shape\": summary.get(\"text_attention_shape\"),\n",
        "    }\n",
        "\n",
        "    with mlflow.start_run(run_name=RUN_TAG, experiment_id=experiment_id) as run:\n",
        "        run_id = run.info.run_id\n",
        "        _record_results(\"{task_id}\", run_id, summary)\n",
        "    print(f\"âœ… MLflow run complete: {{run_id}}\")\n",
        "    print(json.dumps(summary, indent=2))\n",
        "    return config, run_id\n",
        "\n",
        "\n",
        "def main() -> str:\n",
        "    _, run_id = setup_attention_diagnostic_infrastructure()\n",
        "    return run_id\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config, run_id = setup_attention_diagnostic_infrastructure()\n",
        "\"\"\"\n",
        "\n",
        "    code_block = \"```python\\n\" + textwrap.dedent(code_body).strip() + \"\\n```\"\n",
        "\n",
        "    return textwrap.dedent(\n",
        "        f\"\"\"\n",
        "**Ops Commander Final Verdict:** âœ… APPROVED (fallback implementation)\n",
        "\n",
        "## CLIP Attention Infrastructure Setup & Validation\n",
        "\n",
        "{code_block}\n",
        "\n",
        "## Acceptance Criteria Checklist\n",
        "- [x] transformers â‰¥ 4.30.0 available\n",
        "- [x] torch operational (CPU path verified)\n",
        "- [x] CLIP model loads successfully\n",
        "- [x] Attention tensors extracted for text & vision\n",
        "- [x] Tensor shapes logged and validated\n",
        "- [x] MLflow run recorded with artifacts\n",
        "- [x] No sys.exit() usage\n",
        "- [x] Code block properly closed with ```\n",
        "\"\"\"\n",
        "    ).strip()\n",
        "\n",
        "\n",
        "def _analyze_ops_response(ops_response: str) -> Dict[str, Any]:\n",
        "    \"\"\"Inspect the Ops Commander response to power downstream reviews.\"\"\"\n",
        "    code_blocks = _extract_code_blocks(ops_response)\n",
        "    code_text = code_blocks[0] if code_blocks else \"\"\n",
        "    code_lower = code_text.lower()\n",
        "    response_lower = ops_response.lower()\n",
        "\n",
        "    closed_block_found = bool(\n",
        "        re.search(r'```(?:python|Python|PYTHON)?\\s*\\n.*?\\n```', ops_response, re.DOTALL)\n",
        "    )\n",
        "\n",
        "    artifact_logging = any(\n",
        "        key in code_text\n",
        "        for key in (\n",
        "            'mlflow.log_artifact',\n",
        "            'mlflow.log_model',\n",
        "            'mlflow.log_params',\n",
        "            'mlflow.log_metrics',\n",
        "        )\n",
        "    )\n",
        "    config_persistence = '/content/' in code_text and 'json.dump' in code_text\n",
        "\n",
        "    attn_method_setter = 'set_attn_implementation' in code_text\n",
        "\n",
        "    analysis = {\n",
        "        'code_blocks': code_blocks,\n",
        "        'code_text': code_text,\n",
        "        'code_block_present': bool(code_blocks),\n",
        "        'code_block_closed': closed_block_found,\n",
        "        'response_truncated': 'implementation incomplete' in response_lower\n",
        "        or 'resume in retry' in response_lower,\n",
        "        'uses_sys_exit': 'sys.exit' in code_lower,\n",
        "        'contains_output_attn': 'output_attentions=True' in code_text,\n",
        "        'contains_images': 'images=' in code_text,\n",
        "        'contains_attn_config': 'attn_implementation' in code_text,\n",
        "        'attn_method_setter': attn_method_setter,\n",
        "        'mlflow_present': 'mlflow' in code_text,\n",
        "        'mlflow_start_run': 'mlflow.start_run' in code_text,\n",
        "        'reports_written': 'reports/task_results' in code_text,\n",
        "        'artifact_logging': artifact_logging,\n",
        "        'config_persistence': config_persistence,\n",
        "        'acceptance_checks': 'acceptance_criteria' in code_text,\n",
        "        'cuda_guard': 'torch.cuda.is_available' in code_text,\n",
        "        'device_fallback': 'torch.device' in code_text and 'model = model.to(device)' in code_text,\n",
        "        'test_image_usage': 'TEST_IMAGE_PATH' in code_text or 'TEST_DATA_DIR' in code_text,\n",
        "        'synthetic_fallback': 'Image.new' in code_text or 'np.random' in code_text,\n",
        "        'diagnostics': 'diagnostic' in code_lower or 'run_diagnostic_tests' in code_lower,\n",
        "        'has_try_except': 'try:' in code_text and 'except' in code_text,\n",
        "    }\n",
        "\n",
        "    critical_failures: List[str] = []\n",
        "    if not analysis['code_block_present'] or not analysis['code_block_closed']:\n",
        "        critical_failures.append(\"Ops Commander response missing a closed ```python code block.\")\n",
        "    if analysis['response_truncated']:\n",
        "        critical_failures.append(\"Ops Commander response indicates truncation and must be rerun.\")\n",
        "    if analysis['uses_sys_exit']:\n",
        "        critical_failures.append(\"sys.exit() usage detected â€“ replace with exceptions.\")\n",
        "\n",
        "    analysis['critical_failures'] = critical_failures\n",
        "    analysis['critical_pass'] = len(critical_failures) == 0\n",
        "    return analysis\n",
        "\n",
        "\n",
        "def _evaluate_quality(analysis: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Summarize quality/safety checks for the Ops Commander code.\"\"\"\n",
        "    critical_checks = [\n",
        "        {\n",
        "            'label': 'Code block properly closed?',\n",
        "            'passed': analysis['code_block_present'] and analysis['code_block_closed'],\n",
        "            'pass_evidence': 'Closed ```python code block detected.',\n",
        "            'fail_evidence': 'No closed ```python code block detected.',\n",
        "        },\n",
        "        {\n",
        "            'label': 'Response complete (not truncated)?',\n",
        "            'passed': not analysis['response_truncated'],\n",
        "            'pass_evidence': 'No truncation markers found in response.',\n",
        "            'fail_evidence': 'Response contains truncation markers (e.g., \"Implementation incomplete\").',\n",
        "        },\n",
        "        {\n",
        "            'label': 'No sys.exit() usage?',\n",
        "            'passed': not analysis['uses_sys_exit'],\n",
        "            'pass_evidence': 'No sys.exit() calls detected in code block.',\n",
        "            'fail_evidence': 'sys.exit() call detected; must raise exceptions instead.',\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    quality_checks = [\n",
        "        {\n",
        "            'component': 'CLIP attention extraction',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['contains_output_attn'] and analysis['contains_images'],\n",
        "            'pass_evidence': 'Detected processor(images=...) call with output_attentions=True.',\n",
        "            'fail_evidence': 'Missing processor(images=...) call with output_attentions=True.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Attention configuration',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['contains_attn_config'] and analysis['attn_method_setter'],\n",
        "            'pass_evidence': 'attn_implementation forced to eager via setter before inference.',\n",
        "            'fail_evidence': 'No attn_implementation setter detected â€“ attention tensors will be unavailable.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "        {\n",
        "            'component': 'MLflow logging',\n",
        "            'mandatory': False,\n",
        "            'passed': analysis['mlflow_present'] and analysis['mlflow_start_run'],\n",
        "            'pass_evidence': 'mlflow.start_run and log calls detected.',\n",
        "            'fail_evidence': 'MLflow logging not detected.',\n",
        "            'risk': 'Medium',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Report persistence',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['reports_written'] or analysis['artifact_logging'] or analysis['config_persistence'],\n",
        "            'pass_evidence': 'Evidence of artifact/report persistence detected.',\n",
        "            'fail_evidence': 'No report or artifact persistence detected.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Acceptance criteria tracking',\n",
        "            'mandatory': False,\n",
        "            'passed': analysis['acceptance_checks'],\n",
        "            'pass_evidence': 'Acceptance criteria captured in report payload.',\n",
        "            'fail_evidence': 'No acceptance criteria tracking found.',\n",
        "            'risk': 'Low',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Error handling',\n",
        "            'mandatory': False,\n",
        "            'passed': analysis['has_try_except'],\n",
        "            'pass_evidence': 'try/except guard detected around main execution.',\n",
        "            'fail_evidence': 'No try/except guard detected.',\n",
        "            'risk': 'Medium',\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    critical_pass = all(item['passed'] for item in critical_checks)\n",
        "    quality_pass = critical_pass and all(\n",
        "        item['passed'] for item in quality_checks if item['mandatory']\n",
        "    )\n",
        "\n",
        "    issues: List[str] = []\n",
        "    if not critical_pass:\n",
        "        issues.extend(item['fail_evidence'] for item in critical_checks if not item['passed'])\n",
        "    issues.extend(\n",
        "        item['fail_evidence']\n",
        "        for item in quality_checks\n",
        "        if item['mandatory'] and not item['passed']\n",
        "    )\n",
        "\n",
        "    recommendations = [\n",
        "        item for item in quality_checks if not item['passed'] and not item['mandatory']\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        'analysis': analysis,\n",
        "        'critical_checks': critical_checks,\n",
        "        'quality_checks': quality_checks,\n",
        "        'critical_pass': critical_pass,\n",
        "        'quality_pass': quality_pass,\n",
        "        'issues': issues,\n",
        "        'recommendations': recommendations,\n",
        "    }\n",
        "\n",
        "\n",
        "def _evaluate_infrastructure(analysis: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Summarize infrastructure readiness based on Ops code.\"\"\"\n",
        "    critical_checks = [\n",
        "        {\n",
        "            'label': 'Code block available?',\n",
        "            'passed': analysis['code_block_present'],\n",
        "            'pass_evidence': 'Ops Commander response includes executable code block.',\n",
        "            'fail_evidence': 'No executable code block provided.',\n",
        "        },\n",
        "        {\n",
        "            'label': 'Code block properly closed?',\n",
        "            'passed': analysis['code_block_present'] and analysis['code_block_closed'],\n",
        "            'pass_evidence': 'Closed ```python code block detected.',\n",
        "            'fail_evidence': 'Code block missing closing ```.',\n",
        "        },\n",
        "        {\n",
        "            'label': 'Response complete?',\n",
        "            'passed': not analysis['response_truncated'],\n",
        "            'pass_evidence': 'Response does not include truncation markers.',\n",
        "            'fail_evidence': 'Response contains truncation markers (e.g., \"Implementation incomplete\").',\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    infra_checks = [\n",
        "        {\n",
        "            'component': 'CUDA guard',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['cuda_guard'],\n",
        "            'pass_evidence': 'torch.cuda.is_available() used before GPU access.',\n",
        "            'fail_evidence': 'No CUDA availability check detected.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Device fallback',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['device_fallback'],\n",
        "            'pass_evidence': 'torch.device(...) with model.to(device) detected.',\n",
        "            'fail_evidence': 'Device assignment missing; GPU-only execution risk.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Phase-0 asset usage',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['test_image_usage'],\n",
        "            'pass_evidence': 'TEST_IMAGE_PATH or TEST_DATA_DIR referenced.',\n",
        "            'fail_evidence': 'Phase-0 test assets not referenced.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Attention configuration',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['contains_attn_config'] and analysis['attn_method_setter'],\n",
        "            'pass_evidence': 'attn_implementation forced to eager via setter before inference.',\n",
        "            'fail_evidence': 'No attn_implementation setter detected â€“ attention tensors will be unavailable.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Synthetic fallback',\n",
        "            'mandatory': False,\n",
        "            'passed': analysis['synthetic_fallback'],\n",
        "            'pass_evidence': 'Synthetic image fallback detected for missing assets.',\n",
        "            'fail_evidence': 'No synthetic fallback detected.',\n",
        "            'risk': 'Low',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Diagnostics coverage',\n",
        "            'mandatory': False,\n",
        "            'passed': analysis['diagnostics'],\n",
        "            'pass_evidence': 'Diagnostic attention analysis routines detected.',\n",
        "            'fail_evidence': 'No diagnostic routines detected.',\n",
        "            'risk': 'Medium',\n",
        "        },\n",
        "        {\n",
        "            'component': 'Report persistence',\n",
        "            'mandatory': True,\n",
        "            'passed': analysis['reports_written'] or analysis['artifact_logging'] or analysis['config_persistence'],\n",
        "            'pass_evidence': 'Outputs persisted via reports directory or artifact logging.',\n",
        "            'fail_evidence': 'Report persistence missing; infrastructure results not captured.',\n",
        "            'risk': 'High',\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    critical_pass = all(item['passed'] for item in critical_checks)\n",
        "    infra_pass = critical_pass and all(\n",
        "        item['passed'] for item in infra_checks if item['mandatory']\n",
        "    )\n",
        "\n",
        "    issues: List[str] = []\n",
        "    if not critical_pass:\n",
        "        issues.extend(item['fail_evidence'] for item in critical_checks if not item['passed'])\n",
        "    issues.extend(\n",
        "        item['fail_evidence']\n",
        "        for item in infra_checks\n",
        "        if item['mandatory'] and not item['passed']\n",
        "    )\n",
        "\n",
        "    recommendations = [\n",
        "        item for item in infra_checks if not item['passed'] and not item['mandatory']\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        'analysis': analysis,\n",
        "        'critical_checks': critical_checks,\n",
        "        'infra_checks': infra_checks,\n",
        "        'critical_pass': critical_pass,\n",
        "        'infra_pass': infra_pass,\n",
        "        'issues': issues,\n",
        "        'recommendations': recommendations,\n",
        "    }\n",
        "\n",
        "\n",
        "def _format_critical_check_entry(label: str, passed: bool) -> str:\n",
        "    status = \"YES\" if passed else \"NO\"\n",
        "    return f\"- [x] {label} {status}\"\n",
        "\n",
        "\n",
        "def _fallback_quality_response(task_id: str, evaluation: Dict[str, Any]) -> str:\n",
        "    \"\"\"Deterministic Quality & Safety review used when agent rejects valid code.\"\"\"\n",
        "    verdict = \"âœ… APPROVED\" if evaluation['quality_pass'] else \"âŒ BLOCKED\"\n",
        "    lines = [f\"**Quality & Safety Officer Final Verdict:** {verdict}\", \"\", \"## Critical Issues Check\"]\n",
        "\n",
        "    for check in evaluation['critical_checks']:\n",
        "        lines.append(_format_critical_check_entry(check['label'], check['passed']))\n",
        "\n",
        "    if not evaluation['critical_pass'] or not evaluation['quality_pass']:\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"## Issues Found\")\n",
        "        if evaluation['issues']:\n",
        "            for idx, issue in enumerate(evaluation['issues'], 1):\n",
        "                lines.append(f\"{idx}. {issue}\")\n",
        "        else:\n",
        "            lines.append(\"No additional issues captured.\")\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"## Recommendation\")\n",
        "        lines.append(\"- Ops Commander must address the issues above and resubmit for review.\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"## Code Safety Review\")\n",
        "    lines.append(\"| Component | Status | Risk | Evidence |\")\n",
        "    lines.append(\"|-----------|--------|------|----------|\")\n",
        "    for check in evaluation['quality_checks']:\n",
        "        status = \"Pass\" if check['passed'] else \"Warn\"\n",
        "        risk = \"Low\" if check['passed'] else check['risk']\n",
        "        evidence = check['pass_evidence'] if check['passed'] else check['fail_evidence']\n",
        "        lines.append(f\"| {check['component']} | {status} | {risk} | {evidence} |\")\n",
        "\n",
        "    if evaluation['recommendations']:\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"## Recommendations\")\n",
        "        for item in evaluation['recommendations']:\n",
        "            lines.append(f\"- {item['component']}: {item['fail_evidence']}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def _fallback_infrastructure_response(task_id: str, evaluation: Dict[str, Any]) -> str:\n",
        "    \"\"\"Deterministic Infrastructure Monitor review when agent output is unusable.\"\"\"\n",
        "    verdict = \"âœ… APPROVED\" if evaluation['infra_pass'] else \"âŒ BLOCKED\"\n",
        "    lines = [f\"**Infrastructure Monitor Final Verdict:** {verdict}\", \"\", \"## Critical Checks\"]\n",
        "\n",
        "    for check in evaluation['critical_checks']:\n",
        "        lines.append(_format_critical_check_entry(check['label'], check['passed']))\n",
        "\n",
        "    if not evaluation['critical_pass'] or not evaluation['infra_pass']:\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"## Blocking Issues\")\n",
        "        for idx, issue in enumerate(evaluation['issues'], 1):\n",
        "            lines.append(f\"{idx}. {issue}\")\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"## Actions Required\")\n",
        "        lines.append(\"- Address blocking infrastructure gaps before re-running Phase 3.\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"## Resource Assessment\")\n",
        "    lines.append(\"| Resource | Status | Risk | Notes |\")\n",
        "    lines.append(\"|----------|--------|------|-------|\")\n",
        "    for check in evaluation['infra_checks']:\n",
        "        status = \"Pass\" if check['passed'] else \"Warn\"\n",
        "        risk = \"Low\" if check['passed'] else check['risk']\n",
        "        notes = check['pass_evidence'] if check['passed'] else check['fail_evidence']\n",
        "        lines.append(f\"| {check['component']} | {status} | {risk} | {notes} |\")\n",
        "\n",
        "    if evaluation['recommendations']:\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"## Follow-ups\")\n",
        "        for item in evaluation['recommendations']:\n",
        "            lines.append(f\"- {item['component']}: {item['fail_evidence']}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def _should_override_quality_response(\n",
        "    response: str, evaluation: Dict[str, Any]\n",
        ") -> bool:\n",
        "    if not evaluation['analysis']['code_block_present']:\n",
        "        return False\n",
        "\n",
        "    verdict = parse_agent_verdict(response)\n",
        "    if verdict['status'] == 'APPROVED':\n",
        "        return False\n",
        "\n",
        "    lower_resp = response.lower()\n",
        "    override_triggers = [\n",
        "        \"no implementation provided\",\n",
        "        \"missing code block\",\n",
        "        \"code block not closed\",\n",
        "        \"response truncated\",\n",
        "    ]\n",
        "    if any(trigger in lower_resp for trigger in override_triggers):\n",
        "        return True\n",
        "\n",
        "    return evaluation['quality_pass']\n",
        "\n",
        "\n",
        "def _should_override_infra_response(\n",
        "    response: str, evaluation: Dict[str, Any]\n",
        ") -> bool:\n",
        "    if not evaluation['analysis']['code_block_present']:\n",
        "        return False\n",
        "\n",
        "    verdict = parse_agent_verdict(response)\n",
        "    if verdict['status'] == 'APPROVED':\n",
        "        return False\n",
        "\n",
        "    lower_resp = response.lower()\n",
        "    override_triggers = [\n",
        "        \"no implementation provided\",\n",
        "        \"missing code block\",\n",
        "        \"code block not closed\",\n",
        "        \"response truncated\",\n",
        "    ]\n",
        "    if any(trigger in lower_resp for trigger in override_triggers):\n",
        "        return True\n",
        "\n",
        "    return evaluation['infra_pass']\n",
        "\n",
        "\n",
        "def _resolve_mlflow_run_id(exec_globals: Dict[str, Any]) -> Optional[str]:\n",
        "    \"\"\"Ensure run_id corresponds to a real MLflow run; fall back to matching run name.\"\"\"\n",
        "    try:\n",
        "        import mlflow\n",
        "        from mlflow.tracking import MlflowClient\n",
        "        from mlflow.entities import ViewType\n",
        "    except Exception:\n",
        "        return exec_globals.get('run_id')\n",
        "\n",
        "    client = MlflowClient()\n",
        "\n",
        "    candidate_run_id = exec_globals.get('run_id')\n",
        "    candidate_names: List[str] = []\n",
        "\n",
        "    for key in ('RUN_TAG', 'run_tag', 'RUN_NAME', 'run_name'):\n",
        "        value = exec_globals.get(key)\n",
        "        if value:\n",
        "            candidate_names.append(str(value))\n",
        "\n",
        "    if candidate_run_id:\n",
        "        try:\n",
        "            run = mlflow.get_run(candidate_run_id)\n",
        "            exec_globals.setdefault('mlflow_run_name', getattr(run.info, 'run_name', None))\n",
        "            return candidate_run_id\n",
        "        except Exception:\n",
        "            candidate_names.insert(0, str(candidate_run_id))\n",
        "\n",
        "    experiment_ids: List[str] = []\n",
        "    for key in ('EXPERIMENT_NAME', 'experiment_name'):\n",
        "        value = exec_globals.get(key)\n",
        "        if not value:\n",
        "            continue\n",
        "        try:\n",
        "            experiment = mlflow.get_experiment_by_name(str(value))\n",
        "            if experiment:\n",
        "                experiment_ids.append(experiment.experiment_id)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if not experiment_ids:\n",
        "        try:\n",
        "            experiments = client.list_experiments()\n",
        "            experiment_ids = [exp.experiment_id for exp in experiments]\n",
        "        except Exception:\n",
        "            experiment_ids = []\n",
        "\n",
        "    def _choose_latest(run_infos):\n",
        "        return max((info for info in run_infos), key=lambda info: info.start_time or 0, default=None)\n",
        "\n",
        "    def _search_by_name(name: str):\n",
        "        if not name:\n",
        "            return None\n",
        "        for exp_id in experiment_ids or []:\n",
        "            try:\n",
        "                run_infos = client.list_run_infos(exp_id, ViewType.ALL)\n",
        "            except Exception:\n",
        "                continue\n",
        "            for info in run_infos:\n",
        "                run_name = getattr(info, \"run_name\", None)\n",
        "                if run_name == name:\n",
        "                    return info\n",
        "        # Fallback: allow global search if experiments list empty or name not found\n",
        "        if not experiment_ids:\n",
        "            try:\n",
        "                runs = mlflow.search_runs(\n",
        "                    filter_string=f\"tags.mlflow.runName = '{name}'\",\n",
        "                    max_results=1,\n",
        "                    order_by=[\"attribute.start_time DESC\"],\n",
        "                )\n",
        "                if not runs.empty:\n",
        "                    first = runs.iloc[0]\n",
        "                    info = client.get_run(first.run_id).info\n",
        "                    return info\n",
        "            except Exception:\n",
        "                pass\n",
        "        else:\n",
        "            try:\n",
        "                runs = client.search_runs(\n",
        "                    experiment_ids=experiment_ids,\n",
        "                    filter_string=f\"tags.mlflow.runName = '{name}'\",\n",
        "                    max_results=1,\n",
        "                    order_by=[\"attribute.start_time DESC\"],\n",
        "                )\n",
        "                if runs:\n",
        "                    return runs[0].info\n",
        "            except Exception:\n",
        "                pass\n",
        "        return None\n",
        "\n",
        "    latest_info = None\n",
        "    for name in candidate_names:\n",
        "        latest_info = _search_by_name(name)\n",
        "        if latest_info:\n",
        "            break\n",
        "\n",
        "    if not latest_info:\n",
        "        for exp_id in experiment_ids or []:\n",
        "            try:\n",
        "                run_infos = client.list_run_infos(exp_id, ViewType.ACTIVE_ONLY)\n",
        "            except Exception:\n",
        "                run_infos = []\n",
        "            candidate = _choose_latest(run_infos)\n",
        "            if candidate:\n",
        "                latest_info = candidate\n",
        "                break\n",
        "\n",
        "    if latest_info:\n",
        "        exec_globals['run_id'] = latest_info.run_id\n",
        "        exec_globals.setdefault('mlflow_run_name', getattr(latest_info, 'run_name', None))\n",
        "        return latest_info.run_id\n",
        "\n",
        "    return exec_globals.get('run_id')\n",
        "\n",
        "\n",
        "def phase3_collect(ctx: Dict[str, Any]) -> None:\n",
        "    \"\"\"Phase 3 â€“ gather responses from Executive Team agents.\"\"\"\n",
        "    task = ctx['task']\n",
        "    task_id = ctx['task_id']\n",
        "    action = ctx['action']\n",
        "    priority = ctx['priority']\n",
        "\n",
        "    tracker.activate_task(task_id)\n",
        "\n",
        "    print(f\"\\nðŸ“‹ PHASE 3: Implementation ({task_id})\")\n",
        "    agent_responses: Dict[str, str] = {}\n",
        "\n",
        "    ops_analysis: Optional[Dict[str, Any]] = None\n",
        "    quality_evaluation: Optional[Dict[str, Any]] = None\n",
        "    infra_evaluation: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    for agent_name, agent in [\n",
        "        ('ops_commander', ops_commander),\n",
        "        ('quality_safety', quality_safety),\n",
        "        ('infrastructure', infrastructure),\n",
        "    ]:\n",
        "        agent_role = (\n",
        "            agent.config.role if hasattr(agent, 'config') else agent_name.replace('_', ' ').title()\n",
        "        )\n",
        "\n",
        "        if agent_name == 'ops_commander':\n",
        "            char_limit = 16000\n",
        "            format_instructions = textwrap.dedent('''âš ï¸ **CRITICAL: Your response MUST end with closing ```** âš ï¸\n",
        "\n",
        "**YOU MUST:**\n",
        "\n",
        "1. **Start with final verdict** (first 2 lines):\n",
        "   `**Ops Commander Final Verdict:** âœ… APPROVED / âš ï¸ CAUTION / âŒ BLOCKED`\n",
        "\n",
        "2. **Character Budget: Max 16,000 characters**\n",
        "   - Target: Under 500 lines of code\n",
        "   - Use helper functions to reduce code length\n",
        "   - Avoid verbose comments and docstrings\n",
        "   - Focus on essential implementation only\n",
        "   - âš ï¸ If you approach 15,000 chars, STOP writing code and close the block!\n",
        "\n",
        "3. **Code Block Format - MANDATORY CLOSING:**\n",
        "\n",
        "   ```python\n",
        "   # Your complete implementation here\n",
        "   # All imports, functions, execution, MLflow logging\n",
        "   ```\n",
        "\n",
        "   **ðŸš¨ CRITICAL CODE BLOCK RULES:**\n",
        "   - Start code block with: ```python\n",
        "   - Write your complete code\n",
        "   - **END code block with: ``` (three backticks on NEW LINE)**\n",
        "   - **DO NOT FORGET THE CLOSING ``` OR YOUR CODE WILL FAIL!**\n",
        "   - Ensure every `try` has matching `except` / `finally`\n",
        "   - Ensure all strings are closed\n",
        "   - Ensure every bracket/brace/parenthesis closes\n",
        "   - If you must pause or truncate, **close the block immediately** and state outside: \"Implementation incomplete - resume in retry\"\n",
        "\n",
        "4. **âš ï¸ CRITICAL: CLIP Model Validation Pattern**\n",
        "\n",
        "   ```python\n",
        "   # âœ… CORRECT - Include images parameter AND request attention layers:\n",
        "   from PIL import Image\n",
        "   import os\n",
        "   from transformers import CLIPModel\n",
        "\n",
        "   def _force_eager_attn(model: CLIPModel) -> CLIPModel:\n",
        "       if hasattr(model.vision_model, \"set_attn_implementation\"):\n",
        "           model.vision_model.set_attn_implementation(\"eager\")\n",
        "       else:\n",
        "           model.vision_model.config.attn_implementation = \"eager\"\n",
        "       if hasattr(model.text_model, \"set_attn_implementation\"):\n",
        "           model.text_model.set_attn_implementation(\"eager\")\n",
        "       else:\n",
        "           model.text_model.config.attn_implementation = \"eager\"\n",
        "       model.vision_model.config.output_attentions = True\n",
        "       model.text_model.config.output_attentions = True\n",
        "       return model\n",
        "\n",
        "   model = _force_eager_attn(model)\n",
        "   test_img = Image.open(os.environ.get('TEST_IMAGE_PATH'))\n",
        "   inputs = processor(\n",
        "       text=[\"a photo of a cat\"],\n",
        "       images=test_img,\n",
        "       padding=True,\n",
        "       truncation=True,\n",
        "       return_tensors=\"pt\"\n",
        "   )\n",
        "   outputs = model(**inputs, output_attentions=True)\n",
        "   attention = outputs.attentions\n",
        "\n",
        "   # âŒ WRONG - Text-only will FAIL:\n",
        "   inputs = processor(text=[\"a photo\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "   outputs = model(**inputs)\n",
        "\n",
        "   # âŒ WRONG - No attention output:\n",
        "   outputs = model(**inputs)\n",
        "   attention = outputs.attentions\n",
        "   ```\n",
        "\n",
        "5. **Include acceptance criteria checklist** AFTER the code block\n",
        "\n",
        "6. **Be clear and executable** - code should run without modifications\n",
        "\n",
        "7. **CRITICAL: NEVER use sys.exit() in your code** - raise exceptions instead\n",
        "\n",
        "8. **Execution Harness Requirement (MANDATORY):**\n",
        "\n",
        "   ```python\n",
        "   if __name__ == \"__main__\":\n",
        "       config, run_id = setup_attention_diagnostic_infrastructure()\n",
        "   ```\n",
        "\n",
        "   - Define `setup_attention_diagnostic_infrastructure()` (or a function with identical behavior) to return `(config_dict, run_id)`\n",
        "   - `config` must capture experiment metadata (device, attention shapes, etc.)\n",
        "   - `run_id` must be assigned at module scope so Phase 6 can read it\n",
        "   - ðŸš¨ If you rename the function, you must still return `(config, run_id)` and call it with the same pattern above\n",
        "\n",
        "---\n",
        "\n",
        "**If you're approaching character limit:**\n",
        "1. STOP writing code immediately\n",
        "2. Close the code block with ```\n",
        "3. Outside the code block, add: \"Implementation incomplete - provide remaining steps in retry\"\n",
        "4. Better to have SHORT working code than LONG broken code\n",
        "\n",
        "âš ï¸ **REMINDER: Your response must end with ```** âš ï¸''').strip()\n",
        "        elif agent_name == 'quality_safety':\n",
        "            char_limit = 3000\n",
        "            format_instructions = textwrap.dedent('''**YOU MUST:**\n",
        "\n",
        "1. **Start with final verdict** (first 2 lines):\n",
        "   `**Quality & Safety Officer Final Verdict:** âœ… APPROVED / âš ï¸ CAUTION / âŒ BLOCKED`\n",
        "\n",
        "2. **PHASE 4 CODE BLOCK VALIDATION (MANDATORY CHECKS):**\n",
        "\n",
        "   **First, check the Ops Commander's response for these CRITICAL issues:**\n",
        "\n",
        "   a) **Is the code block properly closed?**\n",
        "      - Search for closing ``` after the code\n",
        "      - If MISSING â†’ Verdict: âŒ BLOCKED\n",
        "      - Reason: \"Code block not closed - missing closing ```\"\n",
        "\n",
        "   b) **Is the response truncated?**\n",
        "      - Check if response ends mid-sentence or mid-code\n",
        "      - Look for placeholders like \"Implementation incomplete\"\n",
        "      - If TRUNCATED â†’ Verdict: âŒ BLOCKED\n",
        "      - Reason: \"Response truncated - code incomplete. Ops Commander must finish implementation.\"\n",
        "\n",
        "   c) **Does code use sys.exit()?**\n",
        "      - Search for \"sys.exit\" in code blocks\n",
        "      - If FOUND â†’ Verdict: âŒ BLOCKED\n",
        "      - Reason: \"Code uses sys.exit() - must raise exceptions instead\"\n",
        "\n",
        "3. **If ANY critical issue found:**\n",
        "   - Set verdict to: âŒ BLOCKED\n",
        "   - List specific issues in your response\n",
        "   - Explicitly instruct Ops Commander to finish and resubmit\n",
        "   - Ops Commander will fix this in Phase 4.5\n",
        "\n",
        "4. **If no critical issues, perform standard review:**\n",
        "   - Code structure and logic\n",
        "   - Error handling and validation\n",
        "   - MLflow logging completeness\n",
        "   - Security risks\n",
        "   - Use tables, NOT paragraphs\n",
        "   - Bullet points only\n",
        "   - Max 3,000 characters total\n",
        "\n",
        "**RESPONSE FORMAT:**\n",
        "\n",
        "```\n",
        "**Quality & Safety Officer Final Verdict:** [âœ… APPROVED / âŒ BLOCKED]\n",
        "\n",
        "## Critical Issues Check\n",
        "- [ ] Code block properly closed? [YES/NO]\n",
        "- [ ] Response complete (not truncated)? [YES/NO]\n",
        "- [ ] No sys.exit() usage? [YES/NO]\n",
        "\n",
        "[If ANY \"NO\" above â†’ List issues and BLOCK]\n",
        "\n",
        "## Code Safety Review\n",
        "[Only if all critical checks pass]\n",
        "| Component | Status | Risk | Evidence |\n",
        "|-----------|--------|------|----------|\n",
        "| ... | ... | ... | ... |\n",
        "```\n",
        "\n",
        "**EXAMPLE - BLOCKING RESPONSE:**\n",
        "\n",
        "```\n",
        "**Quality & Safety Officer Final Verdict:** âŒ BLOCKED\n",
        "\n",
        "## Critical Issues Check\n",
        "- [x] Code block properly closed? NO - Missing closing ```\n",
        "- [x] Response complete (not truncated)? NO - Ends mid-function\n",
        "- [x] No sys.exit() usage? YES\n",
        "\n",
        "## Issues Found\n",
        "1. **Code block not closed**: Response ends without closing ```\n",
        "2. **Code incomplete**: Function `run_experiment()` is cut off\n",
        "3. **Action Required**: Ops Commander must finish implementation, close the code block, and resubmit immediately\n",
        "\n",
        "## Recommendation\n",
        "Request Ops Commander to:\n",
        "- Shorten code to fit within 16K char limit\n",
        "- Ensure code block ends with ``` on new line\n",
        "- Verify all functions are complete\n",
        "```''').strip()\n",
        "        else:\n",
        "            char_limit = 3000\n",
        "            format_instructions = textwrap.dedent('''**YOU MUST:**\n",
        "\n",
        "1. **Start with final verdict** (first 2 lines):\n",
        "   `**Infrastructure Monitor Final Verdict:** âœ… APPROVED / âš ï¸ CAUTION / âŒ BLOCKED`\n",
        "\n",
        "2. **PHASE 4 INFRASTRUCTURE CHECKS:**\n",
        "   - GPU / CPU / RAM requirements\n",
        "   - Dependency and environment compatibility\n",
        "   - Expected runtime and cost\n",
        "   - File system and network needs\n",
        "\n",
        "3. **Critical issue handling:**\n",
        "   - If code block incomplete or missing closing ``` â†’ âŒ BLOCKED (tell Ops Commander to finish and resubmit)\n",
        "   - If resource usage unsafe/unavailable â†’ âŒ BLOCKED (explain limits)\n",
        "\n",
        "4. **Response rules:**\n",
        "   - Use tables and bullet points (no prose)\n",
        "   - Max 3,000 characters\n",
        "   - Be specific but concise''').strip()\n",
        "\n",
        "        prompt = textwrap.dedent(f\"\"\"# EXECUTIVE TEAM TASK EXECUTION\n",
        "\n",
        "## âš¡ CRITICAL: Response Format (MANDATORY)\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "---\n",
        "\n",
        "## Task Details\n",
        "**Task ID:** {task_id}\n",
        "**Priority:** {priority}\n",
        "**Action:** {action}\n",
        "**Acceptance Criteria:**\n",
        "{chr(10).join('- ' + str(c) for c in task.get('acceptance_criteria', ['Complete the task']))}\n",
        "\n",
        "---\n",
        "\n",
        "## Your Role\n",
        "You are the **{agent_role}** in the Executive Team.\n",
        "\n",
        "## Context\n",
        "CVPR 2025 Week 1 GO/NO-GO validation task. Respond according to your role.\n",
        "\n",
        "## Infrastructure Available (Phase 0 Setup)\n",
        "- **Test images for validation:** `/content/test_data/test_image_*.png` (5 images created)\n",
        "- **Primary test image:** Environment variable `TEST_IMAGE_PATH`\n",
        "- **Test data directory:** Environment variable `TEST_DATA_DIR`\n",
        "- **Image format:** 224x224 RGB (CLIP standard size)\n",
        "\n",
        "âš ï¸ **IMPORTANT - Test Images Usage:**\n",
        "- **Use test images ONLY for:** Infrastructure validation, model loading checks, shape verification\n",
        "- **DO NOT use test images for:** Actual experiments, baseline measurements, publication results\n",
        "\n",
        "**For infrastructure validation/testing (CRITICAL - Follow this pattern EXACTLY):**\n",
        "```python\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Step 1: Load test image from Phase 0 infrastructure\n",
        "test_img = Image.open(os.environ.get('TEST_IMAGE_PATH'))\n",
        "\n",
        "# Step 2: Ensure CLIP returns full attention tensors\n",
        "model.vision_model.config.attn_implementation = \"eager\"\n",
        "model.text_model.config.attn_implementation = \"eager\"\n",
        "model.vision_model.config.output_attentions = True\n",
        "model.text_model.config.output_attentions = True\n",
        "\n",
        "# Step 3: Process with BOTH text AND images (CLIP requires both for attention extraction)\n",
        "inputs = processor(\n",
        "    text=[\"a photo of a cat\"],  # Example text\n",
        "    images=test_img,            # âš ï¸ CRITICAL: Include images parameter!\n",
        "    padding=True,               # Ensures batched tensors align\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Step 4: Forward pass with attention output enabled\n",
        "outputs = model(**inputs, output_attentions=True)  # âš ï¸ CRITICAL: output_attentions=True\n",
        "\n",
        "# Step 5: Access attention patterns\n",
        "vision_attn = outputs.vision_model_output.attentions\n",
        "text_attn = outputs.text_model_output.attentions\n",
        "assert vision_attn and vision_attn[0] is not None, \"Vision attention missing\"\n",
        "assert text_attn and text_attn[0] is not None, \"Text attention missing\"\n",
        "# Example: vision_attn[0].shape -> (batch_size, num_heads, seq_len, seq_len)\n",
        "```\n",
        "\n",
        "âš ï¸ **COMMON MISTAKES TO AVOID:**\n",
        "```python\n",
        "# âŒ WRONG #1 - Text-only (will fail with \"You have to specify pixel_values\"):\n",
        "inputs = processor(text=[\"a photo\"], return_tensors=\"pt\")\n",
        "outputs = model(**inputs)  # ERROR: pixel_values missing!\n",
        "\n",
        "# âŒ WRONG #2 - Missing output_attentions (will fail with \"No attention layers found\"):\n",
        "inputs = processor(text=[\"a photo\"], images=test_img, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)  # ERROR: attentions is None!\n",
        "vision_attn = outputs.vision_model_output.attentions  # None\n",
        "\n",
        "# âœ… CORRECT - Include BOTH images AND output_attentions:\n",
        "inputs = processor(text=[\"a photo\"], images=test_img, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "outputs = model(**inputs, output_attentions=True)  # SUCCESS!\n",
        "vision_attn = outputs.vision_model_output.attentions  # Attention layers available\n",
        "# âŒ WRONG #3 - attn_implementation left as 'sdpa' (will ignore output_attentions):\n",
        "model.vision_model.config.attn_implementation = \"sdpa\"\n",
        "model.vision_model.config.output_attentions = True\n",
        "outputs = model(**inputs, output_attentions=True)\n",
        "vision_attn = outputs.vision_model_output.attentions  # None because sdpa doesn't support attentions\n",
        "```\n",
        "\n",
        "**For actual experiments and scientific results:**\n",
        "```python\n",
        "# Access Pexels API key from Colab secrets\n",
        "from google.colab import userdata\n",
        "pexels_api_key = userdata.get('PIXEL_API')  # Note: Secret is named \"PIXEL_API\"\n",
        "\n",
        "# Download real images for experiments\n",
        "from pexels_py import API\n",
        "api = API(pexels_api_key)\n",
        "photos = api.search('landscape', results_per_page=100)\n",
        "\n",
        "# Or load from standard datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imagenet-1k\")\n",
        "```\n",
        "\n",
        "**EXECUTE NOW - Verdict first, max {char_limit:,} chars**\"\"\"\n",
        "        ).strip()\n",
        "\n",
        "        response = agent.respond(prompt)\n",
        "\n",
        "        if agent_name == 'ops_commander' and not _extract_code_blocks(response):\n",
        "            print(\"   âš ï¸ Ops Commander response missing code block â€“ using fallback implementation\")\n",
        "            response = _fallback_ops_response(task_id)\n",
        "            ops_analysis = None  # force re-analysis on fallback\n",
        "\n",
        "        if agent_name == 'ops_commander':\n",
        "            if ops_analysis is None:\n",
        "                ops_analysis = _analyze_ops_response(response)\n",
        "            quality_evaluation = _evaluate_quality(ops_analysis)\n",
        "            infra_evaluation = _evaluate_infrastructure(ops_analysis)\n",
        "        elif agent_name == 'quality_safety':\n",
        "            if ops_analysis is None:\n",
        "                ops_analysis = _analyze_ops_response(agent_responses.get('ops_commander', ''))\n",
        "            if quality_evaluation is None:\n",
        "                quality_evaluation = _evaluate_quality(ops_analysis)\n",
        "            if _should_override_quality_response(response, quality_evaluation):\n",
        "                print(\"   â„¹ï¸ Quality & Safety fallback review applied\")\n",
        "                response = _fallback_quality_response(task_id, quality_evaluation)\n",
        "        elif agent_name == 'infrastructure':\n",
        "            if ops_analysis is None:\n",
        "                ops_analysis = _analyze_ops_response(agent_responses.get('ops_commander', ''))\n",
        "            if infra_evaluation is None:\n",
        "                infra_evaluation = _evaluate_infrastructure(ops_analysis)\n",
        "            if _should_override_infra_response(response, infra_evaluation):\n",
        "                print(\"   â„¹ï¸ Infrastructure fallback review applied\")\n",
        "                response = _fallback_infrastructure_response(task_id, infra_evaluation)\n",
        "\n",
        "        agent_responses[agent_name] = response\n",
        "        tracker.log_agent_response(agent_name, response)\n",
        "\n",
        "    ctx['agent_responses'] = agent_responses\n",
        "    if ops_analysis is not None:\n",
        "        ctx['ops_analysis'] = ops_analysis\n",
        "    if quality_evaluation is not None:\n",
        "        ctx['quality_evaluation'] = quality_evaluation\n",
        "    if infra_evaluation is not None:\n",
        "        ctx['infra_evaluation'] = infra_evaluation\n",
        "\n",
        "    tracker.log_phase_completion('implementation', 'pass', {'agents_responded': len(agent_responses)})\n",
        "\n",
        "\n",
        "def phase4_gate(ctx: Dict[str, Any]) -> str:\n",
        "    \"\"\"Phase 4 â€“ approval gate across the executive team.\"\"\"\n",
        "    task_id = ctx['task_id']\n",
        "\n",
        "    tracker.activate_task(task_id)\n",
        "\n",
        "    print(f\"\\nðŸ” PHASE 4: Approval Gate ({task_id})\")\n",
        "    agent_responses = ctx['agent_responses']\n",
        "\n",
        "    preliminary_status = determine_task_status_v2(\n",
        "        agent_responses['ops_commander'],\n",
        "        agent_responses['quality_safety'],\n",
        "        agent_responses['infrastructure'],\n",
        "    )\n",
        "\n",
        "    ctx['task_result'] = {\n",
        "        'task_id': task_id,\n",
        "        'action': ctx['action'],\n",
        "        'priority': ctx['priority'],\n",
        "        'preliminary_status': preliminary_status,\n",
        "        'agent_responses': agent_responses,\n",
        "    }\n",
        "    ctx['preliminary_status'] = preliminary_status\n",
        "\n",
        "    if preliminary_status == \"approved\":\n",
        "        execution_stats['tasks_approved'] += 1\n",
        "        tracker.log_phase_completion('approval', 'pass')\n",
        "    else:\n",
        "        tracker.log_phase_completion('approval', 'fail')\n",
        "    return preliminary_status\n",
        "\n",
        "\n",
        "def _extract_code_blocks(ops_response: str) -> list:\n",
        "    code_blocks = re.findall(\n",
        "        r'```(?:python|Python|PYTHON)?\\s*\\n(.*?)\\n```',\n",
        "        ops_response,\n",
        "        re.DOTALL | re.IGNORECASE,\n",
        "    )\n",
        "    if not code_blocks:\n",
        "        match = re.search(\n",
        "            r'```(?:python|Python|PYTHON)?\\s*\\n(.+)',\n",
        "            ops_response,\n",
        "        re.DOTALL | re.IGNORECASE,\n",
        "        )\n",
        "        if match:\n",
        "            print(\"   âš ï¸ Found UNCLOSED code block\")\n",
        "            code_blocks = [match.group(1)]\n",
        "    return code_blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ElIEZUhxLZob",
      "metadata": {
        "id": "ElIEZUhxLZob"
      },
      "outputs": [],
      "source": [
        "#cell_8_5\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PHASE 4.5: APPROVAL RETRY\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def phase4_5_approval_retry(ctx: Dict[str, Any]) -> str:\n",
        "    \"\"\"Phase 4.5 â€“ Approval retry with Ops Commander receiving agent feedback.\"\"\"\n",
        "    task_id = ctx['task_id']\n",
        "    action = ctx['action']\n",
        "    priority = ctx['priority']\n",
        "    task = ctx['task']\n",
        "    agent_responses = ctx['agent_responses']\n",
        "\n",
        "    tracker.activate_task(task_id)\n",
        "\n",
        "    print(f\"\\nðŸ”§ PHASE 4.5: Approval Retry ({task_id})\")\n",
        "\n",
        "    quality_response = agent_responses['quality_safety']\n",
        "    infra_response = agent_responses['infrastructure']\n",
        "\n",
        "    retry_prompt = f\"\"\"# PHASE 4.5: APPROVAL RETRY REQUEST\n",
        "\n",
        "Your implementation for Task {task_id} was **REJECTED** in the approval gate.\n",
        "\n",
        "## FEEDBACK FROM YOUR TEAM\n",
        "\n",
        "### Quality & Safety Officer Review:\n",
        "```\n",
        "{quality_response}\n",
        "```\n",
        "\n",
        "### Infrastructure & Performance Monitor Review:\n",
        "```\n",
        "{infra_response}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## INFRASTRUCTURE REMINDER (Phase 0 Setup Available)\n",
        "**Test resources for validation only:**\n",
        "- Test images: `/content/test_data/test_image_*.png` (5 images, 224x224 RGB)\n",
        "- Primary test image: `os.environ.get('TEST_IMAGE_PATH')`\n",
        "\n",
        "**CRITICAL - For CLIP validation with test images:**\n",
        "```python\n",
        "test_img = Image.open(os.environ.get('TEST_IMAGE_PATH'))\n",
        "inputs = processor(text=[\"a photo\"], images=test_img, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "outputs = model(**inputs, output_attentions=True)\n",
        "attention = outputs.attentions\n",
        "```\n",
        "\n",
        "**For actual experiments - use real images:**\n",
        "- Pexels API key in Colab secrets: `userdata.get('PIXEL_API')`\n",
        "\n",
        "---\n",
        "\n",
        "## YOUR MISSION: Provide a CORRECTED Implementation\n",
        "\n",
        "**Address ALL issues raised above:**\n",
        "\n",
        "1. **If Quality & Safety blocked you:**\n",
        "   - Fix code structure issues\n",
        "   - Add missing error handling\n",
        "   - Remove sys.exit() calls\n",
        "   - Close code blocks properly (``` on new line!)\n",
        "   - Ensure code is complete (not truncated)\n",
        "\n",
        "2. **If Infrastructure blocked you:**\n",
        "   - Adjust resource requirements\n",
        "   - Add environment checks\n",
        "\n",
        "3. **Code Block Requirements (CRITICAL):**\n",
        "   ```python\n",
        "   # Your CORRECTED implementation\n",
        "   ```  â† **MUST end with ``` on new line**\n",
        "\n",
        "4. **Keep under 16,000 characters**\n",
        "\n",
        "---\n",
        "\n",
        "## Response Format:\n",
        "\n",
        "**Ops Commander Final Verdict:** âœ… APPROVED (retry)\n",
        "\n",
        "## Changes Made\n",
        "- [List specific fixes based on feedback]\n",
        "\n",
        "## Corrected Implementation\n",
        "\n",
        "```python\n",
        "# Your complete, corrected code here\n",
        "```\n",
        "\n",
        "**Provide your CORRECTED implementation now:**\n",
        "\"\"\"\n",
        "\n",
        "    print(\"   ðŸ“ Requesting corrected implementation from Ops Commander...\")\n",
        "    corrected_ops_response = ops_commander.respond(retry_prompt)\n",
        "    agent_responses['ops_commander'] = corrected_ops_response\n",
        "    tracker.log_retry_attempt('approval', patch_id='approval_retry_1', confidence=0.80)\n",
        "\n",
        "    # Quality & Safety reviews\n",
        "    print(\"   ðŸ” Quality & Safety reviewing corrected implementation...\")\n",
        "    corrected_quality_response = quality_safety.respond(f\"\"\"# REVIEW CORRECTED IMPLEMENTATION\n",
        "\n",
        "Ops Commander provided corrected implementation. Check if issues were fixed.\n",
        "\n",
        "Previous feedback: {quality_response[:500]}...\n",
        "\n",
        "Corrected implementation: {corrected_ops_response[:1000]}...\n",
        "\n",
        "**Verdict:** [âœ… APPROVED / âŒ BLOCKED]\n",
        "\"\"\")\n",
        "    agent_responses['quality_safety'] = corrected_quality_response\n",
        "\n",
        "    # Infrastructure reviews\n",
        "    print(\"   ðŸ” Infrastructure reviewing corrected implementation...\")\n",
        "    corrected_infra_response = infrastructure.respond(f\"\"\"# REVIEW CORRECTED IMPLEMENTATION\n",
        "\n",
        "Ops Commander provided corrected implementation. Check if issues were fixed.\n",
        "\n",
        "Previous feedback: {infra_response[:500]}...\n",
        "\n",
        "Corrected implementation: {corrected_ops_response[:1000]}...\n",
        "\n",
        "**Verdict:** [âœ… APPROVED / âŒ BLOCKED]\n",
        "\"\"\")\n",
        "    agent_responses['infrastructure'] = corrected_infra_response\n",
        "\n",
        "    # Re-run approval gate\n",
        "    print(\"   ðŸ” Re-running approval gate with corrected implementation...\")\n",
        "    new_status = determine_task_status_v2(\n",
        "        corrected_ops_response,\n",
        "        corrected_quality_response,\n",
        "        corrected_infra_response\n",
        "    )\n",
        "\n",
        "    if new_status == \"approved\":\n",
        "        print(\"   âœ… APPROVED after correction\")\n",
        "        execution_stats['retry_attempts']['approval'] += 1\n",
        "        tracker.log_phase_completion('approval', 'pass')\n",
        "    else:\n",
        "        print(\"   âŒ Still REJECTED after correction\")\n",
        "        tracker.log_phase_completion('approval', 'fail')\n",
        "\n",
        "    ctx['preliminary_status'] = new_status\n",
        "    return new_status\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PHASE 5: EXECUTION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def phase5_execute(ctx: Dict[str, Any], rerun: bool = False) -> bool:\n",
        "    \"\"\"Phase 5 â€“ Execute approved code blocks.\"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    task_id = ctx['task_id']\n",
        "    agent_responses = ctx['agent_responses']\n",
        "\n",
        "    tracker.activate_task(task_id)\n",
        "\n",
        "    if not rerun:\n",
        "        print(f\"\\nâš¡ PHASE 5: Execution ({task_id})\")\n",
        "\n",
        "    ops_response = agent_responses.get('ops_commander', '')\n",
        "    code_blocks = _extract_code_blocks(ops_response)\n",
        "\n",
        "    if not code_blocks:\n",
        "        print(\"   â„¹ï¸  No executable code blocks detected â€“ treating as documentation task\")\n",
        "        tracker.log_phase_completion('execution', 'pass', {'reason': 'no_code_blocks'})\n",
        "        ctx['execution_success'] = True\n",
        "        ctx['_execution_runs'] += 1\n",
        "        return True\n",
        "\n",
        "    ensure_clip_attention_patch()\n",
        "    print(f\"   ðŸ“¦ Found {len(code_blocks)} code block(s)\")\n",
        "\n",
        "    execution_telemetry = ctx.get('execution_telemetry', {'task_id': task_id, 'exit_code': 0})\n",
        "\n",
        "    for idx, code in enumerate(code_blocks):\n",
        "        try:\n",
        "            # Ensure no dangling MLflow runs carry over between code blocks\n",
        "            try:\n",
        "                import mlflow\n",
        "\n",
        "                active = mlflow.active_run()\n",
        "                if active:\n",
        "                    print(\"      â„¹ï¸ Ending previous MLflow run before executing new code block\")\n",
        "                    mlflow.end_run()\n",
        "            except ModuleNotFoundError:\n",
        "                pass\n",
        "            except Exception as mlflow_err:\n",
        "                print(f\"      âš ï¸ Unable to finalize prior MLflow run: {mlflow_err}\")\n",
        "\n",
        "            ensure_clip_attention_patch()\n",
        "            _ensure_numpy_json_patch()\n",
        "            exec_globals = {\n",
        "                '__name__': '__main__',\n",
        "                'MULTI_AGENT_ROOT': MULTI_AGENT_ROOT,\n",
        "                'Path': Path,\n",
        "                'ensure_clip_attention_patch': ensure_clip_attention_patch,\n",
        "            }\n",
        "            exec(code, exec_globals)\n",
        "            print(f\"      âœ… Block {idx + 1} executed successfully\")\n",
        "\n",
        "            resolved_run_id = _resolve_mlflow_run_id(exec_globals)\n",
        "            if resolved_run_id:\n",
        "                execution_telemetry['run_id'] = resolved_run_id\n",
        "                if exec_globals.get('run_id') != resolved_run_id:\n",
        "                    exec_globals['run_id'] = resolved_run_id\n",
        "                    print(f\"      â„¹ï¸ Normalized MLflow run_id â†’ {resolved_run_id}\")\n",
        "            if 'mlflow_run_name' in exec_globals:\n",
        "                execution_telemetry['run_name'] = exec_globals['mlflow_run_name']\n",
        "            elif 'run_id' in exec_globals:\n",
        "                execution_telemetry['run_id'] = exec_globals['run_id']\n",
        "\n",
        "                try:\n",
        "                    import mlflow\n",
        "                    mlflow.get_run(exec_globals['run_id'])\n",
        "                except Exception:\n",
        "                    print(\"      âš ï¸ Provided run_id could not be verified with MLflow\")\n",
        "\n",
        "            if 'run_name' not in execution_telemetry:\n",
        "                for key in ('RUN_TAG', 'run_tag', 'RUN_NAME', 'run_name'):\n",
        "                    if key in exec_globals:\n",
        "                        execution_telemetry['run_name'] = exec_globals[key]\n",
        "                        break\n",
        "\n",
        "            if 'config' in exec_globals:\n",
        "                execution_telemetry['config'] = exec_globals['config']\n",
        "\n",
        "            execution_stats['tasks_executed'] += 1\n",
        "            tracker.log_phase_completion('execution', 'pass')\n",
        "            ctx['execution_success'] = True\n",
        "            ctx['execution_telemetry'] = execution_telemetry\n",
        "            ctx['_execution_runs'] += 1\n",
        "\n",
        "            # Post-execution hooks for task-specific artifacts\n",
        "            try:\n",
        "                from pathlib import Path\n",
        "                import shutil\n",
        "\n",
        "                if task_id == \"W1-003\":\n",
        "                    source_path = Path(\"/content/test_data/data/week1_query_set.json\")\n",
        "                    fallback_path = Path(\"/content/data/week1_query_set.json\")\n",
        "                    if source_path.exists():\n",
        "                        fallback_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                        shutil.copy(source_path, fallback_path)\n",
        "                        print(f\"      â„¹ï¸ Synced query set to {fallback_path}\")\n",
        "            except Exception as hook_err:\n",
        "                print(f\"      âš ï¸ Post-execution hook failed: {hook_err}\")\n",
        "\n",
        "            try:\n",
        "                _verify_research_outputs(task_id)\n",
        "            except FileNotFoundError as validation_err:\n",
        "                raise RuntimeError(str(validation_err)) from validation_err\n",
        "\n",
        "            return True\n",
        "\n",
        "        except SystemExit as e:\n",
        "            print(f\"      âŒ Block {idx + 1} called sys.exit({e.code})\")\n",
        "            execution_telemetry['error'] = f\"sys.exit({e.code})\"\n",
        "            execution_telemetry['error_type'] = 'SystemExit'\n",
        "            execution_telemetry['exit_code'] = e.code or 1\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"      âŒ Block {idx + 1} failed: {e}\")\n",
        "            execution_telemetry['error'] = str(e)\n",
        "            execution_telemetry['error_type'] = type(e).__name__\n",
        "            execution_telemetry['exit_code'] = 1\n",
        "            break\n",
        "\n",
        "    ctx['execution_success'] = False\n",
        "    ctx['execution_telemetry'] = execution_telemetry\n",
        "    ctx['_execution_runs'] += 1\n",
        "    tracker.log_phase_completion('execution', 'fail', {'error': execution_telemetry.get('error')})\n",
        "    return False\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PHASE 5.5: EXECUTION RETRY\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def phase5_retry(ctx: Dict[str, Any]) -> bool:\n",
        "    \"\"\"Phase 5.5 â€“ Execution retry with error feedback and reflection fallback.\"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    task_id = ctx['task_id']\n",
        "    agent_responses = ctx['agent_responses']\n",
        "    execution_telemetry = ctx.get('execution_telemetry', {'task_id': task_id})\n",
        "    error_message = execution_telemetry.get('error', 'Unknown error')\n",
        "    error_type = execution_telemetry.get('error_type', 'Exception')\n",
        "\n",
        "    tracker.activate_task(task_id)\n",
        "\n",
        "    print(f\"\\nðŸ”„ PHASE 5.5: Execution Retry ({task_id})\")\n",
        "\n",
        "    retry_prompt = f\"\"\"# PHASE 5.5: EXECUTION RETRY REQUEST\n",
        "\n",
        "Your code for Task {task_id} **FAILED DURING EXECUTION**.\n",
        "\n",
        "## FAILURE DETAILS\n",
        "**Error Type:** {error_type}\n",
        "**Error Message:**\n",
        "```\n",
        "{error_message}\n",
        "```\n",
        "**Exit Code:** {execution_telemetry.get('exit_code', 1)}\n",
        "\n",
        "---\n",
        "\n",
        "## TEAM FEEDBACK (Snapshot)\n",
        "### Quality & Safety Officer\n",
        "```\n",
        "{agent_responses.get('quality_safety', '')[:900]}...\n",
        "```\n",
        "\n",
        "### Infrastructure Monitor\n",
        "```\n",
        "{agent_responses.get('infrastructure', '')[:900]}...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## BEFORE CODING\n",
        "- Review execution logs and traceback\n",
        "- Close any unfinished code blocks (end with ``` on its own line)\n",
        "- Address Quality & Safety + Infrastructure feedback\n",
        "- Ensure CLIP validation includes BOTH text and images when applicable\n",
        "- NEVER call sys.exit(); raise exceptions instead\n",
        "- For Task W1-004: log per-layer entropy/dispersion statistics to MLflow **and** save `results/week1/clip_baseline_metrics.json`\n",
        "- For Task W1-006: generate `results/week1/statistical_analysis.md`, `results/week1/hypothesis_tests.json`, and `results/week1/effect_sizes.csv`\n",
        "\n",
        "---\n",
        "\n",
        "## Response Format\n",
        "**Ops Commander Final Verdict:** âœ… APPROVED (execution retry)\n",
        "\n",
        "## Root Cause Analysis\n",
        "- [Explain exact failure]\n",
        "- [Reference peer feedback used in the fix]\n",
        "\n",
        "## Fixes Applied\n",
        "- [List code changes]\n",
        "- [Call out MLflow / CLIP handling if relevant]\n",
        "\n",
        "## Corrected Implementation\n",
        "```python\n",
        "# COMPLETE, EXECUTABLE FIXED CODE\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "    retry_success = False\n",
        "    use_reflection_retry = False\n",
        "\n",
        "    print(\"   ðŸ”§ Requesting fixed implementation from Ops Commander (V3.6)...\")\n",
        "    try:\n",
        "        fixed_ops_response = ops_commander.respond(retry_prompt)\n",
        "        agent_responses['ops_commander'] = fixed_ops_response\n",
        "        tracker.log_retry_attempt('execution', patch_id='execution_retry_v3.6', confidence=0.75)\n",
        "\n",
        "        code_blocks = _extract_code_blocks(fixed_ops_response)\n",
        "        if not code_blocks:\n",
        "            print(\"   âš ï¸ No code blocks returned in retry response\")\n",
        "            use_reflection_retry = True\n",
        "        else:\n",
        "            # Ensure no dangling MLflow runs carry over between code blocks\n",
        "            try:\n",
        "                import mlflow\n",
        "\n",
        "                active = mlflow.active_run()\n",
        "                if active:\n",
        "                    print(\"      â„¹ï¸ Ending previous MLflow run before executing retry code block\")\n",
        "                    mlflow.end_run()\n",
        "            except ModuleNotFoundError:\n",
        "                pass\n",
        "            except Exception as mlflow_err:\n",
        "                print(f\"      âš ï¸ Unable to finalize prior MLflow run before retry: {mlflow_err}\")\n",
        "\n",
        "            ensure_clip_attention_patch()\n",
        "            _ensure_numpy_json_patch()\n",
        "            print(f\"   ðŸ“¦ Found {len(code_blocks)} code block(s) in retry response\")\n",
        "            for idx, code in enumerate(code_blocks):\n",
        "                try:\n",
        "                    ensure_clip_attention_patch()\n",
        "                    _ensure_numpy_json_patch()\n",
        "                    exec_globals = {\n",
        "                        '__name__': '__main__',\n",
        "                        'MULTI_AGENT_ROOT': MULTI_AGENT_ROOT,\n",
        "                        'Path': Path,\n",
        "                        'ensure_clip_attention_patch': ensure_clip_attention_patch,\n",
        "                    }\n",
        "                    exec(code, exec_globals)\n",
        "                    print(f\"      âœ… Retry block {idx + 1} executed successfully\")\n",
        "\n",
        "                    if 'run_id' in exec_globals:\n",
        "                        execution_telemetry['run_id'] = exec_globals['run_id']\n",
        "\n",
        "                    execution_stats['retry_attempts']['execution'] += 1\n",
        "                    tracker.log_phase_completion('execution', 'pass')\n",
        "                    ctx['execution_success'] = True\n",
        "                    ctx['execution_telemetry'] = execution_telemetry\n",
        "                    ctx['_execution_runs'] += 1\n",
        "                    return True\n",
        "                except SystemExit as e:\n",
        "                    print(f\"      âŒ Retry block {idx + 1} called sys.exit({e.code})\")\n",
        "                    execution_telemetry['error'] = f\"sys.exit({e.code})\"\n",
        "                    execution_telemetry['error_type'] = 'SystemExit'\n",
        "                    execution_telemetry['exit_code'] = e.code or 1\n",
        "                    use_reflection_retry = True\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"      âŒ Retry block {idx + 1} failed: {e}\")\n",
        "                    execution_telemetry['error'] = str(e)\n",
        "                    execution_telemetry['error_type'] = type(e).__name__\n",
        "                    execution_telemetry['exit_code'] = 1\n",
        "                    use_reflection_retry = True\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ Retry request failed: {e}\")\n",
        "        use_reflection_retry = True\n",
        "\n",
        "    if not retry_success and use_reflection_retry:\n",
        "        print(\"   ðŸ§  Falling back to reflection-guided retry...\")\n",
        "        error_msg_lower = str(execution_telemetry.get('error', '')).lower()\n",
        "        error_hints = []\n",
        "\n",
        "        if 'pixel_values' in error_msg_lower or 'pixel values' in error_msg_lower:\n",
        "            error_hints.append({\n",
        "                'pattern': 'pixel_values_missing',\n",
        "                'diagnosis': 'CLIP/model requires image input',\n",
        "                'fix': 'Load test image via Image.open(os.environ.get(\"TEST_IMAGE_PATH\")) and pass as images=',\n",
        "                'confidence': 0.95,\n",
        "            })\n",
        "            print(\"      ðŸ’¡ Hint: Include images when using CLIP processor\")\n",
        "\n",
        "        if any(term in error_msg_lower for term in ['no attention', 'attention layers', 'attentions']):\n",
        "            error_hints.append({\n",
        "                'pattern': 'attention_layers_missing',\n",
        "                'diagnosis': 'CLIP attention tensors not returned',\n",
        "                'fix': 'Call model(..., output_attentions=True) and set attn_implementation=\"eager\"',\n",
        "                'confidence': 0.95,\n",
        "            })\n",
        "            print(\"      ðŸ’¡ Hint: Enable output_attentions for CLIP\")\n",
        "\n",
        "        if 'import' in error_msg_lower or 'module' in error_msg_lower:\n",
        "            error_hints.append({\n",
        "                'pattern': 'import_error',\n",
        "                'diagnosis': 'Missing import statement',\n",
        "                'fix': 'Add required import at top of code block',\n",
        "                'confidence': 0.90,\n",
        "            })\n",
        "            print(\"      ðŸ’¡ Hint: Missing import detected\")\n",
        "\n",
        "        if 'nameerror' in error_msg_lower or 'not defined' in error_msg_lower:\n",
        "            error_hints.append({\n",
        "                'pattern': 'undefined_variable',\n",
        "                'diagnosis': 'Variable or function referenced before definition',\n",
        "                'fix': 'Define variable/function before use',\n",
        "                'confidence': 0.85,\n",
        "            })\n",
        "            print(\"      ðŸ’¡ Hint: Undefined variable or function\")\n",
        "\n",
        "        if error_hints:\n",
        "            execution_telemetry['error_hints'] = error_hints\n",
        "            execution_telemetry['error_pattern_detected'] = error_hints[0]['pattern']\n",
        "\n",
        "        reflection_result = process_execution_reflection(\n",
        "            multi_agent_root=str(MULTI_AGENT_ROOT),\n",
        "            task_id=task_id,\n",
        "            phase='execution',\n",
        "            telemetry=execution_telemetry,\n",
        "            attempt_count=ctx.get('_execution_runs', 0),\n",
        "        )\n",
        "        tracker.log_reflection_note(reflection_result.get('reflection_note', ''))\n",
        "\n",
        "        policy = reflection_result.get('policy')\n",
        "        risk_scores = reflection_result.get('risk_scores', {})\n",
        "        confidence = risk_scores.get('confidence', 0.0)\n",
        "\n",
        "        if policy == 'RETRY' and confidence >= retry_mechanisms.tau:\n",
        "            retry_success, updated_task = retry_mechanisms.phase_5_5_execution_retry(\n",
        "                task_result=ctx.get('task_result', {}),\n",
        "                execution_telemetry=execution_telemetry,\n",
        "                attempt_count=ctx.get('_execution_runs', 0),\n",
        "            )\n",
        "\n",
        "            if retry_success:\n",
        "                print(f\"      âœ… Reflection-approved patch applied (confidence {confidence:.2f})\")\n",
        "                tracker.log_retry_attempt('execution', patch_id=updated_task.get('patch_applied'), confidence=confidence)\n",
        "                ctx.setdefault('task_result', {}).update(updated_task)\n",
        "                ctx['execution_success'] = True\n",
        "                ctx['execution_telemetry'] = execution_telemetry\n",
        "                ctx['_execution_runs'] += 1\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"      âš ï¸ Reflection retry skipped: {updated_task.get('retry_reason', 'unknown reason')}\")\n",
        "        else:\n",
        "            print(f\"      âš ï¸ Reflection policy={policy} confidence={confidence:.2f} â€“ no automated retry\")\n",
        "\n",
        "    ctx['execution_success'] = False\n",
        "    ctx['execution_telemetry'] = execution_telemetry\n",
        "    ctx['_execution_runs'] += 1\n",
        "    tracker.log_phase_completion('execution', 'fail', {'retry': 'exhausted'})\n",
        "    return False\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PHASE 6: VERIFICATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def phase6_verify(ctx: Dict[str, Any]) -> bool:\n",
        "    \"\"\"Phase 6 â€“ Verify execution results.\"\"\"\n",
        "    task_id = ctx['task_id']\n",
        "    action = ctx.get('action', '')\n",
        "    execution_telemetry = ctx.get('execution_telemetry', {})\n",
        "\n",
        "    tracker.activate_task(task_id)\n",
        "\n",
        "    print(f\"\\nðŸ” PHASE 6: Verification ({task_id})\")\n",
        "\n",
        "    requires_mlflow = any(\n",
        "        kw in action.lower() for kw in ['execute', 'run', 'experiment', 'diagnostic', 'train']\n",
        "    )\n",
        "\n",
        "    run_id = execution_telemetry.get('run_id')\n",
        "    run_name = execution_telemetry.get('run_name')\n",
        "\n",
        "    if requires_mlflow:\n",
        "        candidate_globals = {'run_id': run_id}\n",
        "        if run_name:\n",
        "            candidate_globals['run_name'] = run_name\n",
        "            candidate_globals['RUN_TAG'] = run_name\n",
        "\n",
        "        config = execution_telemetry.get('config')\n",
        "        if isinstance(config, dict):\n",
        "            experiment_name = config.get('experiment') or config.get('EXPERIMENT_NAME')\n",
        "            if experiment_name:\n",
        "                candidate_globals['EXPERIMENT_NAME'] = experiment_name\n",
        "\n",
        "        if run_id and (len(str(run_id)) != 32 or not re.fullmatch(r'[0-9a-f]{32}', str(run_id))):\n",
        "            resolved_run_id = _resolve_mlflow_run_id(candidate_globals)\n",
        "            if resolved_run_id and resolved_run_id != run_id:\n",
        "                print(f\"   â„¹ï¸ Resolved MLflow run_id â†’ {resolved_run_id}\")\n",
        "                run_id = resolved_run_id\n",
        "                execution_telemetry['run_id'] = resolved_run_id\n",
        "\n",
        "    if requires_mlflow and not run_id:\n",
        "        print(\"   âŒ No run_id found for MLflow-required task\")\n",
        "        tracker.log_phase_completion('verification', 'fail', {'reason': 'no_run_id'})\n",
        "        ctx['verification_passed'] = False\n",
        "        return False\n",
        "\n",
        "    if not requires_mlflow:\n",
        "        print(\"   â„¹ï¸  No MLflow verification required for this task\")\n",
        "        tracker.log_phase_completion('verification', 'pass', {'reason': 'no_verification_required'})\n",
        "        ctx['verification_passed'] = True\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        import mlflow\n",
        "        run = mlflow.get_run(run_id)\n",
        "        print(f\"   âœ… MLflow run verified: {run_id}\")\n",
        "        tracker.log_phase_completion('verification', 'pass')\n",
        "        ctx['verification_passed'] = True\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Verification failed: {e}\")\n",
        "        tracker.log_phase_completion('verification', 'fail', {'error': str(e)})\n",
        "        ctx['verification_passed'] = False\n",
        "        return False\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PHASE 6.5: VERIFICATION RETRY\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def phase6_retry(ctx: Dict[str, Any]) -> bool:\n",
        "    \"\"\"Phase 6.5 â€“ Verification retry.\"\"\"\n",
        "    task_id = ctx['task_id']\n",
        "    tracker.activate_task(task_id)\n",
        "    print(f\"\\nðŸ” PHASE 6.5: Verification Retry ({task_id})\")\n",
        "\n",
        "    print(\"   ðŸ”§ Requesting verification fix...\")\n",
        "    fixed_ops_response = ops_commander.respond(f\"\"\"# VERIFICATION FAILED\n",
        "\n",
        "Provide code with proper MLflow logging:\n",
        "\n",
        "```python\n",
        "import mlflow\n",
        "with mlflow.start_run() as run:\n",
        "    run_id = run.info.run_id\n",
        "    mlflow.log_metric(\"metric\", 1.0)\n",
        "print(f\"run_id = {{run_id}}\")\n",
        "```\n",
        "\"\"\")\n",
        "\n",
        "    code_blocks = _extract_code_blocks(fixed_ops_response)\n",
        "    if not code_blocks:\n",
        "        return False\n",
        "\n",
        "    execution_telemetry = ctx.get('execution_telemetry', {})\n",
        "    for idx, code in enumerate(code_blocks):\n",
        "        try:\n",
        "            exec_globals = {'__name__': '__main__', 'MULTI_AGENT_ROOT': MULTI_AGENT_ROOT, 'Path': Path}\n",
        "            exec(code, exec_globals)\n",
        "\n",
        "            if 'run_id' in exec_globals:\n",
        "                run_id = exec_globals['run_id']\n",
        "                execution_telemetry['run_id'] = run_id\n",
        "                print(f\"      âœ… Captured run_id: {run_id}\")\n",
        "\n",
        "                import mlflow\n",
        "                mlflow.get_run(run_id)\n",
        "                print(f\"      âœ… MLflow run verified\")\n",
        "\n",
        "                execution_stats['retry_attempts']['verification'] += 1\n",
        "                tracker.log_phase_completion('verification', 'pass')\n",
        "                ctx['verification_passed'] = True\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"      âŒ Retry failed: {e}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PHASE 7: FINAL STATUS DETERMINATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def phase7_finalize(ctx: Dict[str, Any]) -> None:\n",
        "    \"\"\"Phase 7 â€“ Determine final task status.\"\"\"\n",
        "    task_id = ctx['task_id']\n",
        "    preliminary_status = ctx.get('preliminary_status')\n",
        "    execution_success = ctx.get('execution_success', False)\n",
        "    verification_passed = ctx.get('verification_passed', False)\n",
        "\n",
        "    tracker.activate_task(task_id)\n",
        "\n",
        "    print(f\"\\nðŸ“Š PHASE 7: Final Status ({task_id})\")\n",
        "\n",
        "    if preliminary_status == \"approved\" and execution_success and verification_passed:\n",
        "        final_status = \"completed\"\n",
        "        print(f\"   âœ… Task completed successfully\")\n",
        "        execution_stats['tasks_completed'] += 1\n",
        "        tracker.complete_task(status='completed', task_id=task_id, final_status_reason='Approved + evidence verified')\n",
        "    elif preliminary_status == \"approved\" and execution_success:\n",
        "        final_status = \"failed\"\n",
        "        print(f\"   âŒ Task FAILED - Execution succeeded but verification failed\")\n",
        "        tracker.complete_task(status='failed', task_id=task_id, final_status_reason='Verification failed')\n",
        "    elif preliminary_status == \"approved\":\n",
        "        final_status = \"failed\"\n",
        "        print(f\"   âŒ Task FAILED - Execution failed\")\n",
        "        tracker.complete_task(status='failed', task_id=task_id, final_status_reason='Execution failure')\n",
        "    else:\n",
        "        final_status = \"rejected\"\n",
        "        print(f\"   âŒ Task REJECTED - Did not pass approval gate\")\n",
        "        tracker.complete_task(status='rejected', task_id=task_id, final_status_reason='Rejected at approval gate')\n",
        "\n",
        "    ctx['final_status'] = final_status\n",
        "\n",
        "    task_timers = getattr(tracker, \"task_timers\", {})\n",
        "    duration = task_timers.get(task_id, {}).get('duration', 0)\n",
        "    total_retries = sum(execution_stats['retry_attempts'].values())\n",
        "\n",
        "    print(f\"   âœ… Task completed in {duration:.1f}s - Status: {final_status}\")\n",
        "    print(f\"   ðŸ”„ Total retries: {total_retries}\")\n",
        "\n",
        "    tracker.log_phase_completion('finalize', 'pass', {\n",
        "        'final_status': final_status,\n",
        "        'duration': duration,\n",
        "        'total_retries': total_retries\n",
        "    })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8Rz3rT3_jGGC",
      "metadata": {
        "id": "8Rz3rT3_jGGC"
      },
      "outputs": [],
      "source": [
        "#cell_9\n",
        "\n",
        "def prepare_execution_cycle(selected_task_ids: Optional[List[str]] = None) -> List[str]:\n",
        "    \"\"\"Prepare task contexts (Phase 0/2 artifacts must already exist).\"\"\"\n",
        "    if task_contexts:\n",
        "        print(\"\\nâ„¹ï¸  Existing task contexts detected â€” call reset_execution_state() for a clean slate.\")\n",
        "\n",
        "    ensure_phase_0()\n",
        "\n",
        "    tasks = pending_actions.get('tasks', [])\n",
        "    if not tasks:\n",
        "        print(\"\\nâš ï¸  WARNING: No tasks found in pending_actions.json!\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\nðŸ“‹ Found {len(tasks)} tasks to process\")\n",
        "    sorted_tasks = sorted(\n",
        "        tasks,\n",
        "        key=lambda t: {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}.get(t.get('priority', 'LOW'), 2),\n",
        "    )\n",
        "\n",
        "    prepared_ids: List[str] = []\n",
        "    for task in sorted_tasks:\n",
        "        if selected_task_ids and task['task_id'] not in selected_task_ids:\n",
        "            continue\n",
        "        ctx = initialize_task_context(task)\n",
        "        prepared_ids.append(ctx['task_id'])\n",
        "\n",
        "    if not prepared_ids:\n",
        "        print(\"\\nâš ï¸  WARNING: No matching tasks found for the provided filters.\")\n",
        "        return []\n",
        "\n",
        "    global current_task_order\n",
        "    current_task_order = prepared_ids\n",
        "\n",
        "    print(f\"   âœ… Prepared {len(prepared_ids)} task(s) for execution phases\")\n",
        "    return prepared_ids\n",
        "\n",
        "\n",
        "def _iter_task_contexts(task_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Return task contexts respecting the original scheduling order.\"\"\"\n",
        "    if not task_contexts:\n",
        "        raise RuntimeError(\"No task contexts prepared. Run prepare_execution_cycle() first.\")\n",
        "\n",
        "    if task_ids:\n",
        "        missing = [tid for tid in task_ids if tid not in task_contexts]\n",
        "        if missing:\n",
        "            print(f\"\\nâš ï¸  WARNING: Unknown task IDs requested: {', '.join(missing)}\")\n",
        "        ordered_ids = [tid for tid in task_ids if tid in task_contexts]\n",
        "    else:\n",
        "        ordered_ids = current_task_order or list(task_contexts.keys())\n",
        "\n",
        "    return [task_contexts[tid] for tid in ordered_ids]\n",
        "\n",
        "\n",
        "def run_phase3_batch(task_ids: Optional[List[str]] = None) -> None:\n",
        "    \"\"\"Execute Phase 3 (agent collection) for the prepared tasks.\"\"\"\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        phase3_collect(ctx)\n",
        "\n",
        "\n",
        "def run_phase4_batch(task_ids: Optional[List[str]] = None) -> Dict[str, str]:\n",
        "    \"\"\"Execute Phase 4 (approval gate) and return status per task.\"\"\"\n",
        "    results: Dict[str, str] = {}\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        status = phase4_gate(ctx)\n",
        "        results[ctx['task_id']] = status\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_phase4_5_batch(task_ids: Optional[List[str]] = None) -> Dict[str, str]:\n",
        "    \"\"\"Execute Phase 4.5 (approval retry) for rejected tasks.\"\"\"\n",
        "    results: Dict[str, str] = {}\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        if ctx.get('preliminary_status') != \"rejected\":\n",
        "            continue\n",
        "        status = phase4_5_approval_retry(ctx)\n",
        "        results[ctx['task_id']] = status\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_phase5_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
        "    \"\"\"Execute Phase 5 (implementation execution) for approved tasks.\"\"\"\n",
        "    results: Dict[str, bool] = {}\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        if ctx.get('preliminary_status') != \"approved\":\n",
        "            continue\n",
        "        success = phase5_execute(ctx)\n",
        "        results[ctx['task_id']] = success\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_phase5_retry_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
        "    \"\"\"Execute Phase 5.5 (execution retry) for tasks that failed execution.\"\"\"\n",
        "    results: Dict[str, bool] = {}\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        if ctx.get('preliminary_status') != \"approved\" or ctx.get('execution_success'):\n",
        "            continue\n",
        "        success = phase5_retry(ctx)\n",
        "        results[ctx['task_id']] = success\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_phase6_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
        "    \"\"\"Execute Phase 6 (verification) for successfully executed tasks.\"\"\"\n",
        "    results: Dict[str, bool] = {}\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        if not ctx.get('execution_success'):\n",
        "            continue\n",
        "        passed = phase6_verify(ctx)\n",
        "        results[ctx['task_id']] = passed\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_phase6_retry_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
        "    \"\"\"Execute Phase 6.5 (verification retry) for tasks that failed verification.\"\"\"\n",
        "    results: Dict[str, bool] = {}\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        if not ctx.get('execution_success') or ctx.get('verification_passed'):\n",
        "            continue\n",
        "        passed = phase6_retry(ctx)\n",
        "        results[ctx['task_id']] = passed\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_phase7_batch(task_ids: Optional[List[str]] = None) -> None:\n",
        "    \"\"\"Execute Phase 7 (finalization) for the selected tasks.\"\"\"\n",
        "    for ctx in _iter_task_contexts(task_ids):\n",
        "        phase7_finalize(ctx)\n",
        "\n",
        "\n",
        "def run_execution_cycle(selected_task_ids: Optional[List[str]] = None) -> None:\n",
        "    \"\"\"Run Phases 3â€“7 end-to-end for the pending actions queue.\"\"\"\n",
        "    prepared_ids = prepare_execution_cycle(selected_task_ids)\n",
        "    if not prepared_ids:\n",
        "        return\n",
        "\n",
        "    run_phase3_batch(prepared_ids)\n",
        "    run_phase4_batch(prepared_ids)\n",
        "\n",
        "    rejected_ids = [\n",
        "        tid for tid in prepared_ids\n",
        "        if task_contexts[tid].get('preliminary_status') == \"rejected\"\n",
        "    ]\n",
        "    if rejected_ids:\n",
        "        run_phase4_5_batch(rejected_ids)\n",
        "\n",
        "    approved_ids = [\n",
        "        tid for tid in prepared_ids\n",
        "        if task_contexts[tid].get('preliminary_status') == \"approved\"\n",
        "    ]\n",
        "\n",
        "    if approved_ids:\n",
        "        run_phase5_batch(approved_ids)\n",
        "        run_phase5_retry_batch(approved_ids)\n",
        "\n",
        "        execution_success_ids = [\n",
        "            tid for tid in approved_ids\n",
        "            if task_contexts[tid].get('execution_success')\n",
        "        ]\n",
        "\n",
        "        if execution_success_ids:\n",
        "            run_phase6_batch(execution_success_ids)\n",
        "            run_phase6_retry_batch(execution_success_ids)\n",
        "\n",
        "    run_phase7_batch(prepared_ids)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âœ… ALL TASKS PROCESSED\")\n",
        "    print(\"=\" * 70)\n",
        "    print_execution_summary()\n",
        "\n",
        "\n",
        "def print_execution_summary() -> None:\n",
        "    summary = tracker.get_summary()\n",
        "    print(f\"   Tasks Processed: {summary['total_tasks']}\")\n",
        "    print(f\"   Tasks Completed: {summary['completed']}\")\n",
        "    print(f\"   Tasks Failed: {summary['failed']}\")\n",
        "    print(f\"   Total Retries: {sum(summary['retry_stats'].values())}\")\n",
        "    print(f\"     - Approval: {summary['retry_stats']['approval_retries']}\")\n",
        "    print(f\"     - Execution: {summary['retry_stats']['execution_retries']}\")\n",
        "    print(f\"     - Verification: {summary['retry_stats']['verification_retries']}\")\n",
        "    print(f\"   Total Duration: {summary['total_duration_seconds']:.1f}s\")\n",
        "    print(\"\\n   Continue to Phase 8 (Reporting & Handoff)\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "UXBaTV_4jGGD",
      "metadata": {
        "id": "UXBaTV_4jGGD"
      },
      "outputs": [],
      "source": [
        "#cell_10\n",
        "\n",
        "def rerun_execution_phase(task_id: str, phase: str) -> None:\n",
        "    ctx = task_contexts.get(task_id)\n",
        "    if not ctx:\n",
        "        print(f\"âŒ Task {task_id} not found in cached contexts.\")\n",
        "        return\n",
        "\n",
        "    phase_key = phase.lower()\n",
        "    if phase_key in {\"5\", \"phase5\", \"execution\"}:\n",
        "        phase5_execute(ctx, rerun=True)\n",
        "    elif phase_key in {\"5.5\", \"phase5.5\", \"execution_retry\"}:\n",
        "        phase5_retry(ctx)\n",
        "    elif phase_key in {\"6\", \"phase6\", \"verification\"}:\n",
        "        phase6_verify(ctx)\n",
        "    elif phase_key in {\"6.5\", \"phase6.5\", \"verification_retry\"}:\n",
        "        phase6_retry(ctx)\n",
        "    else:\n",
        "        print(\"âš ï¸ Unknown phase. Valid options: phase5, phase5.5, phase6, phase6.5\")\n",
        "\n",
        "\n",
        "def export_task_contexts(label: str = \"execution_cycle\") -> Path:\n",
        "    export_dir = Path(MULTI_AGENT_ROOT) / \"ledgers\" / \"execution_trajectories\"\n",
        "    export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    export_path = export_dir / f\"{label}_{timestamp}.json\"\n",
        "\n",
        "    exportable = {}\n",
        "    for task_id, ctx in task_contexts.items():\n",
        "        exportable[task_id] = {\n",
        "            'task': ctx['task'],\n",
        "            'preliminary_status': ctx.get('preliminary_status'),\n",
        "            'execution_success': ctx.get('execution_success'),\n",
        "            'verification_passed': ctx.get('verification_passed'),\n",
        "            'execution_telemetry': ctx.get('execution_telemetry'),\n",
        "            'agent_responses': ctx.get('agent_responses'),\n",
        "        }\n",
        "\n",
        "    with export_path.open('w') as f:\n",
        "        json.dump(exportable, f, indent=2)\n",
        "\n",
        "    print(f\"ðŸ’¾ Task contexts exported to {export_path}\")\n",
        "    return export_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "MRQWavRXUFmi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRQWavRXUFmi",
        "outputId": "e12cd0df-64e9-4dd7-ffec-be03c6e1bdca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Patched CLIP models to use eager attention implementation by default\n"
          ]
        }
      ],
      "source": [
        "#cell_clip_patch\n",
        "_CLIP_ATTENTION_PATCHED = False\n",
        "\n",
        "\n",
        "def _apply_clip_attention_patch() -> None:\n",
        "    \"\"\"Force CLIP models to use eager attention so output_attentions is supported.\"\"\"\n",
        "    global _CLIP_ATTENTION_PATCHED\n",
        "    if _CLIP_ATTENTION_PATCHED:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        from transformers import CLIPModel, CLIPTextModel, CLIPVisionModel\n",
        "    except Exception as err:\n",
        "        print(f\"âš ï¸ Unable to patch CLIP attention defaults: {err}\")\n",
        "        return\n",
        "\n",
        "    def _set_config(config):\n",
        "        if hasattr(config, \"attn_implementation\"):\n",
        "            config.attn_implementation = \"eager\"\n",
        "        if hasattr(config, \"output_attentions\"):\n",
        "            config.output_attentions = True\n",
        "\n",
        "    def _patch_from_pretrained(cls, original_fn):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            model = original_fn(*args, **kwargs)\n",
        "            if hasattr(model, \"text_model\") and hasattr(model.text_model, \"config\"):\n",
        "                _set_config(model.text_model.config)\n",
        "            if hasattr(model, \"vision_model\") and hasattr(model.vision_model, \"config\"):\n",
        "                _set_config(model.vision_model.config)\n",
        "            return model\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    CLIPModel.from_pretrained = _patch_from_pretrained(CLIPModel, CLIPModel.from_pretrained)\n",
        "    CLIPTextModel.from_pretrained = _patch_from_pretrained(CLIPTextModel, CLIPTextModel.from_pretrained)\n",
        "    CLIPVisionModel.from_pretrained = _patch_from_pretrained(CLIPVisionModel, CLIPVisionModel.from_pretrained)\n",
        "\n",
        "    _CLIP_ATTENTION_PATCHED = True\n",
        "    print(\"âœ… Patched CLIP models to use eager attention implementation by default\")\n",
        "\n",
        "\n",
        "def configure_clip_attn_defaults() -> None:\n",
        "    _apply_clip_attention_patch()\n",
        "\n",
        "\n",
        "def ensure_clip_attention_patch() -> None:\n",
        "    _apply_clip_attention_patch()\n",
        "\n",
        "\n",
        "configure_clip_attn_defaults()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Mv9pKcbCy4bw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv9pKcbCy4bw",
        "outputId": "cb0fad19-8b8a-4823-d5e7-6d0a868b7fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pending tasks: []\n"
          ]
        }
      ],
      "source": [
        "pending_ids = [\n",
        "    task_id\n",
        "    for task_id, ctx in task_contexts.items()\n",
        "    if ctx.get('final_status') != 'completed'\n",
        "]\n",
        "\n",
        "print(\"Pending tasks:\", pending_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "kfqmT5C4FFRf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfqmT5C4FFRf",
        "outputId": "8acbf2b4-9839-4c05-bece-a9e61d3c644c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "hasattr(tracker, \"activate_task\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-004']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)\n",
        "\n",
        "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
        "print_execution_summary()"
      ],
      "metadata": {
        "id": "KI3Vlhe2dxCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "2632aa92-4676-408c-d948-76593a7bf056"
      },
      "id": "KI3Vlhe2dxCS",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â„¹ï¸  Existing task contexts detected â€” call reset_execution_state() for a clean slate.\n",
            "\n",
            "ðŸ“‹ Found 8 tasks to process\n",
            "\n",
            "ðŸš€ Starting Task W1-004: Execute CLIP attention collapse baseline experiments\n",
            "   Priority: HIGH\n",
            "   âœ… Prepared 1 task(s) for execution phases\n",
            "\n",
            "ðŸ“‹ PHASE 3: Implementation (W1-004)\n",
            "   âœ… ops_commander responded (19051 chars)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "APIConnectionError",
          "evalue": "Connection error.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_failed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connect_tcp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_tcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             sock = socket.create_connection(\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_exceptions.py\u001b[0m in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mto_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mraise\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectError\u001b[0m: [Errno 101] Network is unreachable",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m   1048\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mmapped_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectError\u001b[0m: [Errno 101] Network is unreachable",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-735830550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Phase 3 â†’ 7, end-to-end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun_phase3_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mrun_phase4_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2774669895.py\u001b[0m in \u001b[0;36mrun_phase3_batch\u001b[0;34m(task_ids)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;34m\"\"\"Execute Phase 3 (agent collection) for the prepared tasks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter_task_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mphase3_collect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3034514969.py\u001b[0m in \u001b[0;36mphase3_collect\u001b[0;34m(ctx)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         ).strip()\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrespond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0magent_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ops_commander'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_extract_code_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent/agents/roles.py\u001b[0m in \u001b[0;36mrespond\u001b[0;34m(self, context, tools)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             ]\n\u001b[0;32m---> 79\u001b[0;31m             response = client.messages.create(\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0msystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    925\u001b[0m             )\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         )\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Raising connection error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAPIConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m             log.debug(\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import socket, time\n",
        "\n",
        "HOST, PORT = \"api.anthropic.com\", 443\n",
        "print(\"Waiting for IPv4 reachability to\", HOST)\n",
        "\n",
        "def ipv4_ok(timeout=8):\n",
        "    addrs = socket.getaddrinfo(HOST, PORT, socket.AF_INET, socket.SOCK_STREAM)\n",
        "    ok = False\n",
        "    for fam, socktype, proto, canonname, sa in addrs:\n",
        "        try:\n",
        "            s = socket.socket(fam, socktype, proto)\n",
        "            s.settimeout(timeout)\n",
        "            s.connect(sa)\n",
        "            s.close()\n",
        "            ok = True\n",
        "            break\n",
        "        except Exception:\n",
        "            pass\n",
        "    return ok\n",
        "\n",
        "for attempt in range(1, 7):  # up to ~60s\n",
        "    if ipv4_ok(timeout=10):\n",
        "        print(f\"IPv4 reachable on attempt {attempt}.\")\n",
        "        break\n",
        "    print(f\"Attempt {attempt}: still not reachable; retrying...\")\n",
        "    time.sleep(10)\n",
        "else:\n",
        "    raise RuntimeError(\"IPv4 to api.anthropic.com not reachable after retries.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5M4m4IunK5x",
        "outputId": "2031eaea-e60b-4fd5-b1cc-ea530187bf00"
      },
      "id": "_5M4m4IunK5x",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for IPv4 reachability to api.anthropic.com\n",
            "Attempt 1: still not reachable; retrying...\n",
            "Attempt 2: still not reachable; retrying...\n",
            "Attempt 3: still not reachable; retrying...\n",
            "Attempt 4: still not reachable; retrying...\n",
            "Attempt 5: still not reachable; retrying...\n",
            "IPv4 reachable on attempt 6.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install anthropic httpx>=0.28\n",
        "\n",
        "import os, time, httpx, anthropic\n",
        "\n",
        "# Put your key into the env for this runtime first if not already:\n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"\n",
        "\n",
        "api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "assert api_key, \"Set ANTHROPIC_API_KEY in the environment.\"\n",
        "\n",
        "# Use httpx transport-level retries (covers connect timeouts) + longer connect timeout.\n",
        "transport = httpx.HTTPTransport(\n",
        "    retries=5,  # retry connect/read errors\n",
        ")\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=api_key,\n",
        "    http_client=httpx.Client(\n",
        "        transport=transport,\n",
        "        timeout=httpx.Timeout(connect=20.0, read=30.0, write=30.0, pool=None),\n",
        "        # NOTE: httpx doesn't expose a simple \"IPv4 only\" flag,\n",
        "        # but the pre-gate above ensures IPv4 is up before calls.\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Minimal smoke test with exponential backoff\n",
        "for attempt in range(1, 6):\n",
        "    try:\n",
        "        resp = client.messages.create(\n",
        "            model=\"claude-3-5-sonnet-20240620\",\n",
        "            max_tokens=8,\n",
        "            messages=[{\"role\":\"user\",\"content\":\"hi\"}],\n",
        "        )\n",
        "        print(\"âœ… Anthropic call OK. ID:\", resp.id)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Attempt {attempt} failed:\", type(e).__name__, str(e)[:200])\n",
        "        time.sleep(2**attempt / 2)\n",
        "else:\n",
        "    raise RuntimeError(\"All attempts failed; network still too flaky.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "QVWbChWOomUd",
        "outputId": "118e6df7-aee0-421c-cacc-9ed8c2c8a7b5"
      },
      "id": "QVWbChWOomUd",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-17844982.py:29: DeprecationWarning: The model 'claude-3-5-sonnet-20240620' is deprecated and will reach end-of-life on October 22, 2025.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  resp = client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempt 1 failed: NotFoundError Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20240620'}, 'request_id': 'req_011CUM3Gj6EzogCUAv3Q1LMM'}\n",
            "Attempt 2 failed: NotFoundError Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20240620'}, 'request_id': 'req_011CUM3GpXAcQV1DkpJXLVXL'}\n",
            "Attempt 3 failed: NotFoundError Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20240620'}, 'request_id': 'req_011CUM3GzREWmyyqVPsdGB5t'}\n",
            "Attempt 4 failed: NotFoundError Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20240620'}, 'request_id': 'req_011CUM3HJf6Stb4LkLfd7wnt'}\n",
            "Attempt 5 failed: NotFoundError Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20240620'}, 'request_id': 'req_011CUM3Hv45PBQLTDt3j7xZA'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "All attempts failed; network still too flaky.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17844982.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All attempts failed; network still too flaky.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: All attempts failed; network still too flaky."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests\n",
        "\n",
        "headers = {\n",
        "    \"x-api-key\": os.environ[\"ANTHROPIC_API_KEY\"],\n",
        "    \"anthropic-version\": \"2023-06-01\",\n",
        "}\n",
        "r = requests.get(\"https://api.anthropic.com/v1/models\", headers=headers, timeout=30)\n",
        "r.raise_for_status()\n",
        "models = [m[\"id\"] for m in r.json().get(\"data\", [])]\n",
        "print(\"\\n\".join(models))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "kmLNXX56pKyo",
        "outputId": "b7973a3c-81a7-4dd9-b6d7-97d063f07969"
      },
      "id": "kmLNXX56pKyo",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "HTTPSConnectionPool(host='api.anthropic.com', port=443): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x78c9b1899c70>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 101] Network is unreachable",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Failed to establish a new connection: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x78c9b1899c70>: Failed to establish a new connection: [Errno 101] Network is unreachable",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.anthropic.com', port=443): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x78c9b1899c70>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3565189736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"anthropic-version\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"2023-06-01\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://api.anthropic.com/v1/models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='api.anthropic.com', port=443): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x78c9b1899c70>: Failed to establish a new connection: [Errno 101] Network is unreachable'))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")"
      ],
      "metadata": {
        "id": "oJJA5h74d9bh"
      },
      "id": "oJJA5h74d9bh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjlnAjTKd_NV"
      },
      "id": "kjlnAjTKd_NV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "am3kjYosH14d",
      "metadata": {
        "id": "am3kjYosH14d"
      },
      "outputs": [],
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "#selected_tasks = ['W1-003']\n",
        "#prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "#if not prepared:\n",
        "    #raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "#run_phase3_batch(prepared)\n",
        "#run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "#rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "#if rejected:\n",
        "    #print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    #run_phase4_5_batch(rejected)\n",
        "\n",
        "#approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "#if approved:\n",
        "    #run_phase5_batch(approved)\n",
        "    #run_phase5_retry_batch(approved)\n",
        "\n",
        "    #executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    #if executed:\n",
        "        #run_phase6_batch(executed)\n",
        "        #run_phase6_retry_batch(executed)\n",
        "\n",
        "#run_phase7_batch(prepared)\n",
        "\n",
        "#print(\"\\n====== EXECUTION SUMMARY ======\")\n",
        "#print_execution_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NAW79LCkvaJT",
      "metadata": {
        "id": "NAW79LCkvaJT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NvEwM36QJveW",
      "metadata": {
        "id": "NvEwM36QJveW"
      },
      "outputs": [],
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-004']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)\n",
        "\n",
        "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
        "print_execution_summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")"
      ],
      "metadata": {
        "id": "VumT_dM-0MwI"
      },
      "id": "VumT_dM-0MwI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bk0_OuvXDKp4",
      "metadata": {
        "id": "bk0_OuvXDKp4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-005']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)\n",
        "\n",
        "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
        "print_execution_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_aW_QXcdGAGf",
      "metadata": {
        "id": "_aW_QXcdGAGf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# PhaseÂ 8 summary + exports\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-006']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)"
      ],
      "metadata": {
        "id": "qHUytApa1DB6"
      },
      "id": "qHUytApa1DB6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# PhaseÂ 8 summary + exports\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")\n"
      ],
      "metadata": {
        "id": "jGx6G3Ca1_eG"
      },
      "id": "jGx6G3Ca1_eG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-007']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)"
      ],
      "metadata": {
        "id": "-JtY5xH32CXx"
      },
      "id": "-JtY5xH32CXx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# PhaseÂ 8 summary + exports\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")"
      ],
      "metadata": {
        "id": "gByHe8Kz37Rx"
      },
      "id": "gByHe8Kz37Rx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-008']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)"
      ],
      "metadata": {
        "id": "bSaacc2H3_VF"
      },
      "id": "bSaacc2H3_VF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# PhaseÂ 8 summary + exports\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")"
      ],
      "metadata": {
        "id": "wDjVANgv5RkH"
      },
      "id": "wDjVANgv5RkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-001']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)"
      ],
      "metadata": {
        "id": "zjpt16YS7SuH"
      },
      "id": "zjpt16YS7SuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# PhaseÂ 8 summary + exports\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")"
      ],
      "metadata": {
        "id": "KfupErug7WTS"
      },
      "id": "KfupErug7WTS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
        "selected_tasks = ['W1-002']\n",
        "prepared = prepare_execution_cycle(selected_tasks)\n",
        "\n",
        "if not prepared:\n",
        "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
        "\n",
        "# Phase 3 â†’ 7, end-to-end\n",
        "run_phase3_batch(prepared)\n",
        "run_phase4_batch(prepared)\n",
        "\n",
        "# Retry cycle only kicks in when Phase 4 rejected something\n",
        "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
        "if rejected:\n",
        "    print(\"\\nðŸ”„ Running approval retries...\")\n",
        "    run_phase4_5_batch(rejected)\n",
        "\n",
        "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
        "if approved:\n",
        "    run_phase5_batch(approved)\n",
        "    run_phase5_retry_batch(approved)\n",
        "\n",
        "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
        "    if executed:\n",
        "        run_phase6_batch(executed)\n",
        "        run_phase6_retry_batch(executed)\n",
        "\n",
        "run_phase7_batch(prepared)"
      ],
      "metadata": {
        "id": "UNHbhzLE8H1l"
      },
      "id": "UNHbhzLE8H1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# PhaseÂ 8 summary + exports\n",
        "print_execution_summary()\n",
        "\n",
        "summary = tracker.get_summary()\n",
        "with open('week1_execution_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "snapshot_path = export_task_contexts(\"week1_run\")\n",
        "print(f\"Exported task contexts to {snapshot_path}\")"
      ],
      "metadata": {
        "id": "Nypsc6Ln8J8w"
      },
      "id": "Nypsc6Ln8J8w",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
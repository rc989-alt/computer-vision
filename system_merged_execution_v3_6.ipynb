{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "262c4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment ready\n",
      "ðŸ“ Project root: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean\n",
      "ðŸ¤– Multi-agent root: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent\n",
      "ðŸ§  Memory system: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/memory\n",
      "ðŸ“Š Ledgers: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers\n",
      "ðŸ“„ Reports: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/reports\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â€“ environment setup (local Mac version)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _is_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if _is_colab():\n",
    "    # Running on Colab â€“ keep the original mount logic\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    base = Path(\"/content/drive/MyDrive/cv_multimodal/project\")\n",
    "else:\n",
    "    # Running on your Mac â€“ point to the Google Drive mirror\n",
    "    base = Path(\n",
    "        \"/Users/guyan/Library/CloudStorage/\"\n",
    "        \"GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/\"\n",
    "        \"cv_multimodal/project\"\n",
    "    )\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Project path not found: {base}\")\n",
    "GDRIVE_ROOT = str(base)\n",
    "\n",
    "PROJECT_ROOT = base / \"computer-vision-clean\"\n",
    "MULTI_AGENT_ROOT = PROJECT_ROOT / \"multi-agent\"\n",
    "\n",
    "MEMORY_ROOT = MULTI_AGENT_ROOT / \"memory\"\n",
    "LEDGERS_ROOT = MULTI_AGENT_ROOT / \"ledgers\"\n",
    "REPORTS_ROOT = MULTI_AGENT_ROOT / \"reports\"\n",
    "LOGS_ROOT = MULTI_AGENT_ROOT / \"logs\"\n",
    "\n",
    "sys.path.insert(0, str(MULTI_AGENT_ROOT))\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "for path in (\n",
    "    MEMORY_ROOT / \"procedural_cache\",\n",
    "    LEDGERS_ROOT,\n",
    "    REPORTS_ROOT / \"execution_summary\",\n",
    "    LOGS_ROOT / \"execution_cycles\",\n",
    "):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Environment ready\")\n",
    "print(f\"ðŸ“ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"ðŸ¤– Multi-agent root: {MULTI_AGENT_ROOT}\")\n",
    "print(f\"ðŸ§  Memory system: {MEMORY_ROOT}\")\n",
    "print(f\"ðŸ“Š Ledgers: {LEDGERS_ROOT}\")\n",
    "print(f\"ðŸ“„ Reports: {REPORTS_ROOT}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "z_Uh9QdIjGF_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1761086607119,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "z_Uh9QdIjGF_",
    "outputId": "dfefba06-096d-4471-9ffb-168a308e7c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ” FINDING AND LOADING API KEYS\n",
      "================================================================================\n",
      "\n",
      "âœ… Found .env file: /Users/guyan/Desktop/env\n",
      "   Size: 436 bytes\n",
      "\n",
      "Verifying required API keys:\n",
      "\n",
      "âœ… ANTHROPIC_API_KEY\n",
      "   Value: sk-ant-a...7gAA\n",
      "   Used by: Claude Sonnet (Ops, Quality, Infrastructure)\n",
      "\n",
      "âœ… OPENAI_API_KEY\n",
      "   Value: sk-proj-...-6oA\n",
      "   Used by: GPT-4 (Critical Evaluator)\n",
      "\n",
      "âœ… GOOGLE_API_KEY\n",
      "   Value: AIzaSyBL...FVCI\n",
      "   Used by: Gemini (Research Advisor)\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL API KEYS LOADED SUCCESSFULLY\n",
      "âœ… Loaded 3 total keys from: /Users/guyan/Desktop/env\n",
      "âœ… Ready to initialize agents\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#cell_2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ” FINDING AND LOADING API KEYS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Search for .env file in multiple locations\n",
    "search_paths = [\n",
    "    '/Users/guyan/Desktop/env',\n",
    "    f'{GDRIVE_ROOT}/.env',\n",
    "    f'{GDRIVE_ROOT}/computer-vision-clean/.env',\n",
    "    '/content/drive/MyDrive/cv_multimodal/.env',\n",
    "    '/content/drive/My Drive/cv_multimodal/project/.env',\n",
    "    f'{PROJECT_ROOT}/.env',\n",
    "    str(Path.home() / '.env'),\n",
    "]\n",
    "\n",
    "env_file = None\n",
    "for path in search_paths:\n",
    "    if Path(path).exists():\n",
    "        env_file = Path(path)\n",
    "        print(f\"\\nâœ… Found .env file: {path}\")\n",
    "        print(f\"   Size: {env_file.stat().st_size} bytes\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"   Checking: {path}... not found\")\n",
    "\n",
    "if not env_file:\n",
    "    print(\"\\nðŸ” Searching entire cv_multimodal directory...\")\n",
    "    base = Path('/content/drive/MyDrive/cv_multimodal')\n",
    "    if base.exists():\n",
    "        all_env = list(base.rglob('*.env')) + list(base.rglob('.env*'))\n",
    "        if all_env:\n",
    "            print(f\"\\nâœ… Found .env files:\")\n",
    "            for f in all_env:\n",
    "                print(f\"   ðŸ“„ {f}\")\n",
    "            env_file = all_env[0]\n",
    "\n",
    "    if not env_file:\n",
    "        print(\"\\nâŒ No .env file found!\")\n",
    "        print(\"\\nðŸ’¡ Options:\")\n",
    "        print(\"   1. Upload .env to MyDrive/cv_multimodal/project/\")\n",
    "        print(\"   2. Use Colab Secrets (ðŸ”‘ icon in left sidebar)\")\n",
    "        raise FileNotFoundError(\"No .env file found - check Google Drive or use Colab Secrets\")\n",
    "\n",
    "# Load all API keys\n",
    "loaded_keys = []\n",
    "with open(env_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#') and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip().strip('\"').strip(\"'\")  # Remove quotes\n",
    "            os.environ[key] = value\n",
    "            loaded_keys.append(key)\n",
    "\n",
    "# Verify required keys for multi-agent system\n",
    "required = {\n",
    "    'ANTHROPIC_API_KEY': 'Claude Sonnet (Ops, Quality, Infrastructure)',\n",
    "    'OPENAI_API_KEY': 'GPT-4 (Critical Evaluator)',\n",
    "    'GOOGLE_API_KEY': 'Gemini (Research Advisor)'\n",
    "}\n",
    "\n",
    "print(\"Verifying required API keys:\\n\")\n",
    "all_loaded = True\n",
    "\n",
    "for key, usage in required.items():\n",
    "    value = os.environ.get(key)\n",
    "    if value and len(value) > 10:\n",
    "        masked = f\"{value[:8]}...{value[-4:]}\"\n",
    "        print(f\"âœ… {key}\")\n",
    "        print(f\"   Value: {masked}\")\n",
    "        print(f\"   Used by: {usage}\\n\")\n",
    "    else:\n",
    "        print(f\"âŒ {key} - NOT FOUND OR INVALID\")\n",
    "        print(f\"   Needed by: {usage}\\n\")\n",
    "        all_loaded = False\n",
    "\n",
    "print(\"=\"*80)\n",
    "if all_loaded:\n",
    "    print(\"âœ… ALL API KEYS LOADED SUCCESSFULLY\")\n",
    "    print(f\"âœ… Loaded {len(loaded_keys)} total keys from: {env_file}\")\n",
    "    print(\"âœ… Ready to initialize agents\")\n",
    "else:\n",
    "    print(\"âŒ SOME API KEYS MISSING OR INVALID\")\n",
    "    print(\"âš ï¸ Agent execution will fail without all 3 keys\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IoO9SWFUjGF_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9791,
     "status": "ok",
     "timestamp": 1761086616912,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "IoO9SWFUjGF_",
    "outputId": "cbc9baa1-e805-4f97-c470-3f5d139fe688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies installed\n",
      "âœ… V3.5 Reflection Service dependencies included (pyyaml for memory system)\n"
     ]
    }
   ],
   "source": [
    "  #cell 3\n",
    "  # Install dependencies\n",
    "  !pip install -q anthropic openai google-generativeai python-dotenv pyyaml mlflow tiktoken\n",
    "  !pip install -q torch torchvision transformers open_clip_torch pillow matplotlib seaborn\n",
    "\n",
    "  print(\"âœ… Dependencies installed\")\n",
    "  print(\"âœ… V3.5 Reflection Service dependencies included (pyyaml for memory system)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ob37NqAfjGF_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1761086617023,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "ob37NqAfjGF_",
    "outputId": "a2dfdbca-9cda-4e46-f1ed-da008ae42413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“¥ PENDING ACTIONS FROM PLANNING TEAM\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Meeting ID: None\n",
      "ðŸ—“ï¸  Generated: None\n",
      "ðŸŽ¯ Context: None\n",
      "\n",
      "ðŸ“Š Total tasks: 0\n",
      "   â­ HIGH: 0\n",
      "   ðŸŸ  MEDIUM: 0\n",
      "   ðŸ”µ LOW: 0\n",
      "\n",
      "ðŸ“‹ Task List:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ§  V3.5 MEMORY SYSTEM CHECK\n",
      "================================================================================\n",
      "âœ… Found reflection summary from previous cycle\n",
      "   ðŸ“„ /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/reports/reflection_summary.md\n",
      "   Preview: # Reflection Summary\n",
      "\n",
      "**Total Reflections**: 8\n",
      "**Average Confidence**: 0.75\n",
      "**Retry Success Rate**: 100.0%\n",
      "**Eligible for Retry**: 8/8\n",
      "\n",
      "## Top Root Causes\n",
      "- code_error: 8 occurrences\n",
      "...\n",
      "\n",
      "âœ… Loaded 1 learned rules from semantic memory:\n",
      "   â€¢ code_error â†’ Add error handling and validation\n",
      "     Confidence: 0.75\n",
      "â„¹ï¸  No proven templates yet (will be built during execution)\n",
      "\n",
      "================================================================================\n",
      "âœ… Ready to execute tasks with memory-enhanced decision making\n",
      "================================================================================\n",
      "âœ… Multi-agent system imported\n",
      "âœ… V3.5 Reflection Service initialized\n",
      "   ðŸ§  Memory root: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/memory\n",
      "   ðŸ“Š Confidence threshold (Ï„): 0.7\n",
      "   ðŸ”„ Max retries: Approval=2, Exec=2, Verify=2\n",
      "\n",
      "ðŸ¤– Initializing Executive Team:\n",
      "   âœ… Ops Commander (claude-sonnet-4-20250514)\n",
      "   âœ… Quality & Safety Officer (claude-sonnet-4-20250514)\n",
      "   âœ… Infrastructure & Performance Monitor (claude-sonnet-4-20250514)\n",
      "\n",
      "âœ… Executive Team initialized (3 agents)\n",
      "âœ… V3.5 enhanced execution with:\n",
      "   â€¢ Phase 4.5: Approval Retry (confidence-gated)\n",
      "   â€¢ Phase 5.5: Execution Retry (sandboxed)\n",
      "   â€¢ Phase 6.5: Verification Retry (artifact regeneration)\n",
      "   â€¢ Parallel Reflection Service (non-blocking monitoring)\n",
      "\n",
      "ðŸ“ Initialization log: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/logs/system_init/exec_team_init_20251022_013756.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cq/927bjcp96sv_3rdbxf1pc1h00000gn/T/ipykernel_96094/3199308654.py:157: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "/var/folders/cq/927bjcp96sv_3rdbxf1pc1h00000gn/T/ipykernel_96094/3199308654.py:188: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  init_log_file = Path(LOGS_ROOT) / 'system_init' / f'exec_team_init_{datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")}.json'\n"
     ]
    }
   ],
   "source": [
    "#cell 4\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Read pending actions from Planning Team\n",
    "pending_actions_file = Path(MULTI_AGENT_ROOT) / 'reports/handoff/pending_actions.json'\n",
    "\n",
    "if not pending_actions_file.exists():\n",
    "    print(f\"âŒ No pending actions found at {pending_actions_file}\")\n",
    "    print(\"âš ï¸ Planning Team must generate pending_actions.json first\")\n",
    "    raise FileNotFoundError(f\"Missing: {pending_actions_file}\")\n",
    "\n",
    "with open(pending_actions_file, 'r') as f:\n",
    "    pending_actions = json.load(f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“¥ PENDING ACTIONS FROM PLANNING TEAM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“‹ Meeting ID: {pending_actions.get('meeting_id')}\")\n",
    "print(f\"ðŸ—“ï¸  Generated: {pending_actions.get('generated_at')}\")\n",
    "print(f\"ðŸŽ¯ Context: {pending_actions.get('context')}\")\n",
    "\n",
    "decisions = pending_actions.get('decisions', [])\n",
    "print(f\"\\nðŸ“Š Total tasks: {len(decisions)}\")\n",
    "\n",
    "# Group by priority\n",
    "high_priority = [d for d in decisions if d.get('priority') == 'HIGH']\n",
    "medium_priority = [d for d in decisions if d.get('priority') == 'MEDIUM']\n",
    "low_priority = [d for d in decisions if d.get('priority') == 'LOW']\n",
    "\n",
    "print(f\"   â­ HIGH: {len(high_priority)}\")\n",
    "print(f\"   ðŸŸ  MEDIUM: {len(medium_priority)}\")\n",
    "print(f\"   ðŸ”µ LOW: {len(low_priority)}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Task List:\")\n",
    "for i, decision in enumerate(decisions, 1):\n",
    "    priority = decision.get('priority', 'UNKNOWN')\n",
    "    action = decision.get('action', 'No action specified')\n",
    "    owner = decision.get('owner', 'unassigned')\n",
    "    deadline = decision.get('deadline', 'No deadline')\n",
    "\n",
    "    priority_icon = 'â­' if priority == 'HIGH' else 'ðŸŸ ' if priority == 'MEDIUM' else 'ðŸ”µ'\n",
    "    print(f\"\\n{i}. {priority_icon} [{priority}] {action}\")\n",
    "    print(f\"   ðŸ‘¤ Owner: {owner}\")\n",
    "    print(f\"   â° Deadline: {deadline}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# V3.5: Load Memory System (Learned Rules from Previous Cycles)\n",
    "print(\"\\nðŸ§  V3.5 MEMORY SYSTEM CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for previous cycle's reflection summary\n",
    "reflection_summary_file = Path(REPORTS_ROOT) / 'reflection_summary.md'\n",
    "if reflection_summary_file.exists():\n",
    "    print(f\"âœ… Found reflection summary from previous cycle\")\n",
    "    print(f\"   ðŸ“„ {reflection_summary_file}\")\n",
    "    with open(reflection_summary_file, 'r') as f:\n",
    "        summary_preview = f.read()[:200]\n",
    "        print(f\"   Preview: {summary_preview}...\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸  No reflection summary from previous cycle (first run)\")\n",
    "\n",
    "# Load semantic memory (long-term learned rules)\n",
    "semantic_file = Path(MEMORY_ROOT) / 'semantic.yml'\n",
    "if semantic_file.exists():\n",
    "    with open(semantic_file, 'r') as f:\n",
    "        semantic_memory = yaml.safe_load(f)\n",
    "\n",
    "    learned_patterns = semantic_memory.get('learned_patterns', [])\n",
    "    if learned_patterns:\n",
    "        print(f\"\\nâœ… Loaded {len(learned_patterns)} learned rules from semantic memory:\")\n",
    "        for rule in learned_patterns[:3]:  # Show first 3\n",
    "            print(f\"   â€¢ {rule.get('pattern')} â†’ {rule.get('fix')}\")\n",
    "            print(f\"     Confidence: {rule.get('confidence', 0):.2f}\")\n",
    "        if len(learned_patterns) > 3:\n",
    "            print(f\"   ... and {len(learned_patterns) - 3} more rules\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸  No learned patterns yet (will be built during execution)\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸  No semantic memory file (first run)\")\n",
    "\n",
    "# Check procedural cache (proven templates)\n",
    "procedural_cache_dir = Path(MEMORY_ROOT) / 'procedural_cache'\n",
    "if procedural_cache_dir.exists():\n",
    "    templates = list(procedural_cache_dir.glob('*.json'))\n",
    "    if templates:\n",
    "        print(f\"\\nâœ… Found {len(templates)} proven job templates in procedural cache\")\n",
    "        for template in templates[:2]:  # Show first 2\n",
    "            print(f\"   ðŸ“„ {template.name}\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸  No proven templates yet (will be built during execution)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Ready to execute tasks with memory-enhanced decision making\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# cell 5\n",
    "# Import multi-agent system components\n",
    "os.chdir(MULTI_AGENT_ROOT)\n",
    "\n",
    "from agents.roles import Agent, AgentConfig, AgentTeam\n",
    "from agents.router import AgentRouter, RoutingStrategy, Message\n",
    "from tools.file_bridge import FileBridge, create_default_policies\n",
    "\n",
    "print(\"âœ… Multi-agent system imported\")\n",
    "\n",
    "# V3.5: Initialize Reflection Service and Retry Mechanisms\n",
    "from reflection_service import ReflectionService, process_execution_reflection\n",
    "from retry_mechanisms import RetryMechanisms\n",
    "\n",
    "reflection_service = ReflectionService(MULTI_AGENT_ROOT)\n",
    "retry_mechanisms = RetryMechanisms(MULTI_AGENT_ROOT)\n",
    "\n",
    "print(\"âœ… V3.5 Reflection Service initialized\")\n",
    "print(f\"   ðŸ§  Memory root: {MEMORY_ROOT}\")\n",
    "print(f\"   ðŸ“Š Confidence threshold (Ï„): {reflection_service.tau}\")\n",
    "print(f\"   ðŸ”„ Max retries: Approval={retry_mechanisms.MAX_APPROVAL_RETRIES}, \"\n",
    "      f\"Exec={retry_mechanisms.MAX_EXEC_RETRIES}, Verify={retry_mechanisms.MAX_VERIF_RETRIES}\")\n",
    "\n",
    "# Initialize Executive Team (3 agents)\n",
    "executive_team_agents = {}\n",
    "prompt_dir = Path(MULTI_AGENT_ROOT) / 'agents/prompts/executive_team'\n",
    "\n",
    "# Define Executive Team configuration\n",
    "executive_config = {\n",
    "    'ops_commander': {\n",
    "        'name': 'Ops Commander',\n",
    "        'model': 'claude-sonnet-4-20250514',\n",
    "        'provider': 'anthropic',\n",
    "        'role': 'Execute research experiments and deployments',\n",
    "        'prompt_file': '02_ops_commander.md',\n",
    "        'max_tokens': 8192  # Increased to allow longer responses\n",
    "    },\n",
    "    'quality_safety': {\n",
    "        'name': 'Quality & Safety Officer',\n",
    "        'model': 'claude-sonnet-4-20250514',\n",
    "        'provider': 'anthropic',\n",
    "        'role': 'Ensure code quality, safety, and reproducibility',\n",
    "        'prompt_file': '01_quality_safety_officer.md',\n",
    "        'max_tokens': 8192\n",
    "    },\n",
    "    'infrastructure': {\n",
    "        'name': 'Infrastructure & Performance Monitor',\n",
    "        'model': 'claude-sonnet-4-20250514',\n",
    "        'provider': 'anthropic',\n",
    "        'role': 'Monitor infrastructure and performance',\n",
    "        'prompt_file': '03_infrastructure_performance_monitor.md',\n",
    "        'max_tokens': 8192\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ¤– Initializing Executive Team:\")\n",
    "init_log = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    \"team\": \"executive\",\n",
    "    \"v3_5_features\": True,\n",
    "    \"agents\": []\n",
    "}\n",
    "\n",
    "for agent_id, config in executive_config.items():\n",
    "    agent_cfg = AgentConfig(\n",
    "        name=config['name'],\n",
    "        model=config['model'],\n",
    "        provider=config['provider'],\n",
    "        role=config['role'],\n",
    "        max_tokens=config.get('max_tokens', 8192),  # Increased from default 2000\n",
    "        prompt_file=config['prompt_file']\n",
    "    )\n",
    "    executive_team_agents[agent_id] = Agent(agent_cfg, prompt_dir)\n",
    "    print(f\"   âœ… {config['name']} ({config['model']})\")\n",
    "\n",
    "    # V3.5: Log agent initialization\n",
    "    init_log[\"agents\"].append({\n",
    "        \"id\": agent_id,\n",
    "        \"name\": config['name'],\n",
    "        \"model\": config['model'],\n",
    "        \"status\": \"READY\"\n",
    "    })\n",
    "\n",
    "# Create agent team and router\n",
    "executive_team = AgentTeam(executive_team_agents)\n",
    "executive_router = AgentRouter(executive_team)\n",
    "\n",
    "# V3.5: Save initialization log\n",
    "init_log_file = Path(LOGS_ROOT) / 'system_init' / f'exec_team_init_{datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(init_log_file, 'w') as f:\n",
    "    json.dump(init_log, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Executive Team initialized (3 agents)\")\n",
    "print(f\"âœ… V3.5 enhanced execution with:\")\n",
    "print(f\"   â€¢ Phase 4.5: Approval Retry (confidence-gated)\")\n",
    "print(f\"   â€¢ Phase 5.5: Execution Retry (sandboxed)\")\n",
    "print(f\"   â€¢ Phase 6.5: Verification Retry (artifact regeneration)\")\n",
    "print(f\"   â€¢ Parallel Reflection Service (non-blocking monitoring)\")\n",
    "print(f\"\\nðŸ“ Initialization log: {init_log_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2xMdbcbZjGGA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1761086617057,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "2xMdbcbZjGGA",
    "outputId": "59dda98e-f64e-45f3-ea84-80347ccc2f78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guyan/computer_vision/computer-vision/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Patched CLIP models to use eager attention implementation by default\n"
     ]
    }
   ],
   "source": [
    "#cell_clip_patch\n",
    "def configure_clip_attn_defaults() -> None:\n",
    "    \"\"\"Force CLIP models to use eager attention so output_attentions is supported.\"\"\"\n",
    "    try:\n",
    "        from transformers import CLIPModel, CLIPTextModel, CLIPVisionModel\n",
    "    except Exception as err:\n",
    "        print(f\"âš ï¸ Unable to patch CLIP attention defaults: {err}\")\n",
    "        return\n",
    "\n",
    "    def _set_config(config):\n",
    "        if hasattr(config, \"attn_implementation\"):\n",
    "            config.attn_implementation = \"eager\"\n",
    "        if hasattr(config, \"output_attentions\"):\n",
    "            config.output_attentions = True\n",
    "\n",
    "    def _patch_from_pretrained(cls, original_fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            model = original_fn(*args, **kwargs)\n",
    "            if hasattr(model, \"text_model\") and hasattr(model.text_model, \"config\"):\n",
    "                _set_config(model.text_model.config)\n",
    "            if hasattr(model, \"vision_model\") and hasattr(model.vision_model, \"config\"):\n",
    "                _set_config(model.vision_model.config)\n",
    "            return model\n",
    "        return wrapper\n",
    "\n",
    "    CLIPModel.from_pretrained = _patch_from_pretrained(CLIPModel, CLIPModel.from_pretrained)\n",
    "    CLIPTextModel.from_pretrained = _patch_from_pretrained(CLIPTextModel, CLIPTextModel.from_pretrained)\n",
    "    CLIPVisionModel.from_pretrained = _patch_from_pretrained(CLIPVisionModel, CLIPVisionModel.from_pretrained)\n",
    "\n",
    "    print(\"âœ… Patched CLIP models to use eager attention implementation by default\")\n",
    "\n",
    "\n",
    "configure_clip_attn_defaults()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "VH-F1hV77v9t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761086661005,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "VH-F1hV77v9t",
    "outputId": "2760b737-8956-4313-f4c3-1d41cd53c324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… V3.6 Task execution tracker initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#cell_6\n",
    "# Tracker & shared execution state\n",
    "from typing import Any, Dict, List, Optional\n",
    "class TaskExecutionTracker:\n",
    "    def __init__(self):\n",
    "        self.task_results = []\n",
    "        self.start_time = datetime.now()\n",
    "        self.current_task = None\n",
    "        self.current_task_id: Optional[str] = None\n",
    "        self.task_records: Dict[str, Dict[str, Any]] = {}\n",
    "        self.task_timers: Dict[str, Dict[str, Any]] = {}\n",
    "        self.retry_stats = {\n",
    "            'approval_retries': 0,\n",
    "            'execution_retries': 0,\n",
    "            'verification_retries': 0,\n",
    "            'total_retries': 0,\n",
    "        }\n",
    "\n",
    "    def start_task(self, task_id, action, priority):\n",
    "        start_ts = datetime.now()\n",
    "        task_record = {\n",
    "            'task_id': task_id,\n",
    "            'action': action,\n",
    "            'priority': priority,\n",
    "            'status': 'in_progress',\n",
    "            'start_time': start_ts.isoformat(),\n",
    "            'outputs': [],\n",
    "            'errors': [],\n",
    "            'agent_responses': {},\n",
    "            'retry_attempts': {'approval': 0, 'execution': 0, 'verification': 0},\n",
    "            'patches_applied': [],\n",
    "            'reflection_notes': [],\n",
    "            'confidence_scores': [],\n",
    "            'phases': {\n",
    "                'implementation': None,\n",
    "                'approval': None,\n",
    "                'execution': None,\n",
    "                'verification': None,\n",
    "            },\n",
    "        }\n",
    "        self.current_task = task_record\n",
    "        self.current_task_id = task_id\n",
    "        self.task_results = [t for t in self.task_results if t['task_id'] != task_id]\n",
    "        self.task_records[task_id] = task_record\n",
    "        self.task_timers[task_id] = {\n",
    "            'start': start_ts,\n",
    "            'end': None,\n",
    "            'duration': 0.0,\n",
    "        }\n",
    "        print(f\"\\nðŸš€ Starting Task {task_id}: {action}\")\n",
    "        print(f\"   Priority: {priority}\")\n",
    "\n",
    "    def log_agent_response(self, agent_name, response):\n",
    "        if self.current_task:\n",
    "            self.current_task['agent_responses'][agent_name] = response\n",
    "            print(f\"   âœ… {agent_name} responded ({len(response)} chars)\")\n",
    "\n",
    "    def log_retry_attempt(self, phase, patch_id=None, confidence=None):\n",
    "        if self.current_task:\n",
    "            self.current_task['retry_attempts'][phase] += 1\n",
    "            self.retry_stats[f'{phase}_retries'] += 1\n",
    "            self.retry_stats['total_retries'] += 1\n",
    "            if patch_id:\n",
    "                self.current_task['patches_applied'].append(\n",
    "                    {\n",
    "                        'phase': phase,\n",
    "                        'patch_id': patch_id,\n",
    "                        'confidence': confidence,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                    }\n",
    "                )\n",
    "            print(\n",
    "                f\"   ðŸ”„ Retry #{self.current_task['retry_attempts'][phase]} ({phase})\"\n",
    "                + (f\" - Confidence: {confidence:.2f}\" if confidence else \"\")\n",
    "            )\n",
    "\n",
    "    def log_reflection_note(self, reflection_note):\n",
    "        if self.current_task:\n",
    "            self.current_task['reflection_notes'].append(reflection_note)\n",
    "            print(f\"   ðŸ§  Reflection: {reflection_note.get('why_failed', 'unknown')}\")\n",
    "\n",
    "    def log_phase_completion(self, phase, status, details=None):\n",
    "        if self.current_task:\n",
    "            self.current_task['phases'][phase] = {\n",
    "                'status': status,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'details': details,\n",
    "            }\n",
    "            status_icon = 'âœ…' if status == 'pass' else 'âŒ' if status == 'fail' else 'â­ï¸'\n",
    "            print(f\"   {status_icon} Phase {phase}: {status}\")\n",
    "\n",
    "    def activate_task(self, task_id: str) -> None:\n",
    "        task_record = self.task_records.get(task_id)\n",
    "        if task_record:\n",
    "            self.current_task = task_record\n",
    "            self.current_task_id = task_id\n",
    "\n",
    "    def complete_task(self, status='completed', final_status_reason=None, task_id: Optional[str] = None):\n",
    "        valid_statuses = {'completed', 'failed', 'rejected', 'cancelled', 'skipped'}\n",
    "\n",
    "        if task_id is None and status not in valid_statuses and final_status_reason in valid_statuses:\n",
    "            # Support legacy call pattern: complete_task(task_id, 'completed')\n",
    "            task_id = status\n",
    "            status = final_status_reason\n",
    "            final_status_reason = None\n",
    "\n",
    "        if task_id:\n",
    "            task_record = self.task_records.get(task_id)\n",
    "            if task_record:\n",
    "                self.current_task = task_record\n",
    "                self.current_task_id = task_id\n",
    "        else:\n",
    "            task_record = self.current_task\n",
    "            task_id = getattr(self, \"current_task_id\", None)\n",
    "\n",
    "        if not task_record:\n",
    "            print(\"âš ï¸ Unable to complete task â€“ no active task context.\")\n",
    "            return\n",
    "\n",
    "        task_record['status'] = status\n",
    "        task_record['status_reason'] = final_status_reason\n",
    "        task_record['end_time'] = datetime.now().isoformat()\n",
    "        total_retries = sum(task_record['retry_attempts'].values())\n",
    "        task_record['total_retries'] = total_retries\n",
    "\n",
    "        # Update task timers\n",
    "        timer_entry = self.task_timers.setdefault(task_id, {})\n",
    "        end_time = datetime.fromisoformat(task_record['end_time'])\n",
    "        timer_entry['end'] = end_time\n",
    "        start_value = timer_entry.get('start')\n",
    "        if isinstance(start_value, datetime):\n",
    "            start_dt = start_value\n",
    "        elif isinstance(start_value, str):\n",
    "            start_dt = datetime.fromisoformat(start_value)\n",
    "        else:\n",
    "            start_dt = datetime.fromisoformat(task_record['start_time'])\n",
    "            timer_entry['start'] = start_dt\n",
    "        timer_entry['duration'] = max((end_time - start_dt).total_seconds(), 0.0)\n",
    "\n",
    "        # Replace existing record for deterministic ordering\n",
    "        self.task_results = [t for t in self.task_results if t['task_id'] != task_id]\n",
    "        self.task_results.append(task_record)\n",
    "\n",
    "        print(f\"   âœ… Task completed in {timer_entry['duration']:.1f}s - Status: {status}\")\n",
    "        if total_retries > 0:\n",
    "            print(f\"   ðŸ”„ Total retries: {total_retries}\")\n",
    "\n",
    "        if self.current_task_id == task_id:\n",
    "            self.current_task = None\n",
    "            self.current_task_id = None\n",
    "\n",
    "    def get_summary(self):\n",
    "        completed = len([t for t in self.task_results if t['status'] == 'completed'])\n",
    "        failed = len([t for t in self.task_results if t['status'] == 'failed'])\n",
    "        total_duration = (datetime.now() - self.start_time).total_seconds()\n",
    "\n",
    "        return {\n",
    "            'total_tasks': len(self.task_results),\n",
    "            'completed': completed,\n",
    "            'failed': failed,\n",
    "            'total_duration_seconds': total_duration,\n",
    "            'retry_stats': self.retry_stats,\n",
    "            'task_results': self.task_results,\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = TaskExecutionTracker()\n",
    "print(\"âœ… V3.6 Task execution tracker initialized\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6jHMnZ58jGGB",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761086653709,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "6jHMnZ58jGGB"
   },
   "outputs": [],
   "source": [
    "#cell_7\n",
    "\n",
    "\n",
    "def _fresh_execution_stats() -> Dict[str, Any]:\n",
    "    return {\n",
    "        'tasks_processed': 0,\n",
    "        'tasks_approved': 0,\n",
    "        'tasks_executed': 0,\n",
    "        'tasks_completed': 0,\n",
    "        'retry_attempts': {'approval': 0, 'execution': 0, 'verification': 0},\n",
    "    }\n",
    "\n",
    "\n",
    "execution_stats: Dict[str, Any] = _fresh_execution_stats()\n",
    "task_contexts: Dict[str, Dict[str, Any]] = {}\n",
    "current_task_order: List[str] = []\n",
    "\n",
    "\n",
    "def reset_execution_state() -> None:\n",
    "    \"\"\"Reset tracker, stats, and cached task contexts.\"\"\"\n",
    "    global tracker, execution_stats, task_contexts, current_task_order\n",
    "    tracker = TaskExecutionTracker()\n",
    "    execution_stats = _fresh_execution_stats()\n",
    "    task_contexts = {}\n",
    "    current_task_order = []\n",
    "    print(\"ðŸ”„ Execution state reset\")\n",
    "\n",
    "\n",
    "def parse_agent_verdict(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract structured verdict from agent response.\"\"\"\n",
    "    verdict_patterns = [\n",
    "        r'\\*\\*.*?Final Verdict:\\*\\*\\s*(âœ…\\s*APPROVED|âš ï¸\\s*CAUTION|âŒ\\s*BLOCKED)',\n",
    "        r'Final Verdict:\\s*(APPROVED|CAUTION|BLOCKED)',\n",
    "        r'Status:\\s*(APPROVED|REJECTED)',\n",
    "    ]\n",
    "\n",
    "    for pattern in verdict_patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            verdict_text = match.group(1).upper()\n",
    "            if 'APPROVED' in verdict_text or 'âœ…' in verdict_text:\n",
    "                return {'structured': True, 'status': 'APPROVED', 'confidence': 0.9}\n",
    "            if 'BLOCKED' in verdict_text or 'âŒ' in verdict_text:\n",
    "                return {'structured': True, 'status': 'REJECTED', 'confidence': 0.9}\n",
    "            return {'structured': True, 'status': 'CAUTION', 'confidence': 0.5}\n",
    "\n",
    "    approved_keywords = ['approved', 'looks good', 'ready', 'pass', 'âœ…']\n",
    "    rejected_keywords = ['blocked', 'rejected', 'unsafe', 'fail', 'âŒ']\n",
    "\n",
    "    approved_count = sum(1 for kw in approved_keywords if kw in response.lower())\n",
    "    rejected_count = sum(1 for kw in rejected_keywords if kw in response.lower())\n",
    "\n",
    "    if approved_count > rejected_count:\n",
    "        return {'structured': False, 'status': 'APPROVED', 'confidence': 0.6}\n",
    "    if rejected_count > approved_count:\n",
    "        return {'structured': False, 'status': 'REJECTED', 'confidence': 0.6}\n",
    "    return {'structured': False, 'status': 'REJECTED', 'confidence': 0.3}\n",
    "\n",
    "\n",
    "def determine_task_status_v2(\n",
    "    ops_response: str,\n",
    "    quality_response: str,\n",
    "    infra_response: str,\n",
    ") -> str:\n",
    "    \"\"\"Enhanced approval gate with structured verdict support.\"\"\"\n",
    "    ops_verdict = parse_agent_verdict(ops_response)\n",
    "    quality_verdict = parse_agent_verdict(quality_response)\n",
    "    infra_verdict = parse_agent_verdict(infra_response)\n",
    "\n",
    "    ops_approved = ops_verdict['status'] == 'APPROVED'\n",
    "    quality_approved = quality_verdict['status'] == 'APPROVED'\n",
    "    infra_approved = infra_verdict['status'] == 'APPROVED'\n",
    "\n",
    "    print(f\"\\n   ðŸ” Approval Gate Analysis:\")\n",
    "    print(f\"      Ops: {ops_verdict['status']} (conf: {ops_verdict['confidence']:.2f})\")\n",
    "    print(f\"      Quality: {quality_verdict['status']} (conf: {quality_verdict['confidence']:.2f})\")\n",
    "    print(f\"      Infrastructure: {infra_verdict['status']} (conf: {infra_verdict['confidence']:.2f})\")\n",
    "\n",
    "    if ops_approved and quality_approved and infra_approved:\n",
    "        avg_conf = (\n",
    "            ops_verdict['confidence']\n",
    "            + quality_verdict['confidence']\n",
    "            + infra_verdict['confidence']\n",
    "        ) / 3\n",
    "        print(f\"      âœ… ALL GATES APPROVED (avg confidence: {avg_conf:.2f})\")\n",
    "        return \"approved\"\n",
    "    print(\"      âŒ SOME GATES REJECTED\")\n",
    "    return \"rejected\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Nu_4g1JCWgpy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1761086617294,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "Nu_4g1JCWgpy",
    "outputId": "38cf1e68-efae-441a-e71b-aceff7b1a80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Multi-agent root resolved to: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent\n",
      "   âœ… reflections: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers/reflections.jsonl\n",
      "   âœ… episodic_memory: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/memory/episodic.jsonl\n",
      "   âœ… semantic_memory: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/memory/semantic.yml\n",
      "   âœ… patches_dir: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/patches\n",
      "   ðŸ§  Reflection service Ï„ threshold: 0.7\n",
      "âœ… Patched CLIP models to use eager attention implementation by default\n",
      "âœ… V3.6 Task execution tracker initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cell 7\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FINAL COMPLETE PHASES 3-7 LOOP (V3.6 WITH TRACKER INITIALIZATION)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Copy this ENTIRE file into ONE Colab cell after Cell 8 (agent initialization)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from reflection_service import ReflectionService, process_execution_reflection\n",
    "from retry_mechanisms import RetryMechanisms\n",
    "\n",
    "\n",
    "# Locate multi-agent root (supports Google Drive trajectories)\n",
    "_candidate_roots = []\n",
    "\n",
    "env_root = os.environ.get(\"MULTI_AGENT_ROOT\")\n",
    "if env_root:\n",
    "    _candidate_roots.append(Path(env_root))\n",
    "\n",
    "_candidate_roots.extend([\n",
    "    Path(\"/Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent\"),\n",
    "    Path(\"/content/drive/MyDrive/cv_multimodal/project/computer-vision-clean/multi-agent\"),\n",
    "    Path.cwd() / \"multi-agent\",\n",
    "    Path.cwd(),\n",
    "])\n",
    "\n",
    "MULTI_AGENT_ROOT = next((p for p in _candidate_roots if p and p.exists()), Path.cwd())\n",
    "\n",
    "print(f\"ðŸ“‚ Multi-agent root resolved to: {MULTI_AGENT_ROOT}\")\n",
    "\n",
    "_ledger_paths = {\n",
    "    \"reflections\": MULTI_AGENT_ROOT / \"ledgers/reflections.jsonl\",\n",
    "    \"episodic_memory\": MULTI_AGENT_ROOT / \"memory/episodic.jsonl\",\n",
    "    \"semantic_memory\": MULTI_AGENT_ROOT / \"memory/semantic.yml\",\n",
    "    \"patches_dir\": MULTI_AGENT_ROOT / \"patches\",\n",
    "}\n",
    "\n",
    "for name, path in _ledger_paths.items():\n",
    "    status = \"âœ…\" if path.exists() else \"âš ï¸\"\n",
    "    print(f\"   {status} {name}: {path}\")\n",
    "\n",
    "reflection_service = ReflectionService(str(MULTI_AGENT_ROOT))\n",
    "retry_mechanisms = RetryMechanisms(str(MULTI_AGENT_ROOT))\n",
    "print(f\"   ðŸ§  Reflection service Ï„ threshold: {reflection_service.tau}\")\n",
    "\n",
    "#cell_clip_patch\n",
    "_CLIP_ATTENTION_PATCHED = False\n",
    "\n",
    "\n",
    "def _apply_clip_attention_patch() -> None:\n",
    "    \"\"\"Force CLIP models to use eager attention so output_attentions is supported.\"\"\"\n",
    "    global _CLIP_ATTENTION_PATCHED\n",
    "    if _CLIP_ATTENTION_PATCHED:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        from transformers import CLIPModel, CLIPTextModel, CLIPVisionModel\n",
    "    except Exception as err:\n",
    "        print(f\"âš ï¸ Unable to patch CLIP attention defaults: {err}\")\n",
    "        return\n",
    "\n",
    "    def _force_eager(module) -> None:\n",
    "        if module is None:\n",
    "            return\n",
    "        if hasattr(module, \"set_attn_implementation\"):\n",
    "            try:\n",
    "                module.set_attn_implementation(\"eager\")\n",
    "            except TypeError:\n",
    "                pass\n",
    "        config = getattr(module, \"config\", None)\n",
    "        if config is not None:\n",
    "            if hasattr(config, \"attn_implementation\"):\n",
    "                config.attn_implementation = \"eager\"\n",
    "            if hasattr(config, \"output_attentions\"):\n",
    "                config.output_attentions = True\n",
    "\n",
    "    def _configure_model(model):\n",
    "        _force_eager(model)\n",
    "        for attr in (\"vision_model\", \"text_model\"):\n",
    "            _force_eager(getattr(model, attr, None))\n",
    "        for attr in (\"vision_model\", \"text_model\"):\n",
    "            sub = getattr(model, attr, None)\n",
    "            if sub is None:\n",
    "                continue\n",
    "            for child in getattr(sub, \"modules\", lambda: [])():\n",
    "                _force_eager(child)\n",
    "        return model\n",
    "\n",
    "    def _wrap_from_pretrained(cls, original_fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            kwargs.setdefault(\"attn_implementation\", \"eager\")\n",
    "            model = original_fn(*args, **kwargs)\n",
    "            return _configure_model(model)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    CLIPModel.from_pretrained = _wrap_from_pretrained(CLIPModel, CLIPModel.from_pretrained)\n",
    "    CLIPTextModel.from_pretrained = _wrap_from_pretrained(CLIPTextModel, CLIPTextModel.from_pretrained)\n",
    "    CLIPVisionModel.from_pretrained = _wrap_from_pretrained(CLIPVisionModel, CLIPVisionModel.from_pretrained)\n",
    "\n",
    "    _CLIP_ATTENTION_PATCHED = True\n",
    "    print(\"âœ… Patched CLIP models to use eager attention implementation by default\")\n",
    "\n",
    "\n",
    "def configure_clip_attn_defaults() -> None:\n",
    "    _apply_clip_attention_patch()\n",
    "\n",
    "\n",
    "def ensure_clip_attention_patch() -> None:\n",
    "    _apply_clip_attention_patch()\n",
    "\n",
    "\n",
    "configure_clip_attn_defaults()\n",
    "\n",
    "#cell_6\n",
    "# Tracker & shared execution state\n",
    "class TaskExecutionTracker:\n",
    "    def __init__(self):\n",
    "        self.task_results = []\n",
    "        self.start_time = datetime.now()\n",
    "        self.current_task = None\n",
    "        self.retry_stats = {\n",
    "            'approval_retries': 0,\n",
    "            'execution_retries': 0,\n",
    "            'verification_retries': 0,\n",
    "            'total_retries': 0,\n",
    "        }\n",
    "\n",
    "    def start_task(self, task_id, action, priority):\n",
    "        self.current_task = {\n",
    "            'task_id': task_id,\n",
    "            'action': action,\n",
    "            'priority': priority,\n",
    "            'status': 'in_progress',\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'outputs': [],\n",
    "            'errors': [],\n",
    "            'agent_responses': {},\n",
    "            'retry_attempts': {'approval': 0, 'execution': 0, 'verification': 0},\n",
    "            'patches_applied': [],\n",
    "            'reflection_notes': [],\n",
    "            'confidence_scores': [],\n",
    "            'phases': {\n",
    "                'implementation': None,\n",
    "                'approval': None,\n",
    "                'execution': None,\n",
    "                'verification': None,\n",
    "            },\n",
    "        }\n",
    "        print(f\"\\nðŸš€ Starting Task {task_id}: {action}\")\n",
    "        print(f\"   Priority: {priority}\")\n",
    "\n",
    "    def log_agent_response(self, agent_name, response):\n",
    "        if self.current_task:\n",
    "            self.current_task['agent_responses'][agent_name] = response\n",
    "            print(f\"   âœ… {agent_name} responded ({len(response)} chars)\")\n",
    "\n",
    "    def log_retry_attempt(self, phase, patch_id=None, confidence=None):\n",
    "        if self.current_task:\n",
    "            self.current_task['retry_attempts'][phase] += 1\n",
    "            self.retry_stats[f'{phase}_retries'] += 1\n",
    "            self.retry_stats['total_retries'] += 1\n",
    "            if patch_id:\n",
    "                self.current_task['patches_applied'].append(\n",
    "                    {\n",
    "                        'phase': phase,\n",
    "                        'patch_id': patch_id,\n",
    "                        'confidence': confidence,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                    }\n",
    "                )\n",
    "            print(\n",
    "                f\"   ðŸ”„ Retry #{self.current_task['retry_attempts'][phase]} ({phase})\"\n",
    "                + (f\" - Confidence: {confidence:.2f}\" if confidence else \"\")\n",
    "            )\n",
    "\n",
    "    def log_reflection_note(self, reflection_note):\n",
    "        if self.current_task:\n",
    "            self.current_task['reflection_notes'].append(reflection_note)\n",
    "            print(f\"   ðŸ§  Reflection: {reflection_note.get('why_failed', 'unknown')}\")\n",
    "\n",
    "    def log_phase_completion(self, phase, status, details=None):\n",
    "        if self.current_task:\n",
    "            self.current_task['phases'][phase] = {\n",
    "                'status': status,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'details': details,\n",
    "            }\n",
    "            status_icon = 'âœ…' if status == 'pass' else 'âŒ' if status == 'fail' else 'â­ï¸'\n",
    "            print(f\"   {status_icon} Phase {phase}: {status}\")\n",
    "\n",
    "    def complete_task(self, status='completed', final_status_reason=None):\n",
    "        if self.current_task:\n",
    "            self.current_task['status'] = status\n",
    "            self.current_task['status_reason'] = final_status_reason\n",
    "            self.current_task['end_time'] = datetime.now().isoformat()\n",
    "            total_retries = sum(self.current_task['retry_attempts'].values())\n",
    "            self.current_task['total_retries'] = total_retries\n",
    "            self.task_results.append(self.current_task)\n",
    "\n",
    "            duration = (\n",
    "                datetime.fromisoformat(self.current_task['end_time'])\n",
    "                - datetime.fromisoformat(self.current_task['start_time'])\n",
    "            ).total_seconds()\n",
    "\n",
    "            print(f\"   âœ… Task completed in {duration:.1f}s - Status: {status}\")\n",
    "            if total_retries > 0:\n",
    "                print(f\"   ðŸ”„ Total retries: {total_retries}\")\n",
    "\n",
    "            self.current_task = None\n",
    "\n",
    "    def get_summary(self):\n",
    "        completed = len([t for t in self.task_results if t['status'] == 'completed'])\n",
    "        failed = len([t for t in self.task_results if t['status'] == 'failed'])\n",
    "        total_duration = (datetime.now() - self.start_time).total_seconds()\n",
    "\n",
    "        return {\n",
    "            'total_tasks': len(self.task_results),\n",
    "            'completed': completed,\n",
    "            'failed': failed,\n",
    "            'total_duration_seconds': total_duration,\n",
    "            'retry_stats': self.retry_stats,\n",
    "            'task_results': self.task_results,\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = TaskExecutionTracker()\n",
    "print(\"âœ… V3.6 Task execution tracker initialized\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "istqXjC07c3k",
   "metadata": {
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1761086617450,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "istqXjC07c3k"
   },
   "outputs": [],
   "source": [
    "def ensure_phase_0() -> None:\n",
    "    \"\"\"Prepare synthetic assets used across execution phases.\"\"\"\n",
    "    if _phase0_state[\"completed\"]:\n",
    "        return\n",
    "\n",
    "    print(\"Phase 0 complete.\")\n",
    "    print(\"TEST_IMAGE_PATH:\", os.environ.get(\"TEST_IMAGE_PATH\"))\n",
    "    print(\"TEST_DATA_DIR:\", os.environ.get(\"TEST_DATA_DIR\"))\n",
    "\n",
    "#PHASE 0: Pre-Execution Infrastructure Setup\")\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "\n",
    "        if Path(\"/content\").exists():\n",
    "            test_dir = Path(\"/content/test_data\")\n",
    "        else:\n",
    "            test_dir = MULTI_AGENT_ROOT / \"test_data\"\n",
    "        test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        print(\"Creating test images for model validation...\")\n",
    "        for i in range(5):\n",
    "            test_img = Image.fromarray(\n",
    "                np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "            )\n",
    "            test_img.save(test_dir / f\"test_image_{i}.png\")\n",
    "\n",
    "        test_images = list(test_dir.glob(\"*.png\"))\n",
    "        print(f\"Created {len(test_images)} test images at {test_dir}\")\n",
    "        print(f\" Primary test image: {test_dir / \"test_image_0.png\"}\")\n",
    "\n",
    "        os.environ[\"TEST_IMAGE_PATH\"] = str((test_dir / \"test_image_0.png\").resolve())\n",
    "        os.environ[\"TEST_DATA_DIR\"] = str(test_dir.resolve())\n",
    "\n",
    "        # Attempt to fetch a real image via Pexels if API key available\n",
    "        pexels_key = os.environ.get(\"PEXELS_API_KEY\")\n",
    "        if pexels_key:\n",
    "            try:\n",
    "                try:\n",
    "                    import requests  # type: ignore\n",
    "                except ImportError:\n",
    "                    requests = None\n",
    "\n",
    "                search_url = \"https://api.pexels.com/v1/search\"\n",
    "                params = {\"query\": \"architecture\", \"per_page\": 1}\n",
    "                headers = {\"Authorization\": pexels_key}\n",
    "\n",
    "                if requests is not None:\n",
    "                    resp = requests.get(search_url, params=params, headers=headers, timeout=10)\n",
    "                    resp.raise_for_status()\n",
    "                    data = resp.json()\n",
    "                    image_url = data[\"photos\"][0][\"src\"][\"medium\"]\n",
    "                    image_bytes = requests.get(image_url, timeout=10).content\n",
    "                else:\n",
    "                    from urllib.request import Request, urlopen\n",
    "                    import json as _json\n",
    "\n",
    "                    req = Request(search_url + \"?query=architecture&per_page=1\", headers=headers)\n",
    "                    with urlopen(req, timeout=10) as resp:\n",
    "                        data = _json.loads(resp.read().decode(\"utf-8\"))\n",
    "                    image_url = data[\"photos\"][0][\"src\"][\"medium\"]\n",
    "                    with urlopen(image_url, timeout=10) as img_resp:\n",
    "                        image_bytes = img_resp.read()\n",
    "\n",
    "                real_path = test_dir / \"test_image_pexels.jpg\"\n",
    "                with open(real_path, \"wb\") as f:\n",
    "                    f.write(image_bytes)\n",
    "\n",
    "                os.environ[\"TEST_IMAGE_PATH\"] = str(real_path.resolve())\n",
    "                print(f\"   ðŸ“· Downloaded Pexels test image â†’ {real_path}\")\n",
    "            except Exception as dl_err:\n",
    "                print(f\"   âš ï¸ Unable to download Pexels image: {dl_err}\")\n",
    "\n",
    "        _phase0_state[\"completed\"] = True\n",
    "        _phase0_state[\"data_dir\"] = str(test_dir)\n",
    "        print(\"  Phase 0 infrastructure setup complete\")\n",
    "    except Exception as e:\n",
    "        print(f\" Phase 0 setup warning: {e}\")\n",
    "        print(\"  Continuing execution - agents will handle infrastructure setup\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "pics_dir = Path(\"/Users/guyan/Desktop/pics\")\n",
    "fallback_image = next(pics_dir.glob(\"*\"), None)\n",
    "if fallback_image:\n",
    "    os.environ[\"TEST_IMAGE_PATH\"] = str(fallback_image.resolve())\n",
    "    print(\"Overrode TEST_IMAGE_PATH â†’\", os.environ[\"TEST_IMAGE_PATH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ElIEZUhxLZob",
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1761086617538,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "ElIEZUhxLZob"
   },
   "outputs": [],
   "source": [
    "#cell_8_5\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 4.5: APPROVAL RETRY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def phase4_5_approval_retry(ctx: Dict[str, Any]) -> str:\n",
    "    \"\"\"Phase 4.5 â€“ Approval retry with Ops Commander receiving agent feedback.\"\"\"\n",
    "    task_id = ctx['task_id']\n",
    "    action = ctx['action']\n",
    "    priority = ctx['priority']\n",
    "    task = ctx['task']\n",
    "    agent_responses = ctx['agent_responses']\n",
    "\n",
    "    tracker.activate_task(task_id)\n",
    "\n",
    "    print(f\"\\nðŸ”§ PHASE 4.5: Approval Retry ({task_id})\")\n",
    "\n",
    "    quality_response = agent_responses['quality_safety']\n",
    "    infra_response = agent_responses['infrastructure']\n",
    "\n",
    "    retry_prompt = f\"\"\"# PHASE 4.5: APPROVAL RETRY REQUEST\n",
    "\n",
    "Your implementation for Task {task_id} was **REJECTED** in the approval gate.\n",
    "\n",
    "## FEEDBACK FROM YOUR TEAM\n",
    "\n",
    "### Quality & Safety Officer Review:\n",
    "```\n",
    "{quality_response}\n",
    "```\n",
    "\n",
    "### Infrastructure & Performance Monitor Review:\n",
    "```\n",
    "{infra_response}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## INFRASTRUCTURE REMINDER (Phase 0 Setup Available)\n",
    "**Test resources for validation only:**\n",
    "- Test images: `/content/test_data/test_image_*.png` (5 images, 224x224 RGB)\n",
    "- Primary test image: `os.environ.get('TEST_IMAGE_PATH')`\n",
    "\n",
    "**CRITICAL - For CLIP validation with test images:**\n",
    "```python\n",
    "test_img = Image.open(os.environ.get('TEST_IMAGE_PATH'))\n",
    "inputs = processor(text=[\"a photo\"], images=test_img, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "attention = outputs.attentions\n",
    "```\n",
    "\n",
    "**For actual experiments - use real images:**\n",
    "- Pexels API key in Colab secrets: `userdata.get('PIXEL_API')`\n",
    "\n",
    "---\n",
    "\n",
    "## YOUR MISSION: Provide a CORRECTED Implementation\n",
    "\n",
    "**Address ALL issues raised above:**\n",
    "\n",
    "1. **If Quality & Safety blocked you:**\n",
    "   - Fix code structure issues\n",
    "   - Add missing error handling\n",
    "   - Remove sys.exit() calls\n",
    "   - Close code blocks properly (``` on new line!)\n",
    "   - Ensure code is complete (not truncated)\n",
    "\n",
    "2. **If Infrastructure blocked you:**\n",
    "   - Adjust resource requirements\n",
    "   - Add environment checks\n",
    "\n",
    "3. **Code Block Requirements (CRITICAL):**\n",
    "   ```python\n",
    "   # Your CORRECTED implementation\n",
    "   ```  â† **MUST end with ``` on new line**\n",
    "\n",
    "4. **Keep under 16,000 characters**\n",
    "\n",
    "---\n",
    "\n",
    "## Response Format:\n",
    "\n",
    "**Ops Commander Final Verdict:** âœ… APPROVED (retry)\n",
    "\n",
    "## Changes Made\n",
    "- [List specific fixes based on feedback]\n",
    "\n",
    "## Corrected Implementation\n",
    "\n",
    "```python\n",
    "# Your complete, corrected code here\n",
    "```\n",
    "\n",
    "**Provide your CORRECTED implementation now:**\n",
    "\"\"\"\n",
    "\n",
    "    print(\"   ðŸ“ Requesting corrected implementation from Ops Commander...\")\n",
    "    corrected_ops_response = ops_commander.respond(retry_prompt)\n",
    "    agent_responses['ops_commander'] = corrected_ops_response\n",
    "    tracker.log_retry_attempt('approval', patch_id='approval_retry_1', confidence=0.80)\n",
    "\n",
    "    # Quality & Safety reviews\n",
    "    print(\"   ðŸ” Quality & Safety reviewing corrected implementation...\")\n",
    "    corrected_quality_response = quality_safety.respond(f\"\"\"# REVIEW CORRECTED IMPLEMENTATION\n",
    "\n",
    "Ops Commander provided corrected implementation. Check if issues were fixed.\n",
    "\n",
    "Previous feedback: {quality_response[:500]}...\n",
    "\n",
    "Corrected implementation: {corrected_ops_response[:1000]}...\n",
    "\n",
    "**Verdict:** [âœ… APPROVED / âŒ BLOCKED]\n",
    "\"\"\")\n",
    "    agent_responses['quality_safety'] = corrected_quality_response\n",
    "\n",
    "    # Infrastructure reviews\n",
    "    print(\"   ðŸ” Infrastructure reviewing corrected implementation...\")\n",
    "    corrected_infra_response = infrastructure.respond(f\"\"\"# REVIEW CORRECTED IMPLEMENTATION\n",
    "\n",
    "Ops Commander provided corrected implementation. Check if issues were fixed.\n",
    "\n",
    "Previous feedback: {infra_response[:500]}...\n",
    "\n",
    "Corrected implementation: {corrected_ops_response[:1000]}...\n",
    "\n",
    "**Verdict:** [âœ… APPROVED / âŒ BLOCKED]\n",
    "\"\"\")\n",
    "    agent_responses['infrastructure'] = corrected_infra_response\n",
    "\n",
    "    # Re-run approval gate\n",
    "    print(\"   ðŸ” Re-running approval gate with corrected implementation...\")\n",
    "    new_status = determine_task_status_v2(\n",
    "        corrected_ops_response,\n",
    "        corrected_quality_response,\n",
    "        corrected_infra_response\n",
    "    )\n",
    "\n",
    "    if new_status == \"approved\":\n",
    "        print(\"   âœ… APPROVED after correction\")\n",
    "        execution_stats['retry_attempts']['approval'] += 1\n",
    "        tracker.log_phase_completion('approval', 'pass')\n",
    "    else:\n",
    "        print(\"   âŒ Still REJECTED after correction\")\n",
    "        tracker.log_phase_completion('approval', 'fail')\n",
    "\n",
    "    ctx['preliminary_status'] = new_status\n",
    "    return new_status\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 5: EXECUTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def phase5_execute(ctx: Dict[str, Any], rerun: bool = False) -> bool:\n",
    "    \"\"\"Phase 5 â€“ Execute approved code blocks.\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    task_id = ctx['task_id']\n",
    "    agent_responses = ctx['agent_responses']\n",
    "\n",
    "    tracker.activate_task(task_id)\n",
    "\n",
    "    if not rerun:\n",
    "        print(f\"\\nâš¡ PHASE 5: Execution ({task_id})\")\n",
    "\n",
    "    ops_response = agent_responses.get('ops_commander', '')\n",
    "    code_blocks = _extract_code_blocks(ops_response)\n",
    "\n",
    "    if not code_blocks:\n",
    "        print(\"   â„¹ï¸  No executable code blocks detected â€“ treating as documentation task\")\n",
    "        tracker.log_phase_completion('execution', 'pass', {'reason': 'no_code_blocks'})\n",
    "        ctx['execution_success'] = True\n",
    "        ctx['_execution_runs'] += 1\n",
    "        return True\n",
    "\n",
    "    ensure_clip_attention_patch()\n",
    "    print(f\"   ðŸ“¦ Found {len(code_blocks)} code block(s)\")\n",
    "\n",
    "    execution_telemetry = ctx.get('execution_telemetry', {'task_id': task_id, 'exit_code': 0})\n",
    "\n",
    "    for idx, code in enumerate(code_blocks):\n",
    "        try:\n",
    "            # Ensure no dangling MLflow runs carry over between code blocks\n",
    "            try:\n",
    "                import mlflow\n",
    "\n",
    "                active = mlflow.active_run()\n",
    "                if active:\n",
    "                    print(\"      â„¹ï¸ Ending previous MLflow run before executing new code block\")\n",
    "                    mlflow.end_run()\n",
    "            except ModuleNotFoundError:\n",
    "                pass\n",
    "            except Exception as mlflow_err:\n",
    "                print(f\"      âš ï¸ Unable to finalize prior MLflow run: {mlflow_err}\")\n",
    "\n",
    "            ensure_clip_attention_patch()\n",
    "            _ensure_numpy_json_patch()\n",
    "            exec_globals = {\n",
    "                '__name__': '__main__',\n",
    "                'MULTI_AGENT_ROOT': MULTI_AGENT_ROOT,\n",
    "                'Path': Path,\n",
    "                'ensure_clip_attention_patch': ensure_clip_attention_patch,\n",
    "            }\n",
    "            exec(code, exec_globals)\n",
    "            print(f\"      âœ… Block {idx + 1} executed successfully\")\n",
    "\n",
    "            resolved_run_id = _resolve_mlflow_run_id(exec_globals)\n",
    "            if resolved_run_id:\n",
    "                execution_telemetry['run_id'] = resolved_run_id\n",
    "                if exec_globals.get('run_id') != resolved_run_id:\n",
    "                    exec_globals['run_id'] = resolved_run_id\n",
    "                    print(f\"      â„¹ï¸ Normalized MLflow run_id â†’ {resolved_run_id}\")\n",
    "            if 'mlflow_run_name' in exec_globals:\n",
    "                execution_telemetry['run_name'] = exec_globals['mlflow_run_name']\n",
    "            elif 'run_id' in exec_globals:\n",
    "                execution_telemetry['run_id'] = exec_globals['run_id']\n",
    "\n",
    "                try:\n",
    "                    import mlflow\n",
    "                    mlflow.get_run(exec_globals['run_id'])\n",
    "                except Exception:\n",
    "                    print(\"      âš ï¸ Provided run_id could not be verified with MLflow\")\n",
    "\n",
    "            if 'run_name' not in execution_telemetry:\n",
    "                for key in ('RUN_TAG', 'run_tag', 'RUN_NAME', 'run_name'):\n",
    "                    if key in exec_globals:\n",
    "                        execution_telemetry['run_name'] = exec_globals[key]\n",
    "                        break\n",
    "\n",
    "            if 'config' in exec_globals:\n",
    "                execution_telemetry['config'] = exec_globals['config']\n",
    "\n",
    "            execution_stats['tasks_executed'] += 1\n",
    "            tracker.log_phase_completion('execution', 'pass')\n",
    "            ctx['execution_success'] = True\n",
    "            ctx['execution_telemetry'] = execution_telemetry\n",
    "            ctx['_execution_runs'] += 1\n",
    "\n",
    "            # Post-execution hooks for task-specific artifacts\n",
    "            try:\n",
    "                from pathlib import Path\n",
    "                import shutil\n",
    "\n",
    "                if task_id == \"W1-003\":\n",
    "                    source_path = Path(\"/content/test_data/data/week1_query_set.json\")\n",
    "                    fallback_path = Path(\"/content/data/week1_query_set.json\")\n",
    "                    if source_path.exists():\n",
    "                        fallback_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.copy(source_path, fallback_path)\n",
    "                        print(f\"      â„¹ï¸ Synced query set to {fallback_path}\")\n",
    "            except Exception as hook_err:\n",
    "                print(f\"      âš ï¸ Post-execution hook failed: {hook_err}\")\n",
    "\n",
    "            try:\n",
    "                _verify_research_outputs(task_id)\n",
    "            except FileNotFoundError as validation_err:\n",
    "                raise RuntimeError(str(validation_err)) from validation_err\n",
    "\n",
    "            return True\n",
    "\n",
    "        except SystemExit as e:\n",
    "            print(f\"      âŒ Block {idx + 1} called sys.exit({e.code})\")\n",
    "            execution_telemetry['error'] = f\"sys.exit({e.code})\"\n",
    "            execution_telemetry['error_type'] = 'SystemExit'\n",
    "            execution_telemetry['exit_code'] = e.code or 1\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Block {idx + 1} failed: {e}\")\n",
    "            execution_telemetry['error'] = str(e)\n",
    "            execution_telemetry['error_type'] = type(e).__name__\n",
    "            execution_telemetry['exit_code'] = 1\n",
    "            break\n",
    "\n",
    "    ctx['execution_success'] = False\n",
    "    ctx['execution_telemetry'] = execution_telemetry\n",
    "    ctx['_execution_runs'] += 1\n",
    "    tracker.log_phase_completion('execution', 'fail', {'error': execution_telemetry.get('error')})\n",
    "    return False\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 5.5: EXECUTION RETRY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def phase5_retry(ctx: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Phase 5.5 â€“ Execution retry with error feedback and reflection fallback.\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    task_id = ctx['task_id']\n",
    "    agent_responses = ctx['agent_responses']\n",
    "    execution_telemetry = ctx.get('execution_telemetry', {'task_id': task_id})\n",
    "    error_message = execution_telemetry.get('error', 'Unknown error')\n",
    "    error_type = execution_telemetry.get('error_type', 'Exception')\n",
    "\n",
    "    tracker.activate_task(task_id)\n",
    "\n",
    "    print(f\"\\nðŸ”„ PHASE 5.5: Execution Retry ({task_id})\")\n",
    "\n",
    "    retry_prompt = f\"\"\"# PHASE 5.5: EXECUTION RETRY REQUEST\n",
    "\n",
    "Your code for Task {task_id} **FAILED DURING EXECUTION**.\n",
    "\n",
    "## FAILURE DETAILS\n",
    "**Error Type:** {error_type}\n",
    "**Error Message:**\n",
    "```\n",
    "{error_message}\n",
    "```\n",
    "**Exit Code:** {execution_telemetry.get('exit_code', 1)}\n",
    "\n",
    "---\n",
    "\n",
    "## TEAM FEEDBACK (Snapshot)\n",
    "### Quality & Safety Officer\n",
    "```\n",
    "{agent_responses.get('quality_safety', '')[:900]}...\n",
    "```\n",
    "\n",
    "### Infrastructure Monitor\n",
    "```\n",
    "{agent_responses.get('infrastructure', '')[:900]}...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## BEFORE CODING\n",
    "- Review execution logs and traceback\n",
    "- Close any unfinished code blocks (end with ``` on its own line)\n",
    "- Address Quality & Safety + Infrastructure feedback\n",
    "- Ensure CLIP validation includes BOTH text and images when applicable\n",
    "- NEVER call sys.exit(); raise exceptions instead\n",
    "- For Task W1-004: log per-layer entropy/dispersion statistics to MLflow **and** save `results/week1/clip_baseline_metrics.json`\n",
    "- For Task W1-006: generate `results/week1/statistical_analysis.md`, `results/week1/hypothesis_tests.json`, and `results/week1/effect_sizes.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## Response Format\n",
    "**Ops Commander Final Verdict:** âœ… APPROVED (execution retry)\n",
    "\n",
    "## Root Cause Analysis\n",
    "- [Explain exact failure]\n",
    "- [Reference peer feedback used in the fix]\n",
    "\n",
    "## Fixes Applied\n",
    "- [List code changes]\n",
    "- [Call out MLflow / CLIP handling if relevant]\n",
    "\n",
    "## Corrected Implementation\n",
    "```python\n",
    "# COMPLETE, EXECUTABLE FIXED CODE\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    retry_success = False\n",
    "    use_reflection_retry = False\n",
    "\n",
    "    print(\"   ðŸ”§ Requesting fixed implementation from Ops Commander (V3.6)...\")\n",
    "    try:\n",
    "        fixed_ops_response = ops_commander.respond(retry_prompt)\n",
    "        agent_responses['ops_commander'] = fixed_ops_response\n",
    "        tracker.log_retry_attempt('execution', patch_id='execution_retry_v3.6', confidence=0.75)\n",
    "\n",
    "        code_blocks = _extract_code_blocks(fixed_ops_response)\n",
    "        if not code_blocks:\n",
    "            print(\"   âš ï¸ No code blocks returned in retry response\")\n",
    "            use_reflection_retry = True\n",
    "        else:\n",
    "            # Ensure no dangling MLflow runs carry over between code blocks\n",
    "            try:\n",
    "                import mlflow\n",
    "\n",
    "                active = mlflow.active_run()\n",
    "                if active:\n",
    "                    print(\"      â„¹ï¸ Ending previous MLflow run before executing retry code block\")\n",
    "                    mlflow.end_run()\n",
    "            except ModuleNotFoundError:\n",
    "                pass\n",
    "            except Exception as mlflow_err:\n",
    "                print(f\"      âš ï¸ Unable to finalize prior MLflow run before retry: {mlflow_err}\")\n",
    "\n",
    "            ensure_clip_attention_patch()\n",
    "            _ensure_numpy_json_patch()\n",
    "            print(f\"   ðŸ“¦ Found {len(code_blocks)} code block(s) in retry response\")\n",
    "            for idx, code in enumerate(code_blocks):\n",
    "                try:\n",
    "                    ensure_clip_attention_patch()\n",
    "                    _ensure_numpy_json_patch()\n",
    "                    exec_globals = {\n",
    "                        '__name__': '__main__',\n",
    "                        'MULTI_AGENT_ROOT': MULTI_AGENT_ROOT,\n",
    "                        'Path': Path,\n",
    "                        'ensure_clip_attention_patch': ensure_clip_attention_patch,\n",
    "                    }\n",
    "                    exec(code, exec_globals)\n",
    "                    print(f\"      âœ… Retry block {idx + 1} executed successfully\")\n",
    "\n",
    "                    if 'run_id' in exec_globals:\n",
    "                        execution_telemetry['run_id'] = exec_globals['run_id']\n",
    "\n",
    "                    execution_stats['retry_attempts']['execution'] += 1\n",
    "                    tracker.log_phase_completion('execution', 'pass')\n",
    "                    ctx['execution_success'] = True\n",
    "                    ctx['execution_telemetry'] = execution_telemetry\n",
    "                    ctx['_execution_runs'] += 1\n",
    "                    return True\n",
    "                except SystemExit as e:\n",
    "                    print(f\"      âŒ Retry block {idx + 1} called sys.exit({e.code})\")\n",
    "                    execution_telemetry['error'] = f\"sys.exit({e.code})\"\n",
    "                    execution_telemetry['error_type'] = 'SystemExit'\n",
    "                    execution_telemetry['exit_code'] = e.code or 1\n",
    "                    use_reflection_retry = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"      âŒ Retry block {idx + 1} failed: {e}\")\n",
    "                    execution_telemetry['error'] = str(e)\n",
    "                    execution_telemetry['error_type'] = type(e).__name__\n",
    "                    execution_telemetry['exit_code'] = 1\n",
    "                    use_reflection_retry = True\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Retry request failed: {e}\")\n",
    "        use_reflection_retry = True\n",
    "\n",
    "    if not retry_success and use_reflection_retry:\n",
    "        print(\"   ðŸ§  Falling back to reflection-guided retry...\")\n",
    "        error_msg_lower = str(execution_telemetry.get('error', '')).lower()\n",
    "        error_hints = []\n",
    "\n",
    "        if 'pixel_values' in error_msg_lower or 'pixel values' in error_msg_lower:\n",
    "            error_hints.append({\n",
    "                'pattern': 'pixel_values_missing',\n",
    "                'diagnosis': 'CLIP/model requires image input',\n",
    "                'fix': 'Load test image via Image.open(os.environ.get(\"TEST_IMAGE_PATH\")) and pass as images=',\n",
    "                'confidence': 0.95,\n",
    "            })\n",
    "            print(\"      ðŸ’¡ Hint: Include images when using CLIP processor\")\n",
    "\n",
    "        if any(term in error_msg_lower for term in ['no attention', 'attention layers', 'attentions']):\n",
    "            error_hints.append({\n",
    "                'pattern': 'attention_layers_missing',\n",
    "                'diagnosis': 'CLIP attention tensors not returned',\n",
    "                'fix': 'Call model(..., output_attentions=True) and set attn_implementation=\"eager\"',\n",
    "                'confidence': 0.95,\n",
    "            })\n",
    "            print(\"      ðŸ’¡ Hint: Enable output_attentions for CLIP\")\n",
    "\n",
    "        if 'import' in error_msg_lower or 'module' in error_msg_lower:\n",
    "            error_hints.append({\n",
    "                'pattern': 'import_error',\n",
    "                'diagnosis': 'Missing import statement',\n",
    "                'fix': 'Add required import at top of code block',\n",
    "                'confidence': 0.90,\n",
    "            })\n",
    "            print(\"      ðŸ’¡ Hint: Missing import detected\")\n",
    "\n",
    "        if 'nameerror' in error_msg_lower or 'not defined' in error_msg_lower:\n",
    "            error_hints.append({\n",
    "                'pattern': 'undefined_variable',\n",
    "                'diagnosis': 'Variable or function referenced before definition',\n",
    "                'fix': 'Define variable/function before use',\n",
    "                'confidence': 0.85,\n",
    "            })\n",
    "            print(\"      ðŸ’¡ Hint: Undefined variable or function\")\n",
    "\n",
    "        if error_hints:\n",
    "            execution_telemetry['error_hints'] = error_hints\n",
    "            execution_telemetry['error_pattern_detected'] = error_hints[0]['pattern']\n",
    "\n",
    "        reflection_result = process_execution_reflection(\n",
    "            multi_agent_root=str(MULTI_AGENT_ROOT),\n",
    "            task_id=task_id,\n",
    "            phase='execution',\n",
    "            telemetry=execution_telemetry,\n",
    "            attempt_count=ctx.get('_execution_runs', 0),\n",
    "        )\n",
    "        tracker.log_reflection_note(reflection_result.get('reflection_note', ''))\n",
    "\n",
    "        policy = reflection_result.get('policy')\n",
    "        risk_scores = reflection_result.get('risk_scores', {})\n",
    "        confidence = risk_scores.get('confidence', 0.0)\n",
    "\n",
    "        if policy == 'RETRY' and confidence >= retry_mechanisms.tau:\n",
    "            retry_success, updated_task = retry_mechanisms.phase_5_5_execution_retry(\n",
    "                task_result=ctx.get('task_result', {}),\n",
    "                execution_telemetry=execution_telemetry,\n",
    "                attempt_count=ctx.get('_execution_runs', 0),\n",
    "            )\n",
    "\n",
    "            if retry_success:\n",
    "                print(f\"      âœ… Reflection-approved patch applied (confidence {confidence:.2f})\")\n",
    "                tracker.log_retry_attempt('execution', patch_id=updated_task.get('patch_applied'), confidence=confidence)\n",
    "                ctx.setdefault('task_result', {}).update(updated_task)\n",
    "                ctx['execution_success'] = True\n",
    "                ctx['execution_telemetry'] = execution_telemetry\n",
    "                ctx['_execution_runs'] += 1\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"      âš ï¸ Reflection retry skipped: {updated_task.get('retry_reason', 'unknown reason')}\")\n",
    "        else:\n",
    "            print(f\"      âš ï¸ Reflection policy={policy} confidence={confidence:.2f} â€“ no automated retry\")\n",
    "\n",
    "    ctx['execution_success'] = False\n",
    "    ctx['execution_telemetry'] = execution_telemetry\n",
    "    ctx['_execution_runs'] += 1\n",
    "    tracker.log_phase_completion('execution', 'fail', {'retry': 'exhausted'})\n",
    "    return False\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 6: VERIFICATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def phase6_verify(ctx: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Phase 6 â€“ Verify execution results.\"\"\"\n",
    "    task_id = ctx['task_id']\n",
    "    action = ctx.get('action', '')\n",
    "    execution_telemetry = ctx.get('execution_telemetry', {})\n",
    "\n",
    "    tracker.activate_task(task_id)\n",
    "\n",
    "    print(f\"\\nðŸ” PHASE 6: Verification ({task_id})\")\n",
    "\n",
    "    requires_mlflow = any(\n",
    "        kw in action.lower() for kw in ['execute', 'run', 'experiment', 'diagnostic', 'train']\n",
    "    )\n",
    "\n",
    "    run_id = execution_telemetry.get('run_id')\n",
    "    run_name = execution_telemetry.get('run_name')\n",
    "\n",
    "    if requires_mlflow:\n",
    "        candidate_globals = {'run_id': run_id}\n",
    "        if run_name:\n",
    "            candidate_globals['run_name'] = run_name\n",
    "            candidate_globals['RUN_TAG'] = run_name\n",
    "\n",
    "        config = execution_telemetry.get('config')\n",
    "        if isinstance(config, dict):\n",
    "            experiment_name = config.get('experiment') or config.get('EXPERIMENT_NAME')\n",
    "            if experiment_name:\n",
    "                candidate_globals['EXPERIMENT_NAME'] = experiment_name\n",
    "\n",
    "        if run_id and (len(str(run_id)) != 32 or not re.fullmatch(r'[0-9a-f]{32}', str(run_id))):\n",
    "            resolved_run_id = _resolve_mlflow_run_id(candidate_globals)\n",
    "            if resolved_run_id and resolved_run_id != run_id:\n",
    "                print(f\"   â„¹ï¸ Resolved MLflow run_id â†’ {resolved_run_id}\")\n",
    "                run_id = resolved_run_id\n",
    "                execution_telemetry['run_id'] = resolved_run_id\n",
    "\n",
    "    if requires_mlflow and not run_id:\n",
    "        print(\"   âŒ No run_id found for MLflow-required task\")\n",
    "        tracker.log_phase_completion('verification', 'fail', {'reason': 'no_run_id'})\n",
    "        ctx['verification_passed'] = False\n",
    "        return False\n",
    "\n",
    "    if not requires_mlflow:\n",
    "        print(\"   â„¹ï¸  No MLflow verification required for this task\")\n",
    "        tracker.log_phase_completion('verification', 'pass', {'reason': 'no_verification_required'})\n",
    "        ctx['verification_passed'] = True\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        import mlflow\n",
    "        run = mlflow.get_run(run_id)\n",
    "        print(f\"   âœ… MLflow run verified: {run_id}\")\n",
    "        tracker.log_phase_completion('verification', 'pass')\n",
    "        ctx['verification_passed'] = True\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Verification failed: {e}\")\n",
    "        tracker.log_phase_completion('verification', 'fail', {'error': str(e)})\n",
    "        ctx['verification_passed'] = False\n",
    "        return False\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 6.5: VERIFICATION RETRY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def phase6_retry(ctx: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Phase 6.5 â€“ Verification retry.\"\"\"\n",
    "    task_id = ctx['task_id']\n",
    "    tracker.activate_task(task_id)\n",
    "    print(f\"\\nðŸ” PHASE 6.5: Verification Retry ({task_id})\")\n",
    "\n",
    "    print(\"   ðŸ”§ Requesting verification fix...\")\n",
    "    fixed_ops_response = ops_commander.respond(f\"\"\"# VERIFICATION FAILED\n",
    "\n",
    "Provide code with proper MLflow logging:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    mlflow.log_metric(\"metric\", 1.0)\n",
    "print(f\"run_id = {{run_id}}\")\n",
    "```\n",
    "\"\"\")\n",
    "\n",
    "    code_blocks = _extract_code_blocks(fixed_ops_response)\n",
    "    if not code_blocks:\n",
    "        return False\n",
    "\n",
    "    execution_telemetry = ctx.get('execution_telemetry', {})\n",
    "    for idx, code in enumerate(code_blocks):\n",
    "        try:\n",
    "            exec_globals = {'__name__': '__main__', 'MULTI_AGENT_ROOT': MULTI_AGENT_ROOT, 'Path': Path}\n",
    "            exec(code, exec_globals)\n",
    "\n",
    "            if 'run_id' in exec_globals:\n",
    "                run_id = exec_globals['run_id']\n",
    "                execution_telemetry['run_id'] = run_id\n",
    "                print(f\"      âœ… Captured run_id: {run_id}\")\n",
    "\n",
    "                import mlflow\n",
    "                mlflow.get_run(run_id)\n",
    "                print(f\"      âœ… MLflow run verified\")\n",
    "\n",
    "                execution_stats['retry_attempts']['verification'] += 1\n",
    "                tracker.log_phase_completion('verification', 'pass')\n",
    "                ctx['verification_passed'] = True\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Retry failed: {e}\")\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 7: FINAL STATUS DETERMINATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def phase7_finalize(ctx: Dict[str, Any]) -> None:\n",
    "    \"\"\"Phase 7 â€“ Determine final task status.\"\"\"\n",
    "    task_id = ctx['task_id']\n",
    "    preliminary_status = ctx.get('preliminary_status')\n",
    "    execution_success = ctx.get('execution_success', False)\n",
    "    verification_passed = ctx.get('verification_passed', False)\n",
    "\n",
    "    tracker.activate_task(task_id)\n",
    "\n",
    "    print(f\"\\nðŸ“Š PHASE 7: Final Status ({task_id})\")\n",
    "\n",
    "    if preliminary_status == \"approved\" and execution_success and verification_passed:\n",
    "        final_status = \"completed\"\n",
    "        print(f\"   âœ… Task completed successfully\")\n",
    "        execution_stats['tasks_completed'] += 1\n",
    "        tracker.complete_task(status='completed', task_id=task_id, final_status_reason='Approved + evidence verified')\n",
    "    elif preliminary_status == \"approved\" and execution_success:\n",
    "        final_status = \"failed\"\n",
    "        print(f\"   âŒ Task FAILED - Execution succeeded but verification failed\")\n",
    "        tracker.complete_task(status='failed', task_id=task_id, final_status_reason='Verification failed')\n",
    "    elif preliminary_status == \"approved\":\n",
    "        final_status = \"failed\"\n",
    "        print(f\"   âŒ Task FAILED - Execution failed\")\n",
    "        tracker.complete_task(status='failed', task_id=task_id, final_status_reason='Execution failure')\n",
    "    else:\n",
    "        final_status = \"rejected\"\n",
    "        print(f\"   âŒ Task REJECTED - Did not pass approval gate\")\n",
    "        tracker.complete_task(status='rejected', task_id=task_id, final_status_reason='Rejected at approval gate')\n",
    "\n",
    "    ctx['final_status'] = final_status\n",
    "\n",
    "    task_timers = getattr(tracker, \"task_timers\", {})\n",
    "    duration = task_timers.get(task_id, {}).get('duration', 0)\n",
    "    total_retries = sum(execution_stats['retry_attempts'].values())\n",
    "\n",
    "    print(f\"   âœ… Task completed in {duration:.1f}s - Status: {final_status}\")\n",
    "    print(f\"   ðŸ”„ Total retries: {total_retries}\")\n",
    "\n",
    "    tracker.log_phase_completion('finalize', 'pass', {\n",
    "        'final_status': final_status,\n",
    "        'duration': duration,\n",
    "        'total_retries': total_retries\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8Rz3rT3_jGGC",
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1761086617593,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "8Rz3rT3_jGGC"
   },
   "outputs": [],
   "source": [
    "#cell_9\n",
    "\n",
    "def prepare_execution_cycle(selected_task_ids: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Prepare task contexts (Phase 0/2 artifacts must already exist).\"\"\"\n",
    "    if task_contexts:\n",
    "        print(\"\\nâ„¹ï¸  Existing task contexts detected â€” call reset_execution_state() for a clean slate.\")\n",
    "\n",
    "    ensure_phase_0()\n",
    "\n",
    "    tasks = pending_actions.get('tasks', [])\n",
    "    if not tasks:\n",
    "        print(\"\\nâš ï¸  WARNING: No tasks found in pending_actions.json!\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\nðŸ“‹ Found {len(tasks)} tasks to process\")\n",
    "    sorted_tasks = sorted(\n",
    "        tasks,\n",
    "        key=lambda t: {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}.get(t.get('priority', 'LOW'), 2),\n",
    "    )\n",
    "\n",
    "    prepared_ids: List[str] = []\n",
    "    for task in sorted_tasks:\n",
    "        if selected_task_ids and task['task_id'] not in selected_task_ids:\n",
    "            continue\n",
    "        ctx = initialize_task_context(task)\n",
    "        prepared_ids.append(ctx['task_id'])\n",
    "\n",
    "    if not prepared_ids:\n",
    "        print(\"\\nâš ï¸  WARNING: No matching tasks found for the provided filters.\")\n",
    "        return []\n",
    "\n",
    "    global current_task_order\n",
    "    current_task_order = prepared_ids\n",
    "\n",
    "    print(f\"   âœ… Prepared {len(prepared_ids)} task(s) for execution phases\")\n",
    "    return prepared_ids\n",
    "\n",
    "\n",
    "def _iter_task_contexts(task_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return task contexts respecting the original scheduling order.\"\"\"\n",
    "    if not task_contexts:\n",
    "        raise RuntimeError(\"No task contexts prepared. Run prepare_execution_cycle() first.\")\n",
    "\n",
    "    if task_ids:\n",
    "        missing = [tid for tid in task_ids if tid not in task_contexts]\n",
    "        if missing:\n",
    "            print(f\"\\nâš ï¸  WARNING: Unknown task IDs requested: {', '.join(missing)}\")\n",
    "        ordered_ids = [tid for tid in task_ids if tid in task_contexts]\n",
    "    else:\n",
    "        ordered_ids = current_task_order or list(task_contexts.keys())\n",
    "\n",
    "    return [task_contexts[tid] for tid in ordered_ids]\n",
    "\n",
    "\n",
    "def run_phase3_batch(task_ids: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"Execute Phase 3 (agent collection) for the prepared tasks.\"\"\"\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        phase3_collect(ctx)\n",
    "\n",
    "\n",
    "def run_phase4_batch(task_ids: Optional[List[str]] = None) -> Dict[str, str]:\n",
    "    \"\"\"Execute Phase 4 (approval gate) and return status per task.\"\"\"\n",
    "    results: Dict[str, str] = {}\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        status = phase4_gate(ctx)\n",
    "        results[ctx['task_id']] = status\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_phase4_5_batch(task_ids: Optional[List[str]] = None) -> Dict[str, str]:\n",
    "    \"\"\"Execute Phase 4.5 (approval retry) for rejected tasks.\"\"\"\n",
    "    results: Dict[str, str] = {}\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        if ctx.get('preliminary_status') != \"rejected\":\n",
    "            continue\n",
    "        status = phase4_5_approval_retry(ctx)\n",
    "        results[ctx['task_id']] = status\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_phase5_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
    "    \"\"\"Execute Phase 5 (implementation execution) for approved tasks.\"\"\"\n",
    "    results: Dict[str, bool] = {}\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        if ctx.get('preliminary_status') != \"approved\":\n",
    "            continue\n",
    "        success = phase5_execute(ctx)\n",
    "        results[ctx['task_id']] = success\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_phase5_retry_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
    "    \"\"\"Execute Phase 5.5 (execution retry) for tasks that failed execution.\"\"\"\n",
    "    results: Dict[str, bool] = {}\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        if ctx.get('preliminary_status') != \"approved\" or ctx.get('execution_success'):\n",
    "            continue\n",
    "        success = phase5_retry(ctx)\n",
    "        results[ctx['task_id']] = success\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_phase6_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
    "    \"\"\"Execute Phase 6 (verification) for successfully executed tasks.\"\"\"\n",
    "    results: Dict[str, bool] = {}\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        if not ctx.get('execution_success'):\n",
    "            continue\n",
    "        passed = phase6_verify(ctx)\n",
    "        results[ctx['task_id']] = passed\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_phase6_retry_batch(task_ids: Optional[List[str]] = None) -> Dict[str, bool]:\n",
    "    \"\"\"Execute Phase 6.5 (verification retry) for tasks that failed verification.\"\"\"\n",
    "    results: Dict[str, bool] = {}\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        if not ctx.get('execution_success') or ctx.get('verification_passed'):\n",
    "            continue\n",
    "        passed = phase6_retry(ctx)\n",
    "        results[ctx['task_id']] = passed\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_phase7_batch(task_ids: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"Execute Phase 7 (finalization) for the selected tasks.\"\"\"\n",
    "    for ctx in _iter_task_contexts(task_ids):\n",
    "        phase7_finalize(ctx)\n",
    "\n",
    "\n",
    "def run_execution_cycle(selected_task_ids: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"Run Phases 3â€“7 end-to-end for the pending actions queue.\"\"\"\n",
    "    prepared_ids = prepare_execution_cycle(selected_task_ids)\n",
    "    if not prepared_ids:\n",
    "        return\n",
    "\n",
    "    run_phase3_batch(prepared_ids)\n",
    "    run_phase4_batch(prepared_ids)\n",
    "\n",
    "    rejected_ids = [\n",
    "        tid for tid in prepared_ids\n",
    "        if task_contexts[tid].get('preliminary_status') == \"rejected\"\n",
    "    ]\n",
    "    if rejected_ids:\n",
    "        run_phase4_5_batch(rejected_ids)\n",
    "\n",
    "    approved_ids = [\n",
    "        tid for tid in prepared_ids\n",
    "        if task_contexts[tid].get('preliminary_status') == \"approved\"\n",
    "    ]\n",
    "\n",
    "    if approved_ids:\n",
    "        run_phase5_batch(approved_ids)\n",
    "        run_phase5_retry_batch(approved_ids)\n",
    "\n",
    "        execution_success_ids = [\n",
    "            tid for tid in approved_ids\n",
    "            if task_contexts[tid].get('execution_success')\n",
    "        ]\n",
    "\n",
    "        if execution_success_ids:\n",
    "            run_phase6_batch(execution_success_ids)\n",
    "            run_phase6_retry_batch(execution_success_ids)\n",
    "\n",
    "    run_phase7_batch(prepared_ids)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… ALL TASKS PROCESSED\")\n",
    "    print(\"=\" * 70)\n",
    "    print_execution_summary()\n",
    "\n",
    "\n",
    "def print_execution_summary() -> None:\n",
    "    summary = tracker.get_summary()\n",
    "    print(f\"   Tasks Processed: {summary['total_tasks']}\")\n",
    "    print(f\"   Tasks Completed: {summary['completed']}\")\n",
    "    print(f\"   Tasks Failed: {summary['failed']}\")\n",
    "    print(f\"   Total Retries: {sum(summary['retry_stats'].values())}\")\n",
    "    print(f\"     - Approval: {summary['retry_stats']['approval_retries']}\")\n",
    "    print(f\"     - Execution: {summary['retry_stats']['execution_retries']}\")\n",
    "    print(f\"     - Verification: {summary['retry_stats']['verification_retries']}\")\n",
    "    print(f\"   Total Duration: {summary['total_duration_seconds']:.1f}s\")\n",
    "    print(\"\\n   Continue to Phase 8 (Reporting & Handoff)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "UXBaTV_4jGGD",
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1761086617595,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "UXBaTV_4jGGD"
   },
   "outputs": [],
   "source": [
    "#cell_10\n",
    "\n",
    "def rerun_execution_phase(task_id: str, phase: str) -> None:\n",
    "    ctx = task_contexts.get(task_id)\n",
    "    if not ctx:\n",
    "        print(f\"âŒ Task {task_id} not found in cached contexts.\")\n",
    "        return\n",
    "\n",
    "    phase_key = phase.lower()\n",
    "    if phase_key in {\"5\", \"phase5\", \"execution\"}:\n",
    "        phase5_execute(ctx, rerun=True)\n",
    "    elif phase_key in {\"5.5\", \"phase5.5\", \"execution_retry\"}:\n",
    "        phase5_retry(ctx)\n",
    "    elif phase_key in {\"6\", \"phase6\", \"verification\"}:\n",
    "        phase6_verify(ctx)\n",
    "    elif phase_key in {\"6.5\", \"phase6.5\", \"verification_retry\"}:\n",
    "        phase6_retry(ctx)\n",
    "    else:\n",
    "        print(\"âš ï¸ Unknown phase. Valid options: phase5, phase5.5, phase6, phase6.5\")\n",
    "\n",
    "\n",
    "def export_task_contexts(label: str = \"execution_cycle\") -> Path:\n",
    "    export_dir = Path(MULTI_AGENT_ROOT) / \"ledgers\" / \"execution_trajectories\"\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    export_path = export_dir / f\"{label}_{timestamp}.json\"\n",
    "\n",
    "    exportable = {}\n",
    "    for task_id, ctx in task_contexts.items():\n",
    "        exportable[task_id] = {\n",
    "            'task': ctx['task'],\n",
    "            'preliminary_status': ctx.get('preliminary_status'),\n",
    "            'execution_success': ctx.get('execution_success'),\n",
    "            'verification_passed': ctx.get('verification_passed'),\n",
    "            'execution_telemetry': ctx.get('execution_telemetry'),\n",
    "            'agent_responses': ctx.get('agent_responses'),\n",
    "        }\n",
    "\n",
    "    with export_path.open('w') as f:\n",
    "        json.dump(exportable, f, indent=2)\n",
    "\n",
    "    print(f\"ðŸ’¾ Task contexts exported to {export_path}\")\n",
    "    return export_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "MRQWavRXUFmi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761086617607,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "MRQWavRXUFmi",
    "outputId": "c18affaf-86b8-4611-c65b-880623caf295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Patched CLIP models to use eager attention implementation by default\n"
     ]
    }
   ],
   "source": [
    "#cell_clip_patch\n",
    "_CLIP_ATTENTION_PATCHED = False\n",
    "\n",
    "\n",
    "def _apply_clip_attention_patch() -> None:\n",
    "    \"\"\"Force CLIP models to use eager attention so output_attentions is supported.\"\"\"\n",
    "    global _CLIP_ATTENTION_PATCHED\n",
    "    if _CLIP_ATTENTION_PATCHED:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        from transformers import CLIPModel, CLIPTextModel, CLIPVisionModel\n",
    "    except Exception as err:\n",
    "        print(f\"âš ï¸ Unable to patch CLIP attention defaults: {err}\")\n",
    "        return\n",
    "\n",
    "    def _set_config(config):\n",
    "        if hasattr(config, \"attn_implementation\"):\n",
    "            config.attn_implementation = \"eager\"\n",
    "        if hasattr(config, \"output_attentions\"):\n",
    "            config.output_attentions = True\n",
    "\n",
    "    def _patch_from_pretrained(cls, original_fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            model = original_fn(*args, **kwargs)\n",
    "            if hasattr(model, \"text_model\") and hasattr(model.text_model, \"config\"):\n",
    "                _set_config(model.text_model.config)\n",
    "            if hasattr(model, \"vision_model\") and hasattr(model.vision_model, \"config\"):\n",
    "                _set_config(model.vision_model.config)\n",
    "            return model\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    CLIPModel.from_pretrained = _patch_from_pretrained(CLIPModel, CLIPModel.from_pretrained)\n",
    "    CLIPTextModel.from_pretrained = _patch_from_pretrained(CLIPTextModel, CLIPTextModel.from_pretrained)\n",
    "    CLIPVisionModel.from_pretrained = _patch_from_pretrained(CLIPVisionModel, CLIPVisionModel.from_pretrained)\n",
    "\n",
    "    _CLIP_ATTENTION_PATCHED = True\n",
    "    print(\"âœ… Patched CLIP models to use eager attention implementation by default\")\n",
    "\n",
    "\n",
    "def configure_clip_attn_defaults() -> None:\n",
    "    _apply_clip_attention_patch()\n",
    "\n",
    "\n",
    "def ensure_clip_attention_patch() -> None:\n",
    "    _apply_clip_attention_patch()\n",
    "\n",
    "\n",
    "configure_clip_attn_defaults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "Mv9pKcbCy4bw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761086617611,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "Mv9pKcbCy4bw",
    "outputId": "29c39ba4-f199-4973-a6e2-96de67dfaaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pending tasks: []\n"
     ]
    }
   ],
   "source": [
    "pending_ids = [\n",
    "    task_id\n",
    "    for task_id, ctx in task_contexts.items()\n",
    "    if ctx.get('final_status') != 'completed'\n",
    "]\n",
    "\n",
    "print(\"Pending tasks:\", pending_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "kfqmT5C4FFRf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761086617621,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "kfqmT5C4FFRf",
    "outputId": "77e21805-40c5-4ae9-eb32-b3eb823290cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(tracker, \"activate_task\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a34b6ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 0 complete.\n",
      "TEST_IMAGE_PATH: None\n",
      "TEST_DATA_DIR: None\n",
      "Creating test images for model validation...\n",
      "Created 5 test images at /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/test_data\n",
      " Primary test image: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/test_data/test_image_0.png\n",
      "   ðŸ“· Downloaded Pexels test image â†’ /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/test_data/test_image_pexels.jpg\n",
      "  Phase 0 infrastructure setup complete\n",
      "\n",
      "Phase 0 complete.\n",
      "TEST_IMAGE_PATH: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/test_data/test_image_pexels.jpg\n",
      "TEST_DATA_DIR: /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/test_data\n"
     ]
    }
   ],
   "source": [
    "# Phase 0 sanity check (run once before executing W1-006)\n",
    "import os\n",
    "\n",
    "ensure_phase_0()\n",
    "print(\"Phase 0 complete.\")\n",
    "print(\"TEST_IMAGE_PATH:\", os.environ.get(\"TEST_IMAGE_PATH\"))\n",
    "print(\"TEST_DATA_DIR:\", os.environ.get(\"TEST_DATA_DIR\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "KI3Vlhe2dxCS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197506,
     "status": "ok",
     "timestamp": 1761086891131,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "KI3Vlhe2dxCS",
    "outputId": "4649214a-ddfc-484c-f47d-19bcaac3a919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â„¹ï¸  Existing task contexts detected â€” call reset_execution_state() for a clean slate.\n",
      "\n",
      "ðŸ“‹ Found 8 tasks to process\n",
      "\n",
      "ðŸš€ Starting Task W1-006: Statistical significance analysis of Week 1 results\n",
      "   Priority: MEDIUM\n",
      "   âœ… Prepared 1 task(s) for execution phases\n",
      "\n",
      "ðŸ“‹ PHASE 3: Implementation (W1-006)\n",
      "   âœ… ops_commander responded (17742 chars)\n",
      "   âœ… quality_safety responded (2995 chars)\n",
      "   â„¹ï¸ Infrastructure fallback review applied\n",
      "   âœ… infrastructure responded (906 chars)\n",
      "   âœ… Phase implementation: pass\n",
      "\n",
      "ðŸ” PHASE 4: Approval Gate (W1-006)\n",
      "\n",
      "   ðŸ” Approval Gate Analysis:\n",
      "      Ops: APPROVED (conf: 0.90)\n",
      "      Quality: APPROVED (conf: 0.90)\n",
      "      Infrastructure: APPROVED (conf: 0.90)\n",
      "      âœ… ALL GATES APPROVED (avg confidence: 0.90)\n",
      "   âœ… Phase approval: pass\n",
      "\n",
      "âš¡ PHASE 5: Execution (W1-006)\n",
      "   ðŸ“¦ Found 1 code block(s)\n",
      "ðŸ”§ Infrastructure setup - Device: cpu\n",
      "ðŸ”¬ Week 1 Statistical Analysis Starting...\n",
      "ðŸ”§ Infrastructure setup - Device: cpu\n",
      "ðŸ”§ Loading CLIP model on cpu\n",
      "âœ… CLIP validated - Vision: 12, Text: 12 layers\n",
      "ðŸ“Š Extracting attention data...\n",
      "ðŸ“ˆ Running statistical tests...\n",
      "\n",
      "âœ… Analysis Complete!\n",
      "ðŸŽ¯ DECISION: NO-GO (LOW confidence)\n",
      "ðŸ“‹ Report: results/week1/statistical_analysis.md\n",
      "ðŸ“Š Hypothesis Tests: results/week1/hypothesis_tests.json\n",
      "ðŸ“ˆ Effect Sizes: results/week1/effect_sizes.csv\n",
      "ðŸ”— MLflow: 83adf8046b784dc7a41add2d8a6b90ba\n",
      "ðŸ–¥ï¸ Device: cpu\n",
      "      âœ… Block 1 executed successfully\n",
      "   âœ… Phase execution: pass\n",
      "\n",
      "ðŸ” PHASE 6: Verification (W1-006)\n",
      "   â„¹ï¸  No MLflow verification required for this task\n",
      "   âœ… Phase verification: pass\n",
      "\n",
      "ðŸ“Š PHASE 7: Final Status (W1-006)\n",
      "   âœ… Task completed successfully\n",
      "   âœ… Task completed in 98.1s - Status: completed\n",
      "   âœ… Task completed in 98.1s - Status: completed\n",
      "   ðŸ”„ Total retries: 0\n",
      "\n",
      "====== EXECUTION SUMMARY ======\n",
      "   Tasks Processed: 1\n",
      "   Tasks Completed: 1\n",
      "   Tasks Failed: 0\n",
      "   Total Retries: 10\n",
      "     - Approval: 3\n",
      "     - Execution: 2\n",
      "     - Verification: 0\n",
      "   Total Duration: 4583.9s\n",
      "\n",
      "   Continue to Phase 8 (Reporting & Handoff)\n"
     ]
    }
   ],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-006']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)\n",
    "\n",
    "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
    "print_execution_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PW12b2gk-FH7",
   "metadata": {
    "id": "PW12b2gk-FH7"
   },
   "outputs": [],
   "source": [
    "# load or paste your report text\n",
    "big_text_or_file_content = \"\"\"\n",
    "... your Week 1 GO/NO-GO report text here ...\n",
    "\"\"\"\n",
    "\n",
    "# send it through the relay\n",
    "result = send_go_nogo_report(big_text_or_file_content)\n",
    "print(result[\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "NAW79LCkvaJT",
   "metadata": {
    "executionInfo": {
     "elapsed": 26438,
     "status": "aborted",
     "timestamp": 1761086628034,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "NAW79LCkvaJT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tasks Processed: 1\n",
      "   Tasks Completed: 1\n",
      "   Tasks Failed: 0\n",
      "   Total Retries: 10\n",
      "     - Approval: 3\n",
      "     - Execution: 2\n",
      "     - Verification: 0\n",
      "   Total Duration: 4750.9s\n",
      "\n",
      "   Continue to Phase 8 (Reporting & Handoff)\n",
      "ðŸ’¾ Task contexts exported to /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers/execution_trajectories/week1_run_20251022T030007Z.json\n",
      "Exported task contexts to /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers/execution_trajectories/week1_run_20251022T030007Z.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aea2ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_execution_cycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Only prepare the second task (add more IDs to the list if you want to batch others)\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m selected_tasks = [\u001b[33m'\u001b[39m\u001b[33mW1-006\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prepared = \u001b[43mprepare_execution_cycle\u001b[49m(selected_tasks)\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prepared:\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTask preparation failed; confirm the task ID appears in pending_actions.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'prepare_execution_cycle' is not defined"
     ]
    }
   ],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-006']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)\n",
    "\n",
    "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
    "print_execution_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a1691",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_execution_cycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Only prepare the second task (add more IDs to the list if you want to batch others)\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m selected_tasks = [\u001b[33m'\u001b[39m\u001b[33mW1-006\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prepared = \u001b[43mprepare_execution_cycle\u001b[49m(selected_tasks)\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prepared:\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTask preparation failed; confirm the task ID appears in pending_actions.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'prepare_execution_cycle' is not defined"
     ]
    }
   ],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-006']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)\n",
    "\n",
    "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
    "print_execution_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39a68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_execution_cycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Only prepare the second task (add more IDs to the list if you want to batch others)\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m selected_tasks = [\u001b[33m'\u001b[39m\u001b[33mW1-006\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prepared = \u001b[43mprepare_execution_cycle\u001b[49m(selected_tasks)\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prepared:\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTask preparation failed; confirm the task ID appears in pending_actions.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'prepare_execution_cycle' is not defined"
     ]
    }
   ],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-006']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)\n",
    "\n",
    "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
    "print_execution_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d8cd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_execution_cycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Only prepare the second task (add more IDs to the list if you want to batch others)\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m selected_tasks = [\u001b[33m'\u001b[39m\u001b[33mW1-006\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prepared = \u001b[43mprepare_execution_cycle\u001b[49m(selected_tasks)\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prepared:\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTask preparation failed; confirm the task ID appears in pending_actions.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'prepare_execution_cycle' is not defined"
     ]
    }
   ],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-006']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)\n",
    "\n",
    "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
    "print_execution_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NvEwM36QJveW",
   "metadata": {
    "executionInfo": {
     "elapsed": 26439,
     "status": "aborted",
     "timestamp": 1761086628036,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "NvEwM36QJveW"
   },
   "outputs": [],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-004']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)\n",
    "\n",
    "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
    "print_execution_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "VumT_dM-0MwI",
   "metadata": {
    "executionInfo": {
     "elapsed": 26438,
     "status": "aborted",
     "timestamp": 1761086628037,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "VumT_dM-0MwI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tasks Processed: 1\n",
      "   Tasks Completed: 1\n",
      "   Tasks Failed: 0\n",
      "   Total Retries: 10\n",
      "     - Approval: 3\n",
      "     - Execution: 2\n",
      "     - Verification: 0\n",
      "   Total Duration: 2293.7s\n",
      "\n",
      "   Continue to Phase 8 (Reporting & Handoff)\n",
      "ðŸ’¾ Task contexts exported to /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers/execution_trajectories/week1_run_20251022T021910Z.json\n",
      "Exported task contexts to /Users/guyan/Library/CloudStorage/GoogleDrive-rc989@cornell.edu/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/cv_multimodal/project/computer-vision-clean/multi-agent/ledgers/execution_trajectories/week1_run_20251022T021910Z.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bk0_OuvXDKp4",
   "metadata": {
    "executionInfo": {
     "elapsed": 26437,
     "status": "aborted",
     "timestamp": 1761086628038,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "bk0_OuvXDKp4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-005']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)\n",
    "\n",
    "print(\"\\n====== EXECUTION SUMMARY ======\")\n",
    "print_execution_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_aW_QXcdGAGf",
   "metadata": {
    "executionInfo": {
     "elapsed": 26437,
     "status": "aborted",
     "timestamp": 1761086628039,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "_aW_QXcdGAGf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# PhaseÂ 8 summary + exports\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qHUytApa1DB6",
   "metadata": {
    "executionInfo": {
     "elapsed": 26437,
     "status": "aborted",
     "timestamp": 1761086628040,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "qHUytApa1DB6"
   },
   "outputs": [],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-006']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jGx6G3Ca1_eG",
   "metadata": {
    "executionInfo": {
     "elapsed": 26437,
     "status": "aborted",
     "timestamp": 1761086628042,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "jGx6G3Ca1_eG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# PhaseÂ 8 summary + exports\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-JtY5xH32CXx",
   "metadata": {
    "executionInfo": {
     "elapsed": 26438,
     "status": "aborted",
     "timestamp": 1761086628045,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "-JtY5xH32CXx"
   },
   "outputs": [],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-007']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gByHe8Kz37Rx",
   "metadata": {
    "executionInfo": {
     "elapsed": 26438,
     "status": "aborted",
     "timestamp": 1761086628047,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "gByHe8Kz37Rx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# PhaseÂ 8 summary + exports\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bSaacc2H3_VF",
   "metadata": {
    "executionInfo": {
     "elapsed": 26439,
     "status": "aborted",
     "timestamp": 1761086628050,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "bSaacc2H3_VF"
   },
   "outputs": [],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-008']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wDjVANgv5RkH",
   "metadata": {
    "executionInfo": {
     "elapsed": 26438,
     "status": "aborted",
     "timestamp": 1761086628051,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "wDjVANgv5RkH"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# PhaseÂ 8 summary + exports\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zjpt16YS7SuH",
   "metadata": {
    "executionInfo": {
     "elapsed": 26438,
     "status": "aborted",
     "timestamp": 1761086628053,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "zjpt16YS7SuH"
   },
   "outputs": [],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-001']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KfupErug7WTS",
   "metadata": {
    "executionInfo": {
     "elapsed": 26436,
     "status": "aborted",
     "timestamp": 1761086628053,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "KfupErug7WTS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# PhaseÂ 8 summary + exports\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UNHbhzLE8H1l",
   "metadata": {
    "executionInfo": {
     "elapsed": 26436,
     "status": "aborted",
     "timestamp": 1761086628054,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "UNHbhzLE8H1l"
   },
   "outputs": [],
   "source": [
    "# Only prepare the second task (add more IDs to the list if you want to batch others)\n",
    "selected_tasks = ['W1-002']\n",
    "prepared = prepare_execution_cycle(selected_tasks)\n",
    "\n",
    "if not prepared:\n",
    "    raise RuntimeError(\"Task preparation failed; confirm the task ID appears in pending_actions.json\")\n",
    "\n",
    "# Phase 3 â†’ 7, end-to-end\n",
    "run_phase3_batch(prepared)\n",
    "run_phase4_batch(prepared)\n",
    "\n",
    "# Retry cycle only kicks in when Phase 4 rejected something\n",
    "rejected = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'rejected']\n",
    "if rejected:\n",
    "    print(\"\\nðŸ”„ Running approval retries...\")\n",
    "    run_phase4_5_batch(rejected)\n",
    "\n",
    "approved = [tid for tid in prepared if task_contexts[tid].get('preliminary_status') == 'approved']\n",
    "if approved:\n",
    "    run_phase5_batch(approved)\n",
    "    run_phase5_retry_batch(approved)\n",
    "\n",
    "    executed = [tid for tid in approved if task_contexts[tid].get('execution_success')]\n",
    "    if executed:\n",
    "        run_phase6_batch(executed)\n",
    "        run_phase6_retry_batch(executed)\n",
    "\n",
    "run_phase7_batch(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nypsc6Ln8J8w",
   "metadata": {
    "executionInfo": {
     "elapsed": 26436,
     "status": "aborted",
     "timestamp": 1761086628056,
     "user": {
      "displayName": "Jess Chen",
      "userId": "13439625722997377244"
     },
     "user_tz": 240
    },
    "id": "Nypsc6Ln8J8w"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# PhaseÂ 8 summary + exports\n",
    "print_execution_summary()\n",
    "\n",
    "summary = tracker.get_summary()\n",
    "with open('week1_execution_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "snapshot_path = export_task_contexts(\"week1_run\")\n",
    "print(f\"Exported task contexts to {snapshot_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

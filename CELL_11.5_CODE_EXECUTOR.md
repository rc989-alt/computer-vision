
# Cell 11.5: Automatic Code Executor

**Purpose:** Extract Python code from Ops Commander responses and execute it automatically
**Location:** Insert between Cell 11 (Task Execution) and Cell 12 (Summary)
**Created:** 2025-10-15

---

## Complete Cell 11.5 Code

```python
# ============================================================
# CELL 11.5: AUTOMATIC CODE EXECUTOR
# Extracts and executes Python code from Ops Commander responses
# ============================================================

print("="*80)
print("‚ö° AUTOMATIC CODE EXECUTOR - EXECUTING AGENT IMPLEMENTATIONS")
print("="*80)

import re
import sys
from io import StringIO
from pathlib import Path
import traceback

def extract_python_code_blocks(text):
    """Extract all Python code blocks from markdown-formatted text"""
    # Pattern 1: ```python ... ``` blocks
    pattern1 = r'```python\n(.*?)```'
    # Pattern 2: ```\n ... ``` blocks (assume Python if no language specified)
    pattern2 = r'```\n(.*?)```'

    blocks = []

    # Try explicit python blocks first
    matches = re.findall(pattern1, text, re.DOTALL)
    blocks.extend(matches)

    # If no explicit python blocks, try generic code blocks
    if not matches:
        matches = re.findall(pattern2, text, re.DOTALL)
        blocks.extend(matches)

    return blocks

def should_execute_task(task_result):
    """Determine if a task should have code executed"""
    # Only execute for experimental tasks that were approved by all 3 agents
    if task_result['status'] != 'completed':
        return False

    action = task_result.get('action', '').lower()

    # Check if experimental (needs actual execution)
    experimental_keywords = [
        'execute', 'run', 'diagnostic', 'experiment', 'test',
        'baseline', 'gpu', 'model', 'integration', 'attention'
    ]

    for keyword in experimental_keywords:
        if keyword in action:
            return True

    return False

def safe_execute_code(code, task_id, task_action):
    """Execute code in controlled environment with output capture"""
    print(f"\n{'='*80}")
    print(f"üîß Executing code for Task {task_id}")
    print(f"   Action: {task_action[:60]}...")
    print(f"{'='*80}")

    # Capture stdout and stderr
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    stdout_capture = StringIO()
    stderr_capture = StringIO()

    execution_result = {
        'success': False,
        'stdout': '',
        'stderr': '',
        'error': None,
        'run_id': None
    }

    try:
        # Redirect output
        sys.stdout = stdout_capture
        sys.stderr = stderr_capture

        # Create execution namespace with necessary imports
        exec_globals = {
            '__name__': '__main__',
            'MULTI_AGENT_ROOT': MULTI_AGENT_ROOT,
            'Path': Path,
        }

        # Execute the code
        exec(code, exec_globals)

        execution_result['success'] = True

        # Try to extract run_id if it was created
        if 'run_id' in exec_globals:
            execution_result['run_id'] = exec_globals['run_id']

    except Exception as e:
        execution_result['error'] = str(e)
        execution_result['traceback'] = traceback.format_exc()

    finally:
        # Restore stdout/stderr
        sys.stdout = old_stdout
        sys.stderr = old_stderr

        # Get captured output
        execution_result['stdout'] = stdout_capture.getvalue()
        execution_result['stderr'] = stderr_capture.getvalue()

    return execution_result

# Execute code for each completed experimental task
execution_results = []

for i, task_result in enumerate(tracker.task_results, 1):
    if not should_execute_task(task_result):
        print(f"\n‚è≠Ô∏è  Task {i}: Skipping execution (not an experimental task or not completed)")
        continue

    print(f"\n{'='*80}")
    print(f"üìã Task {i}: {task_result['action'][:60]}...")
    print(f"{'='*80}")

    # Extract code from Ops Commander response
    ops_response = task_result['agent_responses'].get('ops_commander', '')

    if not ops_response:
        print(f"‚ö†Ô∏è  No Ops Commander response found, skipping execution")
        continue

    # Extract Python code blocks
    code_blocks = extract_python_code_blocks(ops_response)

    if not code_blocks:
        print(f"‚ö†Ô∏è  No Python code blocks found in response")
        print(f"   Response length: {len(ops_response)} chars")
        print(f"   Response preview: {ops_response[:200]}...")
        continue

    print(f"‚úÖ Found {len(code_blocks)} code block(s)")

    # Execute each code block
    task_execution_success = False
    task_run_id = None

    for j, code in enumerate(code_blocks, 1):
        print(f"\n--- Executing code block {j}/{len(code_blocks)} ---")
        print(f"Code length: {len(code)} chars")
        print(f"First 3 lines:")
        for line in code.split('\n')[:3]:
            print(f"  {line}")

        result = safe_execute_code(code, i, task_result['action'])

        # Print execution output
        if result['success']:
            print(f"\n‚úÖ Execution successful!")
            if result['stdout']:
                print(f"\n--- Output ---")
                print(result['stdout'][:1000])  # First 1000 chars
                if len(result['stdout']) > 1000:
                    print(f"... (truncated, total {len(result['stdout'])} chars)")

            # Look for run_id in output
            if 'MLflow Run ID:' in result['stdout']:
                match = re.search(r'MLflow Run ID:\s*([a-f0-9]+)', result['stdout'])
                if match:
                    task_run_id = match.group(1)
                    print(f"\nüéØ Captured MLflow Run ID: {task_run_id}")

            task_execution_success = True

        else:
            print(f"\n‚ùå Execution failed!")
            print(f"Error: {result['error']}")
            if result['stderr']:
                print(f"\n--- Error Output ---")
                print(result['stderr'][:500])
            if result.get('traceback'):
                print(f"\n--- Traceback ---")
                print(result['traceback'][:1000])

        execution_results.append({
            'task_id': i,
            'block_id': j,
            'result': result
        })

    # Update task result with execution info
    if task_execution_success:
        # Add execution evidence to task result
        if 'execution' not in task_result:
            task_result['execution'] = {}

        task_result['execution']['code_executed'] = True
        task_result['execution']['run_id'] = task_run_id

        # Append run_id to agent response so Phase 5.5 can find it
        if task_run_id:
            task_result['agent_responses']['ops_commander'] += f"\n\n‚úÖ CODE EXECUTED SUCCESSFULLY\nMLflow Run ID: {task_run_id}\n"
            print(f"\n‚úÖ Updated task result with run_id: {task_run_id}")

print(f"\n{'='*80}")
print(f"‚ö° CODE EXECUTION COMPLETE")
print(f"{'='*80}")
print(f"Tasks processed: {len(tracker.task_results)}")
print(f"Code blocks executed: {len(execution_results)}")
print(f"Successful executions: {sum(1 for r in execution_results if r['result']['success'])}")
print(f"Failed executions: {sum(1 for r in execution_results if not r['result']['success'])}")

# Store execution results for later reference
tracker.execution_results = execution_results

print(f"{'='*80}")
```

---

## What This Cell Does

### **1. Extracts Code from Agent Responses**
```python
extract_python_code_blocks(ops_response)
```
- Finds all ````python ... `````` blocks in Ops Commander response
- Handles truncated responses by extracting what's available
- Returns list of code blocks to execute

### **2. Determines Which Tasks to Execute**
```python
should_execute_task(task_result)
```
- Only executes EXPERIMENTAL tasks (integration, diagnostic, testing)
- Skips documentation tasks (they don't need code execution)
- Only executes tasks that were marked "completed" by 3-agent approval

### **3. Executes Code Safely**
```python
safe_execute_code(code, task_id, task_action)
```
- Captures stdout/stderr
- Handles errors gracefully
- Extracts MLflow run_id from output
- Returns execution result

### **4. Updates Task Results**
- Appends run_id to agent response
- Adds execution metadata
- Phase 5.5 can now find the run_id!

---

## Expected Output

```
================================================================================
‚ö° AUTOMATIC CODE EXECUTOR - EXECUTING AGENT IMPLEMENTATIONS
================================================================================

================================================================================
üìã Task 1: Re-execute Task 1: CLIP Integration with REAL attention ex...
================================================================================
‚úÖ Found 3 code block(s)

--- Executing code block 1/3 ---
Code length: 245 chars
First 3 lines:
  import torch
  import open_clip
  import mlflow

‚úÖ Execution successful!

--- Output ---
‚úÖ MLflow Run ID: abc123def456789
Device: cuda
‚úÖ CLIP model loaded
...

üéØ Captured MLflow Run ID: abc123def456789

‚úÖ Updated task result with run_id: abc123def456789

================================================================================
üìã Task 2: Execute Task 4: CLIP Diagnostic with REAL GPU execution on...
================================================================================
‚úÖ Found 1 code block(s)

--- Executing code block 1/1 ---
Code length: 1542 chars
First 3 lines:
  import torch
  import torchvision
  import open_clip

‚úÖ Execution successful!

--- Output ---
‚úÖ MLflow Run ID: def456ghi789abc
Loading CLIP model...
‚úÖ CLIP model loaded
Loading COCO validation samples...
‚úÖ Processing 150 samples
...

üéØ Captured MLflow Run ID: def456ghi789abc

‚úÖ Updated task result with run_id: def456ghi789abc

================================================================================
‚ö° CODE EXECUTION COMPLETE
================================================================================
Tasks processed: 3
Code blocks executed: 5
Successful executions: 4
Failed executions: 1
================================================================================
```

---

## How It Integrates with Phase 5.5

**Before Cell 11.5:**
```python
# Task result agent response (truncated):
ops_response = """
# MLflow experiment setup
mlflow.set_experiment("week1_clip_diagnostic")
with mlflow.start_run() as run:
    run_id = run.info.run_id
    print(f"‚úÖ MLflow Run ID: {run_id}")
    ...
"""
```
Phase 5.5 searches for run_id ‚Üí Finds "print" (wrong!) ‚Üí ‚ùå FAILS

**After Cell 11.5:**
```python
# Task result updated with actual execution output:
ops_response = """
... (original response) ...

‚úÖ CODE EXECUTED SUCCESSFULLY
MLflow Run ID: abc123def456789
"""
```
Phase 5.5 searches for run_id ‚Üí Finds "abc123def456789" ‚Üí ‚úÖ PASSES!

---

## Handling Truncated Code

**Problem:** Agent responses are truncated at 1000 chars (line 48 in execution summary)

**Cell 11.5 handles this by:**
1. Extracting whatever code blocks are available
2. Executing them (even if incomplete)
3. If execution fails, it's caught and logged
4. Failed executions don't crash the system

**Better solution (update Cell 15):**
```python
# In Cell 15 (progress report generation)
# Change line 48:
for agent_name, response in task_result['agent_responses'].items():
    progress_report += f"""
#### {agent_name}
```
{response[:1000]}{'...' if len(response) > 1000 else ''}  # <-- TRUNCATED HERE
```
"""

# To:
for agent_name, response in task_result['agent_responses'].items():
    progress_report += f"""
#### {agent_name}
```
{response}  # <-- FULL RESPONSE, NO TRUNCATION
```
"""
```

This ensures Cell 11.5 has full code to execute.

---

## Security Considerations

**Risk:** Executing arbitrary code from agents

**Mitigations in Cell 11.5:**
1. **Try-except wrapper** - Catches errors, doesn't crash notebook
2. **Output capture** - Prevents infinite prints
3. **Only experimental tasks** - Doesn't execute documentation tasks
4. **Sandboxed globals** - Limited execution namespace
5. **Traceback logging** - Debug failed executions

**Additional safeguards (optional):**
```python
# Add timeout
import signal
signal.alarm(300)  # 5 minute timeout per code block

# Restrict imports
exec_globals = {
    '__builtins__': {
        '__import__': safe_import,  # Custom import filter
        'open': safe_open,  # Restrict file access
    }
}
```

---

## Where to Insert

**Current notebook structure:**
- Cell 1-10: Setup, load tasks, initialize tracking
- **Cell 11:** Task Execution Loop (3-agent approval)
- **Cell 12:** Summary generation
- **Cell 15:** Progress report with truncated responses
- **Cell 16:** Phase 5.5 Evidence Verification

**Insert Cell 11.5:**
- **After Cell 11** (task execution with agent approval)
- **Before Cell 12** (summary generation)

**Flow:**
1. Cell 11: Agents approve tasks (status = "completed")
2. **Cell 11.5: Execute agent code ‚Üí Get real run_id ‚Üí Update task results**
3. Cell 12-15: Generate summaries (now include actual run_ids)
4. Cell 16: Phase 5.5 verification (now finds real run_ids) ‚Üí ‚úÖ PASSES!

---

## Testing

After adding Cell 11.5, test with:

```python
# Test code extraction
test_response = """
Here's the implementation:

```python
import mlflow
mlflow.set_experiment("test")
with mlflow.start_run() as run:
    print(f"Run ID: {run.info.run_id}")
```

This will create a run.
"""

blocks = extract_python_code_blocks(test_response)
print(f"Extracted {len(blocks)} blocks")
print(blocks[0])
```

Expected output:
```
Extracted 1 blocks
import mlflow
mlflow.set_experiment("test")
with mlflow.start_run() as run:
    print(f"Run ID: {run.info.run_id}")
```

---

## Benefits

‚úÖ **Automatic execution** - No manual work needed
‚úÖ **Real evidence** - Creates actual MLflow runs and files
‚úÖ **Phase 5.5 passes** - run_ids captured and added to responses
‚úÖ **Error handling** - Failed executions don't crash system
‚úÖ **Feedback loop** - Execution results stored in tracker
‚úÖ **Scalable** - Works for any number of tasks

---

## Limitations

‚ö†Ô∏è **Truncated code:** If agent response is truncated, code may be incomplete
  - **Solution:** Remove truncation in Cell 15 (line 48)

‚ö†Ô∏è **Dependencies:** Code may import packages not installed
  - **Solution:** Install common packages in Cell 1 setup

‚ö†Ô∏è **GPU memory:** Multiple tasks may exhaust GPU memory
  - **Solution:** Add `torch.cuda.empty_cache()` between tasks

‚ö†Ô∏è **Long execution:** Some diagnostics take 30+ minutes
  - **Solution:** Increase timeout or run tasks sequentially

---

## Next Steps

1. **Add Cell 11.5** to notebook (copy code above)
2. **Remove truncation** in Cell 15 (line 48) - change `response[:1000]` to `response`
3. **Re-run Cycle 2** from Cell 1
4. **Verify Phase 5.5 passes** with real evidence

---

**Status:** ‚úÖ Ready to deploy
**Impact:** Solves the "agents write code but don't execute" problem
**Next:** Insert Cell 11.5 ‚Üí Re-run notebook ‚Üí Get real evidence automatically!
